{
  "url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786",
  "repository_url": "https://api.github.com/repos/prysmaticlabs/prysm",
  "labels_url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786/labels{/name}",
  "comments_url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786/comments",
  "events_url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786/events",
  "html_url": "https://github.com/prysmaticlabs/prysm/issues/8786",
  "id": 861234154,
  "node_id": "MDU6SXNzdWU4NjEyMzQxNTQ=",
  "number": 8786,
  "title": "Internal Monitoring Service",
  "user": {
    "login": "nisdas",
    "id": 33201827,
    "node_id": "MDQ6VXNlcjMzMjAxODI3",
    "avatar_url": "https://avatars.githubusercontent.com/u/33201827?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/nisdas",
    "html_url": "https://github.com/nisdas",
    "followers_url": "https://api.github.com/users/nisdas/followers",
    "following_url": "https://api.github.com/users/nisdas/following{/other_user}",
    "gists_url": "https://api.github.com/users/nisdas/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/nisdas/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/nisdas/subscriptions",
    "organizations_url": "https://api.github.com/users/nisdas/orgs",
    "repos_url": "https://api.github.com/users/nisdas/repos",
    "events_url": "https://api.github.com/users/nisdas/events{/privacy}",
    "received_events_url": "https://api.github.com/users/nisdas/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 802129906,
      "node_id": "MDU6TGFiZWw4MDIxMjk5MDY=",
      "url": "https://api.github.com/repos/prysmaticlabs/prysm/labels/Enhancement",
      "name": "Enhancement",
      "color": "84b6eb",
      "default": false,
      "description": "New feature or request"
    },
    {
      "id": 934596141,
      "node_id": "MDU6TGFiZWw5MzQ1OTYxNDE=",
      "url": "https://api.github.com/repos/prysmaticlabs/prysm/labels/Discussion",
      "name": "Discussion",
      "color": "f9d0c4",
      "default": false,
      "description": "Simply a thread for talking about stuff"
    },
    {
      "id": 1241586918,
      "node_id": "MDU6TGFiZWwxMjQxNTg2OTE4",
      "url": "https://api.github.com/repos/prysmaticlabs/prysm/labels/Tracking",
      "name": "Tracking",
      "color": "d0ff7f",
      "default": false,
      "description": "Gotta Catch 'Em All"
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2021-04-19T12:00:19Z",
  "updated_at": "2022-02-01T23:13:22Z",
  "closed_at": null,
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "body": "# ðŸ’Ž Issue\r\n\r\n### Background\r\n\r\nCurrently any monitoring of the beacon node is done via external services(prometheus,grafana,jaeger). So any actions to restart/control the beacon node either come from data gleaned these services or the OS(in the event of an OOM).\r\n\r\n### Description\r\n\r\nThe beacon node should also be able to internally monitor the status of itself. Rather than rely on external services to determine whether to 'perform' an action, the beacon node should be able to do it self.\r\n\r\nFrom the go runtime, the beacon node has access to relevant metrics on active heap size and CPU utilization. Using these values we can either dynamically size any caches, or modify GC parameters to be more aggressive/lenient. In the event of a sudden growth in heap size(ex: memory leak), rather than allow it to trigger an OOM and cause the node to fail and go offline, we can instead tune the GC to be more aggressive or perform `FreeOSMemory` . This would at the very least allow the node to continue functioning albeit at the cost of increased CPU usage and page thrashing. \r\n\r\nSome Open Questions:\r\n- At what bounds does the internal monitoring service become 'active' \r\n- How aggressive should the node be in performing a GC and returning memory to the OS. \r\n- Do we continue with static cache sizes or instead of have dynamic ones( based on total available system memory). The latter is better for nodes which are on higher specced hardware. The former forces us to optimize for nodes on more resource constrained environments.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/comments/874813474",
    "html_url": "https://github.com/prysmaticlabs/prysm/issues/8786#issuecomment-874813474",
    "issue_url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786",
    "id": 874813474,
    "node_id": "MDEyOklzc3VlQ29tbWVudDg3NDgxMzQ3NA==",
    "user": {
      "login": "jmozah",
      "id": 940575,
      "node_id": "MDQ6VXNlcjk0MDU3NQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/940575?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jmozah",
      "html_url": "https://github.com/jmozah",
      "followers_url": "https://api.github.com/users/jmozah/followers",
      "following_url": "https://api.github.com/users/jmozah/following{/other_user}",
      "gists_url": "https://api.github.com/users/jmozah/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jmozah/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jmozah/subscriptions",
      "organizations_url": "https://api.github.com/users/jmozah/orgs",
      "repos_url": "https://api.github.com/users/jmozah/repos",
      "events_url": "https://api.github.com/users/jmozah/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jmozah/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2021-07-06T14:32:05Z",
    "updated_at": "2021-07-06T14:32:05Z",
    "author_association": "CONTRIBUTOR",
    "body": "+1 for this.\r\n\r\n> At what bounds does the internal monitoring service become 'active'\r\n\r\nThis choice should be left to the user. I may have a 64 core machine, but i would want `prysm` to use only 4 to 6 cores only. Same for memory and bandwidth. User should give some sort of `upper` and `lower` thresholds for resource utilisation and we can use the internal monitoring service to tune the CPU / Memory/ Bandwidth accordingly.\r\n\r\n> How aggressive should the node be in performing a GC and returning memory to the OS.\r\n\r\nAgain, this should be based on the memory / CPU thresholds. We should try to spread the resource utilisation such that it never exceeds the thresholds. The idea is to reduce having spikes in any of the resource utilisation.\r\n\r\n> Do we continue with static cache sizes or instead of have dynamic ones( based on total available system memory). The latter is better for nodes which are on higher specced hardware. The former forces us to optimize for nodes on more resource constrained environments.\r\n\r\nWe can start with few caches, have them dynamic and then scale this idea.",
    "reactions": {
      "url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/comments/874813474/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/comments/1023325920",
    "html_url": "https://github.com/prysmaticlabs/prysm/issues/8786#issuecomment-1023325920",
    "issue_url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/8786",
    "id": 1023325920,
    "node_id": "IC_kwDOBvuov848_rbg",
    "user": {
      "login": "rauljordan",
      "id": 5572669,
      "node_id": "MDQ6VXNlcjU1NzI2Njk=",
      "avatar_url": "https://avatars.githubusercontent.com/u/5572669?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/rauljordan",
      "html_url": "https://github.com/rauljordan",
      "followers_url": "https://api.github.com/users/rauljordan/followers",
      "following_url": "https://api.github.com/users/rauljordan/following{/other_user}",
      "gists_url": "https://api.github.com/users/rauljordan/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/rauljordan/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/rauljordan/subscriptions",
      "organizations_url": "https://api.github.com/users/rauljordan/orgs",
      "repos_url": "https://api.github.com/users/rauljordan/repos",
      "events_url": "https://api.github.com/users/rauljordan/events{/privacy}",
      "received_events_url": "https://api.github.com/users/rauljordan/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-01-27T15:21:27Z",
    "updated_at": "2022-01-27T15:21:27Z",
    "author_association": "CONTRIBUTOR",
    "body": "@nisdas and @jmozah after thinking about this for some time, most people that are running Prysm and have a need to regular and cap resource use are doing it in cloud environments, such as Kubernetes. These environments allow specifying resource limits to pods, for example. It's possible it should not be the responsibility of our code itself to monitor resource use and cap it, but we should instead leave that to the operators. I'm wondering how useful this feature will be by itself",
    "reactions": {
      "url": "https://api.github.com/repos/prysmaticlabs/prysm/issues/comments/1023325920/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
