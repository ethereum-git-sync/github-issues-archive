{
  "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
  "repository_url": "https://api.github.com/repos/ethereum/go-ethereum",
  "labels_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482/labels{/name}",
  "comments_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482/comments",
  "events_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482/events",
  "html_url": "https://github.com/ethereum/go-ethereum/issues/27482",
  "id": 1760006138,
  "node_id": "I_kwDOAOvK985o54_6",
  "number": 27482,
  "title": "Failure to catch-up sync after downtime",
  "user": {
    "login": "holiman",
    "id": 142290,
    "node_id": "MDQ6VXNlcjE0MjI5MA==",
    "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/holiman",
    "html_url": "https://github.com/holiman",
    "followers_url": "https://api.github.com/users/holiman/followers",
    "following_url": "https://api.github.com/users/holiman/following{/other_user}",
    "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
    "organizations_url": "https://api.github.com/users/holiman/orgs",
    "repos_url": "https://api.github.com/users/holiman/repos",
    "events_url": "https://api.github.com/users/holiman/events{/privacy}",
    "received_events_url": "https://api.github.com/users/holiman/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 72233650,
      "node_id": "MDU6TGFiZWw3MjIzMzY1MA==",
      "url": "https://api.github.com/repos/ethereum/go-ethereum/labels/type:bug",
      "name": "type:bug",
      "color": "FF5E5E",
      "default": false,
      "description": ""
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 9,
  "created_at": "2023-06-16T06:29:22Z",
  "updated_at": "2023-06-19T15:05:14Z",
  "closed_at": "2023-06-19T15:05:14Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "body": "I am not sure if this is an issue in geth, or lighthouse, but this is a good place to start discussing, I guess. Recently, on a machine running lighthouse +geth, the geth node went down. It was down for several days, and after starting up again, it just printed out `Forkchoice requested unknown head hash=7cadd7..d467d1`. \r\n\r\nThis message happens only via `FCU`. IT does not trigger geth to actually _do_ anything further: \r\n```golang\r\n\t// Check whether we have the block yet in our database or not. If not, we'll\r\n\t// need to either trigger a sync, or to reject this forkchoice update for a\r\n\t// reason.\r\n\tblock := api.eth.BlockChain().GetBlockByHash(update.HeadBlockHash)\r\n\tif block == nil {\r\n\t\t// If this block was previously invalidated, keep rejecting it here too\r\n\t\tif res := api.checkInvalidAncestor(update.HeadBlockHash, update.HeadBlockHash); res != nil {\r\n\t\t\treturn engine.ForkChoiceResponse{PayloadStatus: *res, PayloadID: nil}, nil\r\n\t\t}\r\n\t\t// If the head hash is unknown (was not given to us in a newPayload request),\r\n\t\t// we cannot resolve the header, so not much to do. This could be extended in\r\n\t\t// the future to resolve from the `eth` network, but it's an unexpected case\r\n\t\t// that should be fixed, not papered over.\r\n\t\theader := api.remoteBlocks.get(update.HeadBlockHash)\r\n\t\tif header == nil {\r\n\t\t\tlog.Warn(\"Forkchoice requested unknown head\", \"hash\", update.HeadBlockHash)\r\n\t\t\treturn engine.STATUS_SYNCING, nil\r\n\t\t}\r\n```\r\n\r\n\r\nSo, \r\n1. Geth & Lighthouse are in sync\r\n2. Geth goes down\r\n3. Geth starts up (days later)\r\n4. Lighthouse _only_ does `FCU` calls. \r\n\r\n\r\nI tried getting out of this rut by wiping geth and restarting, however, it got stuck in the same place again. Only when I wiped lighthouse did it finally trigger geth to sync properly again. \r\n\r\nNot sure if this is expected behaviour or not, and also which client is doing something wrong here.  \r\n\r\ncc @paulhauner",
  "closed_by": {
    "login": "holiman",
    "id": 142290,
    "node_id": "MDQ6VXNlcjE0MjI5MA==",
    "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/holiman",
    "html_url": "https://github.com/holiman",
    "followers_url": "https://api.github.com/users/holiman/followers",
    "following_url": "https://api.github.com/users/holiman/following{/other_user}",
    "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
    "organizations_url": "https://api.github.com/users/holiman/orgs",
    "repos_url": "https://api.github.com/users/holiman/repos",
    "events_url": "https://api.github.com/users/holiman/events{/privacy}",
    "received_events_url": "https://api.github.com/users/holiman/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482/reactions",
    "total_count": 1,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 1
  },
  "timeline_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
[
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594185021",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1594185021",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1594185021,
    "node_id": "IC_kwDOAOvK985fBVU9",
    "user": {
      "login": "michaelsproul",
      "id": 4452260,
      "node_id": "MDQ6VXNlcjQ0NTIyNjA=",
      "avatar_url": "https://avatars.githubusercontent.com/u/4452260?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelsproul",
      "html_url": "https://github.com/michaelsproul",
      "followers_url": "https://api.github.com/users/michaelsproul/followers",
      "following_url": "https://api.github.com/users/michaelsproul/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelsproul/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelsproul/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelsproul/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelsproul/orgs",
      "repos_url": "https://api.github.com/users/michaelsproul/repos",
      "events_url": "https://api.github.com/users/michaelsproul/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelsproul/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-16T06:41:16Z",
    "updated_at": "2023-06-16T06:42:46Z",
    "author_association": "NONE",
    "body": "Hey @holiman. You don't happen to still have logs from the Lighthouse node do you? Default level INFO logs would be fine, but debug logs would be even better. Lighthouse stores size-limited debug logs by default in `$datadir/beacon/logs`.\r\n\r\nThis could be related to Lighthouse's behaviour during finalized sync. We will avoid sending `newPayload` calls in this case, which allows us to get back in sync faster (it should look similar to checkpoint sync, from the EL's perspective). Once we get close to the head (~2 epochs) we'll resume sending `newPayload`, while remaining in \"optimistic sync\". Lighthouse _should_ follow the chain in this state, and feed a regular stream of `newPayload` _and_ `fcU` to the EL. If we aren't doing that, then it's a bug.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594185021/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594289113",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1594289113",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1594289113,
    "node_id": "IC_kwDOAOvK985fBuvZ",
    "user": {
      "login": "holiman",
      "id": 142290,
      "node_id": "MDQ6VXNlcjE0MjI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/holiman",
      "html_url": "https://github.com/holiman",
      "followers_url": "https://api.github.com/users/holiman/followers",
      "following_url": "https://api.github.com/users/holiman/following{/other_user}",
      "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
      "organizations_url": "https://api.github.com/users/holiman/orgs",
      "repos_url": "https://api.github.com/users/holiman/repos",
      "events_url": "https://api.github.com/users/holiman/events{/privacy}",
      "received_events_url": "https://api.github.com/users/holiman/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-16T08:10:17Z",
    "updated_at": "2023-06-16T08:10:17Z",
    "author_association": "MEMBER",
    "body": "The beacon logs looks a lot like this first, when all is good:\r\n\r\n```\r\nJun 10 17:09:44 crush beacon Jun 10 15:08:48.006 INFO New block received root: 0x435b381166dec9498b2523f79bd4e5cdde5f62d8b728dc74d6b01fcf4898ae6f, slot: 6632142\r\nJun 10 17:09:44 crush beacon Jun 10 15:08:53.000 INFO Synced slot: 6632142, block: 0x435b…ae6f, epoch: 207254, finalized_epoch: 207252, finalized_root: 0x40ee…8cc8, exec_hash: 0xa93e…c274 (verified), peers: 88, service: slot_notifier\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:00.045 INFO New block received root: 0x28533c43121425f0ccfca5d3802856a2f365ca4d6af8873a40050f6f82d5a171, slot: 6632143\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:05.001 INFO Synced slot: 6632143, block: 0x2853…a171, epoch: 207254, finalized_epoch: 207252, finalized_root: 0x40ee…8cc8, exec_hash: 0xc8d7…596e (verified), peers: 88, service: slot_notifier\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:12.557 INFO New block received root: 0x2d32c2c002767b6aeca0ab49e262c8ed9ae1aa09c741a56d1a606716cd8dbbbf, slot: 6632144\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:17.001 INFO Synced slot: 6632144, block: 0x2d32…bbbf, epoch: 207254, finalized_epoch: 207252, finalized_root: 0x40ee…8cc8, exec_hash: 0xfa8c…c4af (verified), peers: 88, service: slot_notifier\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:25.978 INFO New block received root: 0x6a2b63f16f4bea976b0fbb69506a674ddf7be8d6d4a1a7fa4476cce0d42adfa5, slot: 6632145\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:29.001 INFO Synced slot: 6632145, block: 0x6a2b…dfa5, epoch: 207254, finalized_epoch: 207252, finalized_root: 0x40ee…8cc8, exec_hash: 0x6e96…f21b (verified), peers: 88, service: slot_notifier\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:36.136 INFO New block received root: 0x1cfeb11765866a7524a2536aa759ee548238c9cc0082d29d6a4a9765cd61aaa3, slot: 6632146\r\nJun 10 17:09:44 crush beacon Jun 10 15:09:41.001 INFO Synced slot: 6632146, block: 0x1cfe…aaa3, epoch: 207254, finalized_epoch: 207252, finalized_root: 0x40ee…8cc8, exec_hash: 0xd116…0f09 (verified), peers: 88, service: slot_notifier\r\n```\r\nBut then there's a 2-day gap. I'm not sure what happened here (either lost logs or lighthouse also had downtime). The next thing that I see is a lot of this (this is when geth is still down): \r\n```\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:01.140 ERRO Error during execution engine upcheck error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }), service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:05.000 INFO Syncing est_time: --, distance: 9875 slots (1 day 8 hrs), peers: 88, service: slot_notifier\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:05.000 WARN Syncing deposit contract block cache est_blocks_remaining: 118972, service: slot_notifier\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:11.088 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 86.422µs\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:13.141 ERRO Error during execution engine upcheck error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }), service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:17.000 INFO Syncing est_time: --, distance: 9876 slots (1 day 8 hrs), peers: 88, service: slot_notifier\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:17.000 WARN Syncing deposit contract block cache est_blocks_remaining: 118972, service: slot_notifier\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:23.144 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 117.09µs\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:25.142 ERRO Error during execution engine upcheck error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }), service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:29.000 INFO Syncing est_time: --, distance: 9877 slots (1 day 8 hrs), peers: 88, service: slot_notifier\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:29.000 WARN Syncing deposit contract block cache est_blocks_remaining: 118972, service: slot_notifier\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:35.087 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 128.852µs\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:35.910 WARN Error connecting to eth1 node endpoint endpoint: http://geth:8551/, auth=true, service: deposit_contract_rpc\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:35.910 ERRO Error updating deposit contract cache error: Invalid endpoint state: RequestFailed(\"eth_chainId call failed Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \\\"http\\\", cannot_be_a_base: false, username: \\\"\\\", password: None, host: Some(Domain(\\\"geth\\\")), port: Some(8551), path: \\\"/\\\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\\\"dns error\\\", Custom { kind: Uncategorized, error: \\\"failed to lookup address information: Temporary failure in name resolution\\\" })) })\"), retry_millis: 60000, service: deposit_contract_rpc\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:37.143 ERRO Error during execution engine upcheck error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }), service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:40.370 WARN Execution engine call failed error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }), service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:40.370 ERRO Unable to get transition config error: Api { error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }) }, service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:40.370 ERRO Failed to check transition config error: EngineError(Api { error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }) }), service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:40.370 ERRO Error during execution engine upcheck error: Reqwest(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Domain(\"geth\")), port: Some(8551), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"dns error\", Custom { kind: Uncategorized, error: \"failed to lookup address information: Temporary failure in name resolution\" })) }), service: exec\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:41.000 INFO Syncing est_time: --, distance: 9878 slots (1 day 8 hrs), peers: 88, service: slot_notifier\r\nJun 12 02:02:01 crush beacon Jun 12 00:01:41.000 WARN Syncing deposit contract block cache est_blocks_remaining: 118972, service: slot_notifier\r\n```\r\nLater on, when I started `geth` up again, the beacon logs look like this: \r\n```\r\nJun 15 20:34:06 crush beacon Jun 15 18:33:35.913 ERRO Error updating deposit contract cache error: Failed to get remote head and new block ranges: EndpointError(FarBehind), retry_millis: 60000, service: deposit_contract_rpc\r\nJun 15 20:34:06 crush beacon Jun 15 18:33:41.000 INFO Syncing est_time: 50 mins, speed: 8.00 slots/sec, distance: 24238 slots (3 days 8 hrs), peers: 86, service: slot_notifier\r\nJun 15 20:34:06 crush beacon Jun 15 18:33:41.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:34:06 crush beacon Jun 15 18:33:47.316 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 479.2µs\r\nJun 15 20:34:06 crush beacon Jun 15 18:33:53.000 INFO Syncing est_time: 43 mins, speed: 9.33 slots/sec, distance: 24111 slots (3 days 8 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:34:06 crush beacon Jun 15 18:33:53.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:34:06 crush beacon Jun 15 18:33:59.082 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 82.255µs\r\nJun 15 20:34:06 crush beacon Jun 15 18:34:05.000 INFO Syncing est_time: 42 mins, speed: 9.33 slots/sec, distance: 23984 slots (3 days 7 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:34:06 crush beacon Jun 15 18:34:05.003 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:05.002 INFO Syncing est_time: 42 mins, speed: 9.31 slots/sec, distance: 23478 slots (3 days 6 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:05.003 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:11.085 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 82.154µs\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:17.001 INFO Syncing est_time: 41 mins, speed: 9.33 slots/sec, distance: 23350 slots (3 days 5 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:17.001 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:23.089 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 86.722µs\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:29.000 INFO Syncing est_time: 58 mins, speed: 6.67 slots/sec, distance: 23351 slots (3 days 5 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:29.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:35.084 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 81.563µs\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:35.910 WARN Execution endpoint is not synced last_seen_block_unix_timestamp: 1686398495, endpoint: http://geth:8551/, auth=true, service: deposit_contract_rpc\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:35.910 ERRO Error updating deposit contract cache error: Failed to get remote head and new block ranges: EndpointError(FarBehind), retry_millis: 60000, service: deposit_contract_rpc\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:41.001 INFO Syncing est_time: 1 hr 12 mins, speed: 5.33 slots/sec, distance: 23352 slots (3 days 5 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:41.002 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:47.135 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 74.74µs\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:53.000 INFO Syncing est_time: 1 hr 12 mins, speed: 5.35 slots/sec, distance: 23225 slots (3 days 5 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:53.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:35:59.069 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 556.174µs\r\nJun 15 20:36:05 crush beacon Jun 15 18:36:05.000 INFO Syncing est_time: 1 hr 12 mins, speed: 5.33 slots/sec, distance: 23098 slots (3 days 4 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:36:05 crush beacon Jun 15 18:36:05.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:38:05 crush beacon Jun 15 18:37:05.000 INFO Syncing est_time: 40 mins, speed: 9.33 slots/sec, distance: 22591 slots (3 days 3 hrs), peers: 88, service: slot_notifier\r\nJun 15 20:38:05 crush beacon Jun 15 18:37:05.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 20:38:05 crush beacon Jun 15 18:37:11.082 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 80.11µs\r\n```\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594289113/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594305329",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1594305329",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1594305329,
    "node_id": "IC_kwDOAOvK985fBysx",
    "user": {
      "login": "michaelsproul",
      "id": 4452260,
      "node_id": "MDQ6VXNlcjQ0NTIyNjA=",
      "avatar_url": "https://avatars.githubusercontent.com/u/4452260?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelsproul",
      "html_url": "https://github.com/michaelsproul",
      "followers_url": "https://api.github.com/users/michaelsproul/followers",
      "following_url": "https://api.github.com/users/michaelsproul/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelsproul/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelsproul/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelsproul/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelsproul/orgs",
      "repos_url": "https://api.github.com/users/michaelsproul/repos",
      "events_url": "https://api.github.com/users/michaelsproul/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelsproul/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-16T08:23:05Z",
    "updated_at": "2023-06-16T08:23:05Z",
    "author_association": "NONE",
    "body": "@holiman Ah, that makes sense then. It looks like Lighthouse was still doing finalized sync, in which case it won't send newPayload.\n\nIf you would like Lighthouse to send newPayload before it finishes syncing you can use the `--disable-optimistic-finalized-sync` flag. We get reports about this from other EL devs occasionally, and would be open to disabling the behaviour by default if you think that would be better.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594305329/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594319988",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1594319988",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1594319988,
    "node_id": "IC_kwDOAOvK985fB2R0",
    "user": {
      "login": "holiman",
      "id": 142290,
      "node_id": "MDQ6VXNlcjE0MjI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/holiman",
      "html_url": "https://github.com/holiman",
      "followers_url": "https://api.github.com/users/holiman/followers",
      "following_url": "https://api.github.com/users/holiman/following{/other_user}",
      "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
      "organizations_url": "https://api.github.com/users/holiman/orgs",
      "repos_url": "https://api.github.com/users/holiman/repos",
      "events_url": "https://api.github.com/users/holiman/events{/privacy}",
      "received_events_url": "https://api.github.com/users/holiman/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-16T08:34:29Z",
    "updated_at": "2023-06-16T08:34:29Z",
    "author_association": "MEMBER",
    "body": "> Ah, that makes sense then. It looks like Lighthouse was still doing finalized sync, in which case it won't send newPayload.\r\n\r\nCould you please elaborate on that? Both the part \"still doing\" -- as in, did it not finish the initial sync before the downtime? And \"finalized sync\" -- as opposed to what?  ",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594319988/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594391158",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1594391158",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1594391158,
    "node_id": "IC_kwDOAOvK985fCHp2",
    "user": {
      "login": "michaelsproul",
      "id": 4452260,
      "node_id": "MDQ6VXNlcjQ0NTIyNjA=",
      "avatar_url": "https://avatars.githubusercontent.com/u/4452260?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelsproul",
      "html_url": "https://github.com/michaelsproul",
      "followers_url": "https://api.github.com/users/michaelsproul/followers",
      "following_url": "https://api.github.com/users/michaelsproul/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelsproul/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelsproul/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelsproul/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelsproul/orgs",
      "repos_url": "https://api.github.com/users/michaelsproul/repos",
      "events_url": "https://api.github.com/users/michaelsproul/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelsproul/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-16T09:26:36Z",
    "updated_at": "2023-06-16T09:26:36Z",
    "author_association": "NONE",
    "body": "Sure.\r\n\r\nLighthouse has two different sync phases which use slightly different algorithms. The first is finalized sync, which kicks in when syncing long distances. Its goal is to get Lighthouse from wherever it is currently up to the finalized checkpoint reported by peers. Putting aside the issue of malicious peers which might misrepresent their finalized checkpoint, this sync phase will run until Lighthouse is about 2 epochs behind the canonical head. The 2nd sync phase occurs once Lighthouse has caught up to the finalized checkpoint, and is called \"head sync\". It tolerates peers which report diverging chains and will follow the ones that seem most promising until it reaches the head.\r\n\r\nBy default, Lighthouse will only send `newPayload` to the EL when it is in the head sync phase. During finalized sync, it will send `fcU` occasionally, but no `newPayload`. When it changes from one sync phase to the other it will log `INFO Sync state updated` with the old and new phases.\r\n\r\nWhat I think happened in your case was:\r\n\r\n1. Geth goes down, and is completely offline. It doesn't respond to any calls on the engine port.\r\n2. Lighthouse stops being able to make progress because Geth isn't responding, and sits waiting for Geth to come back. That's what the `INFO Syncing est_time: --, distance: 9878 slots (1 day 8 hrs), peers: 88, service: slot_notifier` logs show. Lighthouse does not currently attempt to sync (even optimistically) if the execution node is completely offline (or returning errors). Optimistic sync can only happen when the EL is online and responding with SYNCING/ACCEPTED to `fcU`. Arguably this is a bit strange, because we aren't expecting the EL to actually _do_ much during finalized sync, i.e. we are checking the block roots in Lighthouse rather than sending them to the EL for the block root check & SYNCING response.\r\n3. Once Geth comes back online, Lighthouse resumes trying to sync. It's a long way behind by now, so it uses the finalized sync algorithm which doesn't send `newPayload` to Geth. Based on the logs, it was _still_ attempting finalized sync when it logged: `INFO Syncing est_time: 40 mins, speed: 9.33 slots/sec, distance: 22591 slots (3 days 3 hrs), peers: 88`. It's 3 days behind the head and estimating that it needs another 40 minutes to catch up.\r\n\r\nIf you waited for Lighthouse to finish finalized sync (distance ~= 2 epochs), then it would transition to head sync, at which point it would start sending `newPayload` to Geth.\r\n\r\nIn terms of tweaking the behaviour here there are two things we could try to optimise:\r\n\r\n1. First is the long wait for Lighthouse to finish finalized sync and send `newPayload`. This can be mitigated by using `--disable-optimistic-finalized-sync`, which was previously the default. With that flag, Lighthouse would send `newPayload` as soon as Geth came back (start of step 3).\r\n2. The other thing that would be nice to avoid is the long downtime in step (2). To do this would require \"going the other way\" in terms of allowing Lighthouse to do more without Geth. Lighthouse _could_ continue verifying EL block hashes and syncing optimistically while Geth is offline, meaning that it would be synced to the head (and sending `newPayload`) by the time Geth came back online. The reason we don't do this currently is that it's somewhat dangerous for Lighthouse to go into optimistic mode when the execution node is erroring or not responding. If an attacker could publish a block that caused a large portion of ELs to error, and CLs went into optimistic sync, then a _large portion of validators would stop attesting and producing blocks_. We refer to this kind of block as a \"bomb block\" (@paulhauner's term). There might be room however for Lighthouse t sync optimistically on its own during _finalized chain sync_. This would avoid the issues of bomb blocks causing liveness faults, while also allowing a Lighthouse node with an offline/erroring EL to at least keep up with the finalized checkpoint (so it is at most 2 epochs behind by the time the EL comes back).\r\n\r\nSorry for the long response! Curious to hear what you think of this design space and what you think the right trade-offs might be.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594391158/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594417273",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1594417273",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1594417273,
    "node_id": "IC_kwDOAOvK985fCOB5",
    "user": {
      "login": "holiman",
      "id": 142290,
      "node_id": "MDQ6VXNlcjE0MjI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/holiman",
      "html_url": "https://github.com/holiman",
      "followers_url": "https://api.github.com/users/holiman/followers",
      "following_url": "https://api.github.com/users/holiman/following{/other_user}",
      "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
      "organizations_url": "https://api.github.com/users/holiman/orgs",
      "repos_url": "https://api.github.com/users/holiman/repos",
      "events_url": "https://api.github.com/users/holiman/events{/privacy}",
      "received_events_url": "https://api.github.com/users/holiman/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-16T09:47:40Z",
    "updated_at": "2023-06-16T09:47:40Z",
    "author_association": "MEMBER",
    "body": "Thanks for the info!\r\n\r\nSome notes\r\n\r\n>     It's 3 days behind the head and estimating that it needs another 40 minutes to catch up.\r\n\r\n> If you waited for Lighthouse to finish finalized sync (distance ~= 2 epochs), then it would transition to head sync, at which point it would start sending newPayload to Geth.\r\n\r\nSo, by the sound of it, had I waited out those 40 minutes, thing might have gotten ok again. However, here's a snippet roughly `23 minutes` later -- the estimation goes `21 min` -> `1h 23 min` -> `2h 47 min`: \r\n\r\n```\r\nJun 15 21:00:05 crush beacon Jun 15 18:59:53.000 INFO Syncing est_time: 24 mins, speed: 9.34 slots/sec, distance: 13745 slots (1 day 21 hrs), peers: 88, service: slot_notifier\r\nJun 15 21:00:05 crush beacon Jun 15 18:59:53.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 21:00:05 crush beacon Jun 15 18:59:59.081 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 101.851µs\r\nJun 15 21:00:05 crush beacon Jun 15 19:00:05.000 INFO Syncing est_time: 24 mins, speed: 9.33 slots/sec, distance: 13682 slots (1 day 21 hrs), peers: 88, service: slot_notifier\r\nJun 15 21:00:05 crush beacon Jun 15 19:00:05.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 21:02:05 crush beacon Jun 15 19:01:05.000 INFO Syncing est_time: 1 hr 23 mins, speed: 2.67 slots/sec, distance: 13431 slots (1 day 20 hrs), peers: 88, service: slot_notifier\r\nJun 15 21:02:05 crush beacon Jun 15 19:01:05.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\nJun 15 21:02:05 crush beacon Jun 15 19:01:11.081 WARN Error processing HTTP API request method: POST, path: /eth/v1/validator/prepare_beacon_proposer, status: 503 Service Unavailable, elapsed: 97.363µs\r\nJun 15 21:02:05 crush beacon Jun 15 19:01:17.000 INFO Syncing est_time: 2 hrs 47 mins, speed: 1.33 slots/sec, distance: 13368 slots (1 day 20 hrs), peers: 88, service: slot_notifier\r\nJun 15 21:02:05 crush beacon Jun 15 19:01:17.000 WARN Syncing deposit contract block cache est_blocks_remaining: 141792, service: slot_notifier\r\n```\r\nBut I assume that's just the inherent fluctuations of time-estimation; and that means that I could have waited it out, and it would have sorted itself out, eventually. And in that case, I don't see this as a bug anymore, but just \"it works as expected (I just didn't give it enough time)\". \r\n\r\n\r\n---- \r\n\r\nAlso, interesting thoughts about the bomb block. \r\n\r\n> If an attacker could publish a block that caused a large portion of ELs to error, and CLs went into optimistic sync, then a large portion of validators would stop attesting and producing blocks. \r\n\r\nSo what is the alternative? What is the current macro-network-behaviour on \"a large portion of EL drop-offs\"? \r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1594417273/reactions",
      "total_count": 2,
      "+1": 2,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1597112496",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1597112496",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1597112496,
    "node_id": "IC_kwDOAOvK985fMgCw",
    "user": {
      "login": "aa21",
      "id": 486730,
      "node_id": "MDQ6VXNlcjQ4NjczMA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/486730?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/aa21",
      "html_url": "https://github.com/aa21",
      "followers_url": "https://api.github.com/users/aa21/followers",
      "following_url": "https://api.github.com/users/aa21/following{/other_user}",
      "gists_url": "https://api.github.com/users/aa21/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/aa21/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/aa21/subscriptions",
      "organizations_url": "https://api.github.com/users/aa21/orgs",
      "repos_url": "https://api.github.com/users/aa21/repos",
      "events_url": "https://api.github.com/users/aa21/events{/privacy}",
      "received_events_url": "https://api.github.com/users/aa21/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-19T12:37:03Z",
    "updated_at": "2023-06-19T12:37:03Z",
    "author_association": "NONE",
    "body": "Thanks for this report @holiman  and the detailed explanation @michaelsproul.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1597112496/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1597226830",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1597226830",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1597226830,
    "node_id": "IC_kwDOAOvK985fM79O",
    "user": {
      "login": "michaelsproul",
      "id": 4452260,
      "node_id": "MDQ6VXNlcjQ0NTIyNjA=",
      "avatar_url": "https://avatars.githubusercontent.com/u/4452260?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelsproul",
      "html_url": "https://github.com/michaelsproul",
      "followers_url": "https://api.github.com/users/michaelsproul/followers",
      "following_url": "https://api.github.com/users/michaelsproul/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelsproul/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelsproul/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelsproul/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelsproul/orgs",
      "repos_url": "https://api.github.com/users/michaelsproul/repos",
      "events_url": "https://api.github.com/users/michaelsproul/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelsproul/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-19T13:48:14Z",
    "updated_at": "2023-06-19T13:55:09Z",
    "author_association": "NONE",
    "body": "> But I assume that's just the inherent fluctuations of time-estimation; and that means that I could have waited it out, and it would have sorted itself out, eventually.\r\n\r\nI think so, yeah.\r\n\r\n> So what is the alternative? What is the current macro-network-behaviour on \"a large portion of EL drop-offs\"?\r\n\r\nIf the ELs error on one specific block, then the \"bombed\" validators will orphan that block by attesting to its parent and producing a new chain on top of that parent. Assuming the bombed validators have more attesting weight than the un-bombed, this chain will become canonical. If there are multiple bomb blocks then the chain will limp along as best it can, with minority EL validators possibly forking to the bomb chain periodically. The network retains liveness, and could be capable of finalizing depending on the frequency of bomb blocks & the EL/validator share.\r\n\r\nCompare this to if the bombed validators go into optimistic sync: they stop attesting or creating blocks completely. The un-bombed validators continue the chain containing the bomb block, and the bombed validators are unable to follow it. The network grinds to a halt and stops finalizing if the bombed validators make up >1/3 of weight. The best way out of this is for the bombed validators to patch (or switch ELs), and the worst way is for them to get leaked out until they exit (at balance ~= 16 ETH).\r\n\r\nI feel like the first scenario is preferable to the 2nd, but neither are ideal, and maybe there's an argument to be made that the 2nd scenario is OK in that it encourages building a single chain. To me it seems less resilient.\r\n\r\nEDIT: I kind of glossed over the difference between the bombed validators having `k>1/2` vs `1/3 < k <1/2`. In this case even without the optimistic-sync-on-error behaviour this would halt finalization and create two chains that both leak the validators from the opposing chain. This is more similar to the optimistic sync behaviour, and the network wouldn't re-gain finality without patching, or a bit of leaking.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1597226830/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1597349391",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/27482#issuecomment-1597349391",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/27482",
    "id": 1597349391,
    "node_id": "IC_kwDOAOvK985fNZ4P",
    "user": {
      "login": "holiman",
      "id": 142290,
      "node_id": "MDQ6VXNlcjE0MjI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/holiman",
      "html_url": "https://github.com/holiman",
      "followers_url": "https://api.github.com/users/holiman/followers",
      "following_url": "https://api.github.com/users/holiman/following{/other_user}",
      "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
      "organizations_url": "https://api.github.com/users/holiman/orgs",
      "repos_url": "https://api.github.com/users/holiman/repos",
      "events_url": "https://api.github.com/users/holiman/events{/privacy}",
      "received_events_url": "https://api.github.com/users/holiman/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-06-19T15:05:14Z",
    "updated_at": "2023-06-19T15:05:14Z",
    "author_association": "MEMBER",
    "body": "Thanks for all the info! Closing this (but bookmarking it for future refs)",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/1597349391/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
