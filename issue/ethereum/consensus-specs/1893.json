{
  "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
  "repository_url": "https://api.github.com/repos/ethereum/consensus-specs",
  "labels_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893/labels{/name}",
  "comments_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893/comments",
  "events_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893/events",
  "html_url": "https://github.com/ethereum/consensus-specs/issues/1893",
  "id": 638667557,
  "node_id": "MDU6SXNzdWU2Mzg2Njc1NTc=",
  "number": 1893,
  "title": "the Merkleization section of simple-serialize.md",
  "user": {
    "login": "booleanfunction",
    "id": 43776922,
    "node_id": "MDQ6VXNlcjQzNzc2OTIy",
    "avatar_url": "https://avatars.githubusercontent.com/u/43776922?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/booleanfunction",
    "html_url": "https://github.com/booleanfunction",
    "followers_url": "https://api.github.com/users/booleanfunction/followers",
    "following_url": "https://api.github.com/users/booleanfunction/following{/other_user}",
    "gists_url": "https://api.github.com/users/booleanfunction/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/booleanfunction/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/booleanfunction/subscriptions",
    "organizations_url": "https://api.github.com/users/booleanfunction/orgs",
    "repos_url": "https://api.github.com/users/booleanfunction/repos",
    "events_url": "https://api.github.com/users/booleanfunction/events{/privacy}",
    "received_events_url": "https://api.github.com/users/booleanfunction/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1170174610,
      "node_id": "MDU6TGFiZWwxMTcwMTc0NjEw",
      "url": "https://api.github.com/repos/ethereum/consensus-specs/labels/scope:SSZ",
      "name": "scope:SSZ",
      "color": "77428D",
      "default": false,
      "description": "Simple Serialize"
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": {
    "login": "protolambda",
    "id": 19571989,
    "node_id": "MDQ6VXNlcjE5NTcxOTg5",
    "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/protolambda",
    "html_url": "https://github.com/protolambda",
    "followers_url": "https://api.github.com/users/protolambda/followers",
    "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
    "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
    "organizations_url": "https://api.github.com/users/protolambda/orgs",
    "repos_url": "https://api.github.com/users/protolambda/repos",
    "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
    "received_events_url": "https://api.github.com/users/protolambda/received_events",
    "type": "User",
    "site_admin": false
  },
  "assignees": [
    {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    }
  ],
  "milestone": null,
  "comments": 12,
  "created_at": "2020-06-15T08:54:58Z",
  "updated_at": "2020-07-23T17:10:18Z",
  "closed_at": "2020-07-23T17:10:18Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "body": "In [simple-serialize.md](https://github.com/ethereum/eth2.0-specs/blob/dev/ssz/simple-serialize.md), within the Merkelization section, a `chunk_count` function is defined as:\r\n\r\n\r\n>- chunk_count(type): calculate the amount of leafs for merkleization of the type.\r\n>   - all basic types: 1\r\n>   - Bitlist[N] and Bitvector[N]: (N + 255) // 256 (dividing by chunk size, rounding up)\r\n>   - List[B, N] and Vector[B, N], where B is a basic type: (N * size_of(B) + 31) // 32 (dividing by chunk size, rounding up)\r\n>   - List[C, N] and Vector[C, N], where C is a composite type: N\r\n>   - containers: len(fields)\r\n\r\n\r\nIf `chunk_count` does represent _the amount of leafs for merkleization of a type_ then the formulas for `Bitlist[N]`, `List[B,N]` and `List[C,N]` should be amended because any type, including say a `List[B,0]`, requires at least 1 leaf (even if that 1 leaf effectively becomes the root node when calculating the root tree hash).\r\n\r\nNote that empty bitvectors and vectors are illegal and so their `chunk_count` will always be at least 1 anyway. Hence the fix can be easily applied using the current grouping of types to specify the various formulas.\r\n\r\n**FIX 1: Amend formulas to return at minimum of 1 chunk.**\r\n\r\npy-ssz: I can see that in the end everything works out in py-ssz, despite with this mismatch of definition and formula within the specification of `chunk_count`, but it would be great if the spec could be made as precise as possible.\r\n\r\n## Further issues\r\n\r\nThere are several problems (including a missing case and error preventing merkleization of bitlists[N=0] and lists[B,N=0]) in the remainder of the merkleization section, basically stemming from this `chunk_count` issue. Though, again, I note that while everything works out in the py-ssz implementation, from a specification perspective it would be great to fix these problems; therefore ensuring that the spec is precise and consistent.\r\n\r\nI will summarise these inconsistencies and then provide a suggested amendment at the end.\r\n\r\n## 1.\tpack(value) and pack_bits(bits) functions\r\n\r\n> - pack(value): given ordered objects of the same basic type, serialize them, pack them into BYTES_PER_CHUNK-byte chunks, right-pad the last chunk with zero bytes, and return the chunks.\r\n> - pack_bits(bits): Given the bits of bitlist or bitvector, get bitfield_bytes by packing them in bytes and aligning to the start. The length-delimiting bit for bitlists is excluded. And then pack bitfield_bytes into BYTES_PER_CHUNK-byte chunks, right-pad the last chunk with zero bytes, and return the chunks.\r\n\r\n- For `List[B, N=0]` and `Bitlist[N=0]` the current wording of the spec gives `chunk_count=0`for these cases.\r\n- The wording of the `pack(value)` and `pack_bits(bits)` functions imply that at least 1 chunk will be returned i.e. there must be at least 1 chunk if the _last chunk_ is to be _right-pad with zero bytes_. \r\n- Hence the current spec implies that `len(pack(value)) > chunk_count(type`) when `value` is a List[B,N=0], and also `len(pack_bits(bits)) > chunk_count(type)` when `bits` represents a Bitlist[N=0].\r\n- This seems counter-intuitive and will cause an error with merkleization as will be discussed below. \r\n- It makes sense to ensure that padding to whole chunks (such that at least 1 chunk is returned) is performed at the `pack`/`pack_bits` stage.\r\n- Hence, explicitly state that `pack` and `pack_bits` return at least 1 chunk to make the spec precise.\r\n- Also, these points are cited as being further evidence that the `chunk_count` function formulas should be amended so that at least 1 chunk is always returned (see above, Fix 1).\r\n- Note, in py-ssz the `pack` function returns 1 zeroed chunk (the empty chunk) for a list with N=0 but the `pack_bits` function returns [] for a bitlist with N=0. In the bitlist case the padding to 1 empty chunk is left until later, occurring within merkleization. I mention this to acknowledge that the correct output for hash_tree_root is ultimately achieved for these cases in py-ssz but even if implementations differ, the spec should be precise and consistent in the definition of functions.\r\n\r\n**FIX 2: Explicitly state that the pack and pack_bits functions return at least 1 chunk.**\r\n\r\n## 2.\tmerkleize(chunks, limit=None) function\r\n\r\n> - merkleize(chunks, limit=None): Given ordered BYTES_PER_CHUNK-byte chunks, merkleize the chunks, and return the root:\r\n>   - The merkleization depends on the effective input, which can be padded/limited:\r\n>       - if no limit: pad the chunks with zeroed chunks to next_pow_of_two(len(chunks)) (virtually for memory efficiency).\r\n>       - if limit > len(chunks), pad the chunks with zeroed chunks to next_pow_of_two(limit) (virtually for memory efficiency).\r\n>        - if limit < len(chunks): do not merkleize, input exceeds limit. Raise an error instead.\r\n>   - Then, merkleize the chunks (empty input is padded to 1 zero chunk):\r\n>      - If 1 chunk: the root is the chunk itself.\r\n>      - If > 1 chunks: merkleize as binary tree.\r\n\r\n\r\n_Case `limit==len(chunks)` not considered_\r\n\r\n- The case where `limit == len(chunks)` is not considered in the first step of the merkleization function, yet it is possible for the `limit==len(chunks)` case to occur.  \r\n- Later in the spec we see that limit is set to `chunk_count(type)` for bitvectors, lists of basic objects, bitlists, and lists of composite objects.\r\n- It can also be seen that `len(chunks)=len(pack/pack_bits(value))` for bitvectors, lists of basic objects and bitlists; and `len(chunks)=len([hash_tree_root(element) for element in value])` for lists of composite objects.\r\n\r\n\r\n- For bitvectors, lists of basic objects and bitlists effectively we are considering the case where `chunk_count(type)==len(pack/pack_bits(value))`:\r\n  - for bitvectors, `chunk_count(type)==len(pack_bits(value))` will always be true; and,\r\n  - for bitlists and lists of basic objects `chunk_count(type)` represents the maximum number of chunks required given the parameter `N`, and there is no reason why the number of values in a bitlist or list can’t be equal to `N` thus ensuring that it is possible for `chunk_count(type)==len(pack/pack_bits(value))`.\r\n- For lists of composite objects effectively we are considering the case where `chunk_count(type)==len([hash_tree_root(element) for element in value])` and again there is no reason why a List[C,N] can't have the full N objects that would cause this case to occur.\r\n\r\n**FIX 3: Amend `limit > len(chunks)` to be `limit >= len(chunks)`**\r\n\r\n_Error implied for bitlists and list when N=0_\r\n\r\n- As the spec is currently written an error can occur preventing merkleization of bitlists and lists (of basic objects) when N=0.\r\n- For example, a `bitlist[N=0]` has `chunk_count=0` and if `pack_bits` does return at least 1 chunk (as implied by the wording in the spec) then an error will be raised in the `merkleize` function because `limit < len(chunks)`. \r\n   - i.e. since in this case `limit=chunk_count(type)=0` and `len(chunks)=len(pack_bits(bits))=1` as implied by the wording of `pack_bits`.\r\n- In py-ssz the `pack_bits` function returns 0 chunks for a bitlist[0], hence `len(chunks)=len(pack_bits(bits))=0` and so no error is raised. Also note that in py-ssz the `merkleize_with_cache` function specifically deals with the case where `limit==0` and returns `ZERO_HASHES[0]`.\r\n- Hence the _error_ identified here is not applicable in py-ssz however, a change to ensure `chunk_count` > 0 would improve consistency and prevent this error within the spec.\r\n\r\n**FIX 4: Refer to Fix 1.**\r\n\r\n\r\n_Empty input shouldn't occur_\r\n\r\n- The second step in the merkleization function says ‘Then, merkleize the chunks (empty input is padded to 1 zero chunk)’\r\n- It is not possible to have empty input at this step (assuming Fix 3). \r\n- The padding with zeroed chunks to the `next_pow_of_two` chunks that occurs in the first step of merkleize ensures at least 1 chunk by the second step because `next_pow_of_two(i)` > 0.\r\n   - i.e. This would be the case even if 0 chunks were received as input because `next_pow_of_two(0)` should equal 1, assuming that Fix 3 is included to ensure that the first step of the merkleization function is always performed. \r\n- If Fix 3 weren't adopted, for the case when `limit==len(chunks)==0` we could say that if empty input is padded to 1 zeroed chunk during the second step, then the first step doesn't need to be performed.    \r\n  - However, the inclusion of this empty input provision at the second step of the merkleization function doesn't fix the `limit==len(chunks)` case missing in the first step when we have a variable length type holding a maximum N values. \r\n  - Hence Fix 3 is necessary.\r\n\r\n**FIX 5: Remove reference to ‘empty input is padded to 1 zero chunk’**\r\n\r\n## 3.\thash_tree_root(value) function\r\n\r\n>We now define Merkleization hash_tree_root(value) of an object value recursively:\r\n> - merkleize(pack(value)) if value is a basic object or a vector of basic objects.\r\n> - merkleize(pack_bits(value), limit=chunk_count(type)) if value is a bitvector.\r\n> - mix_in_length(merkleize(pack(value), limit=chunk_count(type)), len(value)) if value is a list of basic objects.\r\n> - mix_in_length(merkleize(pack_bits(value), limit=chunk_count(type)), len(value)) if value is a bitlist.\r\n> - merkleize([hash_tree_root(element) for element in value]) if value is a vector of composite objects or a container.\r\n> - mix_in_length(merkleize([hash_tree_root(element) for element in value], limit=chunk_count(type)), len(value)) if value is a list of composite objects.\r\n> - mix_in_type(merkleize(value.value), value.type_index) if value is of union type.\r\n\r\n- The `limit` parameter not required for a bitvector\r\n- As `chunk_count(bitvector)==len(pack_bits(bits)` (i.e `bits` represent a bitvector) it is not necessary to calculate a `limit`, rather the merkelize function can be called with `limit=None`.\r\n\r\n**FIX 6: Amend merkleize for bitvectors to `merkleize(pack_bits(value))`**\r\n\r\n# Suggested wording for the merkleization section.\r\n\r\nThe above suggestions would result in the following amended version of this section of the SSZ specification.\r\n\r\n## Merkleization\r\nWe first define helper functions:\r\n\r\n- `size_of(B)`, where `B` is a basic type: the length, in bytes, of the serialized form of the basic type.\r\n- `chunk_count(type)`: calculate the amount of leafs for merkleization of the type.\r\n   - all basic types: `1`\r\n   - `Bitlist[N]` and `Bitvector[N]`: `max(1,(N + 255) // 256)`\r\n   - `List[B, N]` and `Vector[B, N]`, where `B` is a basic type: `max(1,(N * size_of(B) + 31) // 32)` \r\n   - `List[C, N]` and `Vector[C, N]`, where `C` is a composite type: `max(1,N)`\r\n   - containers: `len(fields)`\r\n- `pack(value)`: given ordered objects of the same basic type, serialize them, pack them into `BYTES_PER_CHUNK`-byte chunks, right-pad the last chunk with zero bytes, and return the chunks. At least 1 chunk is returned.\r\n- `pack_bits(bits)`: Given the `bits` of bitlist or bitvector, get `bitfield_bytes` by packing them in bytes and aligning to the start. The length-delimiting bit for bitlists is excluded. And then pack `bitfield_bytes` into `BYTES_PER_CHUNK`-byte chunks, right-pad the last chunk with zero bytes, and return the chunks. At least 1 chunk is returned.\r\n- `next_pow_of_two(i)`: get the next power of 2 of `i`, if not already a power of 2, with 0 mapping to 1. Examples: `0->1, 1->1, 2->2, 3->4, 4->4, 6->8, 9->16`\r\n- `merkleize(chunks, limit=None)`: Given ordered `BYTES_PER_CHUNK`-byte chunks, merkleize the chunks, and return the root:\r\n   - The merkleization depends on the effective input, which can be padded/limited:\r\n      - if no limit: pad the `chunks` with zeroed chunks to `next_pow_of_two(len(chunks))` (virtually for memory efficiency).\r\n      - if `limit >= len(chunks)`, pad the chunks with zeroed chunks to `next_pow_of_two(limit)` (virtually for memory efficiency).\r\n      - if `limit < len(chunks)`: do not merkleize, input exceeds limit. Raise an error instead.\r\n   - Then, merkleize the chunks:\r\n      - If `1` chunk: the root is the chunk itself.\r\n      - If `> 1` chunks: merkleize as binary tree.\r\n- `mix_in_length`: Given a Merkle root `root` and a length `length` (`\"uint256\"` little-endian serialization) return `hash(root + length)`.\r\n- `mix_in_type`: Given a Merkle root `root` and a type_index `type_index` (`\"uint256\"` little-endian serialization) return `hash(root + type_index)`.\r\n\r\nWe now define Merkleization `hash_tree_root(value)` of an object `value` recursively:\r\n- `merkleize(pack(value))` if `value` is a basic object or a vector of basic objects.\r\n- `merkleize(pack_bits(value))` if `value` is a bitvector.\r\n- `mix_in_length(merkleize(pack(value), limit=chunk_count(type)), len(value))` if `value` is a list of basic objects.\r\n- `mix_in_length(merkleize(pack_bits(value), limit=chunk_count(type)), len(value))` if `value` is a bitlist.\r\n- `merkleize([hash_tree_root(element) for element in value])` if `value` is a vector of composite objects or a container.\r\n- `mix_in_length(merkleize([hash_tree_root(element) for element in value], limit=chunk_count(type)), len(value))` if `value` is a list of composite objects.\r\n- `mix_in_type(merkleize(value.value), value.type_index)` if `value` is of union type.\r\n",
  "closed_by": {
    "login": "protolambda",
    "id": 19571989,
    "node_id": "MDQ6VXNlcjE5NTcxOTg5",
    "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/protolambda",
    "html_url": "https://github.com/protolambda",
    "followers_url": "https://api.github.com/users/protolambda/followers",
    "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
    "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
    "organizations_url": "https://api.github.com/users/protolambda/orgs",
    "repos_url": "https://api.github.com/users/protolambda/repos",
    "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
    "received_events_url": "https://api.github.com/users/protolambda/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
[
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644697946",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-644697946",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 644697946,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NDY5Nzk0Ng==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-16T11:12:16Z",
    "updated_at": "2020-06-16T11:12:16Z",
    "author_association": "MEMBER",
    "body": "Hey, thank you for looking deeper into this, here are my thoughts:\r\n\r\n> If chunk_count does represent the amount of leafs for merkleization of a type then the formulas for Bitlist[N], List[B,N] and List[C,N] should be amended because any type, including say a List[B,0], requires at least 1 leaf\r\n\r\nHaving 1 leaf is not the same as having 0. 1 leaf would mean a single *usable* node in as the root of the content-tree (i.e. without the mix-in). Whereas a 0 count just means that there will only be padding to go to the next power of 2 (= 1 here). So you get an unusable, always-zero, node as the root of the content-tree. So it should definitely have a 0 minimum count to reflect that. \r\n\r\n> Note that empty bitvectors and vectors are illegal and so their chunk_count will always be at least 1 anyway. Hence the fix can be easily applied using the current grouping of types to specify the various formulas.\r\n\r\nI don't think the fix is necessary, but it can definitely be worded better to avoid confusion.\r\n\r\n>  Explicitly state that the pack and pack_bits functions return at least 1 chunk.\r\n\r\nYes, makes sense. But instead of making the chunk-count return something weird (0 data having 1 chunk), it should just fallback to a zeroed chunk at the very last step when there is no data at all.\r\n\r\n> CASE limit==len(chunks) NOT CONSIDERED\r\n\r\nYes, no need to scream in capitals, it's the easiest case: when it fully fills the subtree, it can be used as-is, and nothing needs to change to the input. \"which *can* be padded/limited: \", in this case, it is not padded or limited. The spec should be more clear about this, but screaming is not the way to change it.\r\n\r\n>  Amend limit > len(chunks) to be limit >= len(chunks)\r\n\r\nThe padding would be a no-op, so I dislike this change. Stating that no padding or limit applied when `limit == len(chunks)` would be enough.\r\n\r\n> For example, a bitlist[N=0] has chunk_count=0 and if pack_bits does return at least 1 chunk (as implied by the wording in the spec) then an error will be raised in the merkleize function because limit < len(chunks)\r\n\r\nYou just introduced this bug if you make chunk count non 0.\r\n\r\n> EMPTY INPUT SHOULDN’T OCCUR\r\n\r\nInstead of screaming at something which is only broken in so far other non-tested fixes are applied, we could work out the edge cases of each function and decide where to clarify or emphasize things in the spec.\r\n\r\n> The limit parameter not required for a bitvector ....\r\n\r\nThe whole spec is trying to navigate a fine line between minimalism and consistency. Limiting everything to a single merkleization function caused this unrequired parameter. But I don't think it's a bug, nor does omitting it clarify much. Instead, maybe we should separate the merkleization description more, to be less densely packed, and avoid things like this.\r\n\r\n> Suggested wording for the merkleization section.\r\n\r\nThank you for suggesting changes, but it really helps if you use a diff instead of copying the full thing over. Or submit a PR. I diffed it manually, and it basically boils down to those few \"minimum 1 chunk\" changes, and omitting that bitvector limit.\r\n\r\n---\r\n\r\nAnd some meta-feedback:\r\nUsing capitals and repeating things until they are hard to parse does not help. It comes over aggressive even. Outline the problem you see, ask why it is like this, and try to be constructive.\r\n\r\nMeanwhile we have an open draft for a more elaborate SSZ spec, and plan to move SSZ out of this eth2 specs repository. See https://github.com/protolambda/eth2.0-ssz/\r\n\r\nThis draft is far from perfect yet, but the intention is to improve it, and give SSZ a place where we can go into depth in the more advanced parts of SSZ (partials, multiproofs, backings) and be more welcoming to changes in description, clarification, etc. Given its current state in phase0 in the specs, I'm avoiding making unnecessary changes here.\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644697946/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644701893",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-644701893",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 644701893,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NDcwMTg5Mw==",
    "user": {
      "login": "booleanfunction",
      "id": 43776922,
      "node_id": "MDQ6VXNlcjQzNzc2OTIy",
      "avatar_url": "https://avatars.githubusercontent.com/u/43776922?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/booleanfunction",
      "html_url": "https://github.com/booleanfunction",
      "followers_url": "https://api.github.com/users/booleanfunction/followers",
      "following_url": "https://api.github.com/users/booleanfunction/following{/other_user}",
      "gists_url": "https://api.github.com/users/booleanfunction/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/booleanfunction/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/booleanfunction/subscriptions",
      "organizations_url": "https://api.github.com/users/booleanfunction/orgs",
      "repos_url": "https://api.github.com/users/booleanfunction/repos",
      "events_url": "https://api.github.com/users/booleanfunction/events{/privacy}",
      "received_events_url": "https://api.github.com/users/booleanfunction/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-16T11:21:19Z",
    "updated_at": "2020-06-16T12:12:21Z",
    "author_association": "CONTRIBUTOR",
    "body": "@protolambda I wanted to sincerely apologise for use of capitals. I didn't realise that their use would be taken that way (I was just thinking they created a different style of heading and so I used them with the intention of formatting to break up the sections). I am still getting used to using github and so I apologise if this caused any offence, I will think more carefully about how I format in the future. Thank you for the meta-feedback :)",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644701893/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644776675",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-644776675",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 644776675,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NDc3NjY3NQ==",
    "user": {
      "login": "franck44",
      "id": 14901362,
      "node_id": "MDQ6VXNlcjE0OTAxMzYy",
      "avatar_url": "https://avatars.githubusercontent.com/u/14901362?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/franck44",
      "html_url": "https://github.com/franck44",
      "followers_url": "https://api.github.com/users/franck44/followers",
      "following_url": "https://api.github.com/users/franck44/following{/other_user}",
      "gists_url": "https://api.github.com/users/franck44/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/franck44/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/franck44/subscriptions",
      "organizations_url": "https://api.github.com/users/franck44/orgs",
      "repos_url": "https://api.github.com/users/franck44/repos",
      "events_url": "https://api.github.com/users/franck44/events{/privacy}",
      "received_events_url": "https://api.github.com/users/franck44/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-16T13:46:47Z",
    "updated_at": "2020-06-16T13:46:47Z",
    "author_association": "NONE",
    "body": "@protolambda \r\nthanks for your feedback. I am not sure using capitals mean screaming. It seems to me that the point is to make it clear what problems are uncovered and to structure the exposition of the issue.\r\n\r\nApart from that, I am surprised that we may _think_ something is a bug or not. It is or it is not.\r\nBut to decide we need a non-ambiguous reference, for instance a **formal specification**, which the Python-like specs are not.\r\n\r\n> This draft is far from perfect yet, but the intention is to improve it, and give SSZ a place where we can go into depth in the more advanced parts of SSZ (partials, multiproofs, backings) and be more welcoming to changes in description, clarification, etc. Given its current state in phase0 in the specs, I'm avoiding making unnecessary changes here.\r\n\r\nThis is extremely confusing to have so many different specifications. Which one is the most reliable one? Why is the new one better than the other ones? If you write a new one, does it mean that the other ones are not satisfactory?\r\nWhen does it end?\r\n\r\nWe are also trying hard to improve and **formally prove** some properties of the specs in [this repo](https://github.com/PegaSysEng/eth2.0-dafny). The objective is to increase confidence in the consistency of the specs by using state-of-the-art formal methods and contribute to the ETH ecosystem.\r\nThe issues we have uncovered are non-trivial, and reveal a number of inconsistencies (including in some implementations like py-ssz). \r\nOur specification is the **ONLY** (no screaming, highlighting) formal version of the specs with clear definitions and mathematical (machine-checkable) **correctness proofs** (the K-specs version does not provide formal proofs, just a semantics of the specs). \r\nThe analysis above (that seems to be dismissed rather quickly) is well articulated, and is the result of extremely careful analysis of the existing specs, and a rigorous (logical) treatment.\r\nThis may deserve some attention.\r\n\r\nAs I mentioned in another issue, the specs would probably benefit from a more formal and thorough treatment rather than another Python-centric (no intended offence here, just a fact) code.\r\nSpecs are supposed to be open and not readable only by Python experts. \r\nFor example, do you think the specs can be used to implement a client in Haskell, Scala, Idris, Agda or any functional language? I am not convinced this is the case. They should probably cater for different tastes and inclination in a programming language.\r\nA structured and rigorous approach like the one we are adopting in our formal specs may have some advantages over sometimes hand-wavy and imprecise explanations. ",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644776675/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644803379",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-644803379",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 644803379,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NDgwMzM3OQ==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-16T14:31:24Z",
    "updated_at": "2020-06-16T14:31:24Z",
    "author_association": "MEMBER",
    "body": "> I am not sure using capitals mean screaming\r\n\r\nI know you all intend well, but using full capitals for something is generally not how someone likes to read a conversation, and is often used for screaming in other contexts.\r\n\r\n> we may think something is a bug or not. It is or it is not.\r\n\r\nThe specification may not be formal enough, and I fully agree we should improve it, but if the intention is the same, it is more of a spec issue than a bug.\r\n\r\n>  Which one is the most reliable one? \r\n\r\nThe one right here in the specs. The other one is a draft.\r\n\r\n> Why is the new one better than the other ones? \r\n\r\nBecause this one is very dense, misses a lot of insight and information in futures that are part of SSZ, but not in phase0, and is taking formal verification to get right.\r\n\r\n> If you write a new one, does it mean that the other ones are not satisfactory?\r\n\r\nThe current one in the spec has been sufficient for now, but when it comes to describing the ideas, principles and advanced features of SSZ, it is incomplete.\r\n\r\n> When does it end?\r\n\r\nSo far the only users are client implementers, who have tests and are using a stable spec for more than half a year. The extension/improvements of the SSZ spec have not been a priority because of this, as there are other more pressing parts in both the specs as the implementation. That said, now that you are giving it a nudge, I am working to update the draft and get it shipped.\r\n\r\n> that seems to be dismissed rather quickly\r\n\r\nSorry, I have not closed the issue, and do not dismiss it. What I am trying to say is that the proposed solution is not improving the spec beyond fitting your interpretation in formally correct way. What I like to achieve is to improve it to a point where it is clear to someone without digging through it formally.\r\n\r\n> As I mentioned in another issue, the specs would probably benefit from a more formal and thorough treatment\r\n\r\nI agree and encourage the effort, but with the throughput of issues and implementation details in the past, producing tests, and keeping it readable for developers, the python code had a lot of value.\r\n\r\n> For example, do you think the specs can be used to implement a client in Haskell, Scala, Idris, Agda or any functional language? I am not convinced this is the case.\r\n\r\nI like Scala and Haskell, but the majority of developers are looking at implemention in other languages. We still try to ignore as many python-specific features. Things like `sum` are clear and readable. But things like a metaclass, or even a regular class with methods, do not have a place in the spec for this reason.\r\n\r\nAnd regarding the related `assert` issue, I am fan of separate verification conditions first as well. However, it has been avoided to not have throw verbose exceptions everywhere (which would only make the spec more like an implementation), and keeping the benefits of an  `assert` during debugging (lots of extra information is captured, it's not just a boolean).\r\n\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/644803379/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645074492",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-645074492",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 645074492,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NTA3NDQ5Mg==",
    "user": {
      "login": "franck44",
      "id": 14901362,
      "node_id": "MDQ6VXNlcjE0OTAxMzYy",
      "avatar_url": "https://avatars.githubusercontent.com/u/14901362?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/franck44",
      "html_url": "https://github.com/franck44",
      "followers_url": "https://api.github.com/users/franck44/followers",
      "following_url": "https://api.github.com/users/franck44/following{/other_user}",
      "gists_url": "https://api.github.com/users/franck44/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/franck44/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/franck44/subscriptions",
      "organizations_url": "https://api.github.com/users/franck44/orgs",
      "repos_url": "https://api.github.com/users/franck44/repos",
      "events_url": "https://api.github.com/users/franck44/events{/privacy}",
      "received_events_url": "https://api.github.com/users/franck44/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-17T00:15:31Z",
    "updated_at": "2020-06-17T00:15:31Z",
    "author_association": "NONE",
    "body": "> The specification may not be formal enough, and I fully agree we should improve it, but if the intention is the same, it is more of a spec issue than a bug.\r\n\r\nI must admit that it is hard for me to see what the difference is between a spec issue and a bug.\r\nThe system can crash because of a _spec issue_, correct? \r\nThe specs contain bugs (I can bet some ETH on it), and it is useful to try and identify them early.\r\nSome bugs are even related to type-checking (and we reported a few) and that should be fixed as it is ... a bug.\r\nThis current issue raises some concerns about genuine inconsistencies in the Merkleise specs.\r\nThese problems do show up when we analyse the specs with rigorous techniques.\r\n\r\n> I like Scala and Haskell, but the majority of developers are looking at implemention in other languages. \r\n\r\nWhat IOHK is doing is probably a counter-example to this claim.\r\nAnd I would not venture into guessing what the majority of developers do, and think that the majority is right. There were a few critical issues in the past on some Ethereum chains, so we may be more humble regarding the way we approach things.\r\nAgain, a spec should be open and usable by any developer irrespective of the programming language of choice. This is actually the main feature of a spec: it tells you **what** has to be computed/achieved but gives the free choice of **how** to compute it.\r\n\r\n> We still try to ignore as many python-specific features. Things like sum are clear and readable. But things like a metaclass, or even a regular class with methods, do not have a place in the spec for this reason.\r\n\r\nYou may have look at this part of the specs in `on_block`:\r\n```\r\n# Make a copy of the state to avoid mutability issues\r\n    assert block.parent_root in store.block_states\r\n    pre_state = store.block_states[block.parent_root].copy()\r\n```\r\n\r\nIf this `copy()` is not Python-centric I am rather confused. \r\nThe specs as they are now written mix a lot of Python-idioms and in my opinion this is detrimental to clarity.\r\nFor instance, the potential mutability issue in the comment is not supported by any concrete justification. Just a sort of an arbitrary choice because the author something may cause a problem. \r\n\r\nThe specs contain a lot of clever ideas mixed with low-level implementation details that, in my opinion, act as noise.\r\nMoreover, they often do not define **what** has to be computed but provide an algorithm to compute a result. This is very different to a specification.\r\nAgain an SSZ example is [the encoding of Bitlists](https://github.com/ethereum/eth2.0-specs/blob/dev/ssz/simple-serialize.md#bitlistn)\r\n```\r\narray = [0] * ((len(value) // 8) + 1)\r\nfor i in range(len(value)):\r\n    array[i // 8] |= value[i] << (i % 8)\r\narray[len(value) // 8] |= 1 << (len(value) % 8)\r\nreturn bytes(array)\r\n```\r\n\r\nApart from providing a recipe to compute something (based off the `bytes` Python function) this does not provide much insight.\r\n[Here](https://github.com/PegaSysEng/eth2.0-dafny/blob/546c2d9974aff7cd7c99aa2772c6ff95f09d5b4f/src/dafny/ssz/BitListSeDes.dfy#L62) we have provided a functional definition i.e. **what** should be computed rather than how.\r\nThis is probably more of a spec than the algorithm above.\r\nAnd this is not at odds with software development best practices: some comments to explain what is happening, a recursive definition to explain **what** the encoding should be. \r\n\r\nThe simple fact that every other month a new SSZ spec pops up (more precise, more refined, but always Python-centric) is a probably a sign that Python is not the only avenue for writing a spec.\r\nAgain in my opinion, this is a questionable choice, I have never seen Python used to write specifications. \r\n\r\nOverall, I think diversity (and the work we are doing writing a formal spec in Dafny is part of it) will benefit the specs. \r\nOur intent is to make it work and to provide a high-level of quality and reliability. The Eth2.0 is a critical system and it is important to get it right.\r\nThe comments and proposals at the beginning of this issue are justified and identify real problems and propose validated fixes. \r\n\r\n\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645074492/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645092972",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-645092972",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 645092972,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NTA5Mjk3Mg==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-17T01:26:57Z",
    "updated_at": "2020-06-17T01:26:57Z",
    "author_association": "MEMBER",
    "body": "> If this copy() is not Python-centric I am rather confused.\r\n\r\nGood point, copying is some kind of edge-case, since there is no clear way of doing it otherwise. We would have to use `deepcopy` (very inefficient, but not important), or specify a `copy(v)->v'` helper function somewhere. This is not the kind of method call I'm thinking about when separating python from specs though. It's readable and simple enough. Defining something more eth2-specific as a method call has much bigger (negative) impact on the spec language subset. If you think we should have a copy helper function instead of this call then we can do that, but this is the first time I have heard `.copy()` as a bad example, so I would rather leave it for what it is.\r\n\r\n> Again an SSZ example is the encoding of Bitlists\r\n\r\nYes, I dislike that too, it is very implementation-ish, whereas it could be described in a readable but clear way. However, description comes with a burden of wording every ambiguity, whereas implementation always does one thing. In this sense we may have been lazy as spec writers, but at least client implementers could work with it.\r\nSee the new SSZ repo draft for a more description-focused spec, and I welcome any fix or note you can add to that.\r\n\r\n> The simple fact that every other month a new SSZ spec pops up\r\n\r\nThis is not true. There have been a few minor issues that have been resolved, but SSZ has barely changed for almost a year (with the exception of removing functionality, such as signing-root).\r\n\r\nWhat I am trying to achieve however, is to slowly push that SSZ draft spec forward, while working on many other things, and keeping the current spec usable for implementers (which are long past implementation of SSZ, and mostly focused on latest v0.12 changes and testnets). And then ship the new SSZ spec for future use, hopefully resolving many of these ugly implementation-like issues.\r\n\r\n> Overall, I think diversity ... will benefit the specs.\r\n\r\nReally, you don't have to defend it for me, I am supportive of your effort, but I'd like the fruits of the effort to benefit the readability and formal correctness, instead of twiddling something somewhere (even if it actually fixes a bug), where we need to reference the issue as implementer to understand the change fully. Especially with SSZ, we reached a point where the current spec is too dense, and we could do lots of changes without an implementer fully understanding them. I hope you understand the desire for a new SSZ spec, and we get those bugfixes merged with their full detail :+1: \r\n\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645092972/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645098836",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-645098836",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 645098836,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NTA5ODgzNg==",
    "user": {
      "login": "franck44",
      "id": 14901362,
      "node_id": "MDQ6VXNlcjE0OTAxMzYy",
      "avatar_url": "https://avatars.githubusercontent.com/u/14901362?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/franck44",
      "html_url": "https://github.com/franck44",
      "followers_url": "https://api.github.com/users/franck44/followers",
      "following_url": "https://api.github.com/users/franck44/following{/other_user}",
      "gists_url": "https://api.github.com/users/franck44/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/franck44/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/franck44/subscriptions",
      "organizations_url": "https://api.github.com/users/franck44/orgs",
      "repos_url": "https://api.github.com/users/franck44/repos",
      "events_url": "https://api.github.com/users/franck44/events{/privacy}",
      "received_events_url": "https://api.github.com/users/franck44/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-17T01:48:58Z",
    "updated_at": "2020-06-17T01:48:58Z",
    "author_association": "NONE",
    "body": "> What I am trying to achieve however, is to slowly push that SSZ draft spec forward, while working on many other things, and keeping the current spec usable for implementers (which are long past implementation of SSZ, and mostly focused on latest v0.12 changes and testnets). And then ship the new SSZ spec for future use, hopefully resolving many of these ugly implementation-like issues.\r\n\r\nOur verification effort goes beyond all the testing work that has been going on related to the specs.\r\nIt is a several-month 3 person effort that is building on state-of-the-art techniques and tools.\r\nIt is the only formal, documented, readable, proved and machine-checkable specification.\r\n\r\n>  I hope you understand the desire for a new SSZ spec,\r\n\r\nI'll be honest: no I don't understand. \r\nWe may not need yet a new (informal, ambiguous) one. Just one that is formal and readable, and ours is a good candidate, yet it seems to be completely ignored.\r\n\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645098836/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645355422",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-645355422",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 645355422,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0NTM1NTQyMg==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-17T12:54:04Z",
    "updated_at": "2020-06-17T12:54:04Z",
    "author_association": "MEMBER",
    "body": ">  no I don't understand.\r\n\r\n@franck44 See my comment in the other thread: https://github.com/ethereum/eth2.0-specs/issues/1901#issuecomment-645354568",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/645355422/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/648516511",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-648516511",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 648516511,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0ODUxNjUxMQ==",
    "user": {
      "login": "booleanfunction",
      "id": 43776922,
      "node_id": "MDQ6VXNlcjQzNzc2OTIy",
      "avatar_url": "https://avatars.githubusercontent.com/u/43776922?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/booleanfunction",
      "html_url": "https://github.com/booleanfunction",
      "followers_url": "https://api.github.com/users/booleanfunction/followers",
      "following_url": "https://api.github.com/users/booleanfunction/following{/other_user}",
      "gists_url": "https://api.github.com/users/booleanfunction/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/booleanfunction/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/booleanfunction/subscriptions",
      "organizations_url": "https://api.github.com/users/booleanfunction/orgs",
      "repos_url": "https://api.github.com/users/booleanfunction/repos",
      "events_url": "https://api.github.com/users/booleanfunction/events{/privacy}",
      "received_events_url": "https://api.github.com/users/booleanfunction/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-24T00:44:19Z",
    "updated_at": "2020-06-24T00:44:19Z",
    "author_association": "CONTRIBUTOR",
    "body": "@protolambda thank you for the detailed feedback. I wanted to make sure that I gave myself a bit of time to carefully think about it but if it is ok I think it would really help me to clarify a few things. Originally I wasn't sure how to break up some of my queries but hopefully they will make sense :)\r\n\r\nCan I please check, in the first step of the merkleization function:\r\n\r\n>merkleize(chunks, limit=None): Given ordered BYTES_PER_CHUNK-byte chunks, merkleize the chunks, and return the root:\r\n>- The merkleization depends on the effective input, which can be padded/limited:\r\n>   - if no limit: pad the chunks with zeroed chunks to next_pow_of_two(len(chunks)) (virtually for memory efficiency).\r\n>   - if limit > len(chunks), pad the chunks with zeroed chunks to next_pow_of_two(limit) (virtually for memory efficiency).\r\n>   -  if limit < len(chunks): do not merkleize, input exceeds limit. Raise an error instead.\r\n\r\n1. Is the idea that by the end of it we should have a number of chunks that is equal to a power of 2? Or is it still ok not to have a power of 2 chunks at this point and instead correct for this when creating the binary tree, if needed?\r\n\r\nThe context for my question is that I think I may have misunderstood the intention of this step of the merkelization function. ie. I assumed that there would be a power of 2 chunks by the end of this first step. \r\n\r\nYou mentioned in a reply above, in talking about the `limit==len(chunks)` case:\r\n\r\n>it's the easiest case: when it fully fills the subtree, it can be used as-is, and nothing needs to change to the input. \"which can be padded/limited: \", in this case, it is not padded or limited. \r\n\r\nI was thinking about say, a vector[uint8,96]. When merkleization is called, `limit` would equal its `chunk_count`, which would be 3. Also, as the pack function would create 3 chunks and so len(chunks) would be 3. And so in this example `limit == len(chunks)`.\r\n\r\n2. For this example, is the idea that the first step of the merkleization function would not be implemented, as we have the `limit == len(chunks)` case, but rather just the 3 chunks would be left unchanged and we would go to the second part of the merkleization function? i.e. the part\r\n>Then, merkelize the chunks: \r\n\r\nThank you for your help :)\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/648516511/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/648675720",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-648675720",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 648675720,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0ODY3NTcyMA==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-24T08:25:41Z",
    "updated_at": "2020-06-24T08:25:41Z",
    "author_association": "MEMBER",
    "body": "@booleanfunction The background thay may be missing here is that subtrees of a power of 2, and the approach to padding, have some implications that need to be understood by client implementers. And with the current minimal spec, the prose does not really cover it. The amount of chunks is something that no (production) implementation should actually pad by adding a whole lot of zeroed chunks at the bottom level.\r\n\r\nE.g. if you have a list subtree with 1024 chunks as limit, but 513 in chunk length, then you would have 511 chunks of padding.\r\nFor spec purposes this is fine, but I want the spec to at least hint that it was designed to be done differently: you only need the top nodes in the subtrees of the padding. So when you are padding 511 chunks, you really only need 9 nodes in the tree: `1+2+4+8+16+32+64+128+256=511`.\r\n\r\nAnd since these subtrees in the padding are all zeroed, these values can be precomputed efficiently (`Z[i] = H(Z[i-1], Z[i-1]`). This is what the \"virtually for memory efficiency\" is trying to say.\r\nFor a type like the validator registry, with a high limit of `2**40`, this feature is actually really important.\r\n\r\n\r\nI see now how the `==` part is also misleading: \"which *can* be padded/limited: \" (emphasis mine): we should indeed add a `==` case for clarity, but what really happens here is that since the `length` and `limit` are the same, nothing much happens if it's a power of two already. But if not, like you pointed out (thanks!) it should be clarified that it would involve padding to the next power of two. (which will always be zero due to the limit restricting the actual values).\r\n\r\nAnd then in the original issue we have the question if `pack` and `pack_bits` should return at least one chunk or not. I think we should keep the output strictly `(len(input bytes) + 31) // 32` chunks. Whatever shape or form the input has, it gets packed into chunks of 32 bytes each, independent of its later purpose or context. So 0 input would be 0 chunks.\r\n\r\nWhen `pack` is actually used to build the bottom of the tree, the (maybe virtual) padding of chunks should be handled as part of the merkleization setup. An empty tree has no data, and I think it's best to regard it as just another case where you would fill the void with zeroes. If `limit == 0`, then the `len(chunks) == limit` applies. And if the `limit` is higher, then we fill automatically with the appropriate amount of zeroes (and with the virtual padding thing, this becomes a matter of simply looking up a precomputed value). \r\n\r\nAlso, some minor improvements would be to describe the pad input as `values`, and define `pack_bits` in terms of `pack`.\r\n\r\nSo what we could redefine `pack(value)` as:\r\n- `pack(values)`: given ordered objects of the same basic type:\r\n  1. Serialize the values into bytes\r\n  2. If not aligned to a multiple of `BYTES_PER_CHUNK` bytes, right-pad with zeroes to the next multiple.\r\n  2. Partition the bytes into `BYTES_PER_CHUNK`-byte chunks\r\n  3. Return the chunks.\r\n\r\nAnd then we can define `pack_bits(bits)` as:\r\n- `pack_bits(bits)`: Given the bits of bitlist or bitvector, get `bitfield_bytes` by packing them in bytes and aligning to the start. The length-delimiting bit for bitlists is excluded. Then return `pack(bitfield_bytes)`\r\n\r\nI think it's clear that the input of bytes is serialized as-is in the above, and we avoid some duplication of padding logic.\r\n\r\nAnd then update the merkleization to reflect:\r\n- That it must be limited and padded following the rules (remove the \"can\")\r\n- Change the `limit > len` case to `>=` like you proposed\r\n\r\nWhat do you think? Would like to submit another PR? Alternatively I can make it too, but have my hands full with Altona testnet and networking things at the moment.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/648675720/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/649132351",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-649132351",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 649132351,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0OTEzMjM1MQ==",
    "user": {
      "login": "booleanfunction",
      "id": 43776922,
      "node_id": "MDQ6VXNlcjQzNzc2OTIy",
      "avatar_url": "https://avatars.githubusercontent.com/u/43776922?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/booleanfunction",
      "html_url": "https://github.com/booleanfunction",
      "followers_url": "https://api.github.com/users/booleanfunction/followers",
      "following_url": "https://api.github.com/users/booleanfunction/following{/other_user}",
      "gists_url": "https://api.github.com/users/booleanfunction/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/booleanfunction/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/booleanfunction/subscriptions",
      "organizations_url": "https://api.github.com/users/booleanfunction/orgs",
      "repos_url": "https://api.github.com/users/booleanfunction/repos",
      "events_url": "https://api.github.com/users/booleanfunction/events{/privacy}",
      "received_events_url": "https://api.github.com/users/booleanfunction/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-24T23:48:11Z",
    "updated_at": "2020-06-24T23:48:11Z",
    "author_association": "CONTRIBUTOR",
    "body": "@protolambda that sounds great and yes, I would be happy to do the PR (it will give me another chance to go through the process :) I will organise the PR today using your suggestions.\r\n\r\nAnd thank you for linking this to pack/pack_bits as well, I was still thinking about that part too! :)\r\n\r\n> And then in the original issue we have the question if pack and pack_bits should return at least one chunk or not. I think we should keep the output strictly (len(input bytes) + 31) // 32 chunks. Whatever shape or form the input has, it gets packed into chunks of 32 bytes each, independent of its later purpose or context. So 0 input would be 0 chunks.\r\n\r\nI like this, it keeps things nice and simple. I was thinking about my original dilemma about wording for pack/pack_bits and the number of chunks that they return and well, having more understanding now as to the background/intention in conjunction with these clarifications definitely helps to make it more consistent. I think when I saw that in **py-ssz** one empty chunk is returned from pack for a `List[B, 0]` it previously convinced me that it was _always at least 1_ but from a spec perspective, the consistency that comes from making the output from pack/pack_bits match to the corresponding chunk_count (and so 0 input would be 0 chunks) is great. \r\n\r\nAnyway, thank you also for the extra detail relating to an implementation perspective. I am definitely more what I would term a mathematical programmer, rather than a software developer, but I like thinking about optimisation of algorithms and so any extra knowledge I can gain regarding the implementation is much appreciated. One of the things I think is great about our formal verification project is that while we are aiming to keep the core version as simple as possible (and align with the spec as written), we can also set up implementation/optimised versions of functions and definitively prove they are equivalent - which I think is very cool!\r\n\r\nThank you again",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/649132351/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/663125650",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1893#issuecomment-663125650",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1893",
    "id": 663125650,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY2MzEyNTY1MA==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-07-23T17:10:18Z",
    "updated_at": "2020-07-23T17:10:18Z",
    "author_association": "MEMBER",
    "body": "Closing this issue, I think #1934 covers all of this. If anything was missed, let's move that into a new issue. Thanks for making the PR :+1: ",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/663125650/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
