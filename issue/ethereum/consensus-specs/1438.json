{
  "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438",
  "repository_url": "https://api.github.com/repos/ethereum/consensus-specs",
  "labels_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438/labels{/name}",
  "comments_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438/comments",
  "events_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438/events",
  "html_url": "https://github.com/ethereum/consensus-specs/issues/1438",
  "id": 509388155,
  "node_id": "MDU6SXNzdWU1MDkzODgxNTU=",
  "number": 1438,
  "title": "Generating data availability roots from shard block roots",
  "user": {
    "login": "vbuterin",
    "id": 2230894,
    "node_id": "MDQ6VXNlcjIyMzA4OTQ=",
    "avatar_url": "https://avatars.githubusercontent.com/u/2230894?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/vbuterin",
    "html_url": "https://github.com/vbuterin",
    "followers_url": "https://api.github.com/users/vbuterin/followers",
    "following_url": "https://api.github.com/users/vbuterin/following{/other_user}",
    "gists_url": "https://api.github.com/users/vbuterin/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/vbuterin/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/vbuterin/subscriptions",
    "organizations_url": "https://api.github.com/users/vbuterin/orgs",
    "repos_url": "https://api.github.com/users/vbuterin/repos",
    "events_url": "https://api.github.com/users/vbuterin/events{/privacy}",
    "received_events_url": "https://api.github.com/users/vbuterin/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1170173759,
      "node_id": "MDU6TGFiZWwxMTcwMTczNzU5",
      "url": "https://api.github.com/repos/ethereum/consensus-specs/labels/phase1",
      "name": "phase1",
      "color": "F7C242",
      "default": false,
      "description": ""
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2019-10-19T03:46:32Z",
  "updated_at": "2019-10-20T09:21:39Z",
  "closed_at": null,
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "body": "In a beacon block, we have N data roots `r[1] ... r[n]` (in the worst case, N = 256), each of which represent data `d[1] ... d[n]` of different sizes: on average 128 kB, in the worst case 512 kB. We want to combine these roots into a combined root `R = f(r[1] ... r[n])` which is a Merkle root of some kind of concatenation of `d[1] ...d[n]`.\r\n\r\nThe main desiderata are:\r\n\r\n* `R` can be computed from `r[1] ... r[n]` either directly or without adding much auxiliary data\r\n* Minimal \"wasted space\" in `R` (consider that if we force all `r[i]` to have the same height, on average there will be 75% wasted space)\r\n\r\nPossibilities I see:\r\n\r\n1. Allow `r[i]` to have different heights depending on its size, so there's on average ~30% maximum 49% wasted space. Then create R by combining all `r[i]` values of each size, adding a bit of padding if needed and then combining the different sizes. Main weakness is that variable heights are not SSZ-friendly.\r\n2. Expose a maximum of 4 data sub-roots where `r[i] = root(r[i][0], r[i][1], r[i][2], r[i][3])`. Reduces wasted space even further, and is cleaner (including being SSZ compliant), but increases number of block hashes from 1 to average 1.5 max 4, increasing beacon chain load by 1.2x average-case and 2.33x worst case.\r\n3. Expose 3 data sub-roots where `r[i][0]` is the root of the first half of the data if the data is more than half the max size and otherwise zero, `r[i][1]` is the root of the first quarter of the data not covered by `r[i][0]`, and `r[i][2]` is the root of the first eighth of the data not covered by `r[i][1]`. Reduces wasted space heavily (maybe ~10% wasted), but increases beacon chain load ~1.89x, and is more complicated (including non-SSZ-friendliness from (1)).\r\n\r\nFor (1) here is an example algorithm:\r\n\r\n```python\r\ndef combine_roots(roots: List[Hash], sizes: List[int]) -> Hash:\r\n    roots = []\r\n    cur_size = 1\r\n    ZERO_HASH = b'\\x00' * 32\r\n    while cur_size < max(sizes) * 2 or len(roots) > 1:\r\n        if len(roots) % 2:\r\n            roots.append(ZERO_HASH)\r\n        roots = [hash(roots[i] + roots[i+1]) for i in range(0, len(roots), 2)]\r\n        ZERO_HASH = hash(ZERO_HASH + ZERO_HASH)\r\n        roots.extend([roots[i] for i in range(len(roots)) if cur_size//2 < sizes[i] <= cur_size])\r\n    return roots[0]\r\n```\r\n\r\nFor (2), here are concrete estimates of data overhead. With 64 shards, and a MAX_CATCHUP_RATIO of 4, and 4x slack for different attestations, there are a maximum of 1024 data roots in a block, plus 1024 state roots and 1024 lengths (that's 32 + 32 + 8 = 72 kB). Proposal (2) would increase the former to 1536 data roots hence 48 + 32 + 8 = 88 kB average case and 160 + 32 + 8 = 168 kB worst case. Note that average-case figures are expected to be ~8x less than these worst-case figures.\r\n\r\nI am currently favoring (2) but open to other ideas.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/544216396",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1438#issuecomment-544216396",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438",
    "id": 544216396,
    "node_id": "MDEyOklzc3VlQ29tbWVudDU0NDIxNjM5Ng==",
    "user": {
      "login": "dankrad",
      "id": 6130607,
      "node_id": "MDQ6VXNlcjYxMzA2MDc=",
      "avatar_url": "https://avatars.githubusercontent.com/u/6130607?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/dankrad",
      "html_url": "https://github.com/dankrad",
      "followers_url": "https://api.github.com/users/dankrad/followers",
      "following_url": "https://api.github.com/users/dankrad/following{/other_user}",
      "gists_url": "https://api.github.com/users/dankrad/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/dankrad/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/dankrad/subscriptions",
      "organizations_url": "https://api.github.com/users/dankrad/orgs",
      "repos_url": "https://api.github.com/users/dankrad/repos",
      "events_url": "https://api.github.com/users/dankrad/events{/privacy}",
      "received_events_url": "https://api.github.com/users/dankrad/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-10-20T03:14:14Z",
    "updated_at": "2019-10-20T03:14:14Z",
    "author_association": "MEMBER",
    "body": "I think (2) sounds very reasonable. Could we save a lot of beacon chain load by making the target shard block size just a little bit smaller than 128 kB, say something in the range of 100-120 kB? I we are expecting that the distribution around the target does not have a huge variance, then this would make a lot fewer of the second data roots necessary (not sure if there is an obvious way to get the expected distribution from EIP1559?)\r\n\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/544216396/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/544235234",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1438#issuecomment-544235234",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1438",
    "id": 544235234,
    "node_id": "MDEyOklzc3VlQ29tbWVudDU0NDIzNTIzNA==",
    "user": {
      "login": "vbuterin",
      "id": 2230894,
      "node_id": "MDQ6VXNlcjIyMzA4OTQ=",
      "avatar_url": "https://avatars.githubusercontent.com/u/2230894?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/vbuterin",
      "html_url": "https://github.com/vbuterin",
      "followers_url": "https://api.github.com/users/vbuterin/followers",
      "following_url": "https://api.github.com/users/vbuterin/following{/other_user}",
      "gists_url": "https://api.github.com/users/vbuterin/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/vbuterin/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/vbuterin/subscriptions",
      "organizations_url": "https://api.github.com/users/vbuterin/orgs",
      "repos_url": "https://api.github.com/users/vbuterin/repos",
      "events_url": "https://api.github.com/users/vbuterin/events{/privacy}",
      "received_events_url": "https://api.github.com/users/vbuterin/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2019-10-20T09:21:39Z",
    "updated_at": "2019-10-20T09:21:39Z",
    "author_association": "MEMBER",
    "body": "I do expect the distribution to have a considerable variance! Sure, if you assume it's a normal distribution, on average a block has ~120 txs so the standard deviation should be ~10, but that ignores uneven sizes of transactions, multi-transactions, etc. If I had to guess I'd expect the 25-75 confidence interval to be ~0.6-1.5 times the average.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/544235234/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
