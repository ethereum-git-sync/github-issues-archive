{
  "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
  "repository_url": "https://api.github.com/repos/ethereum/consensus-specs",
  "labels_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819/labels{/name}",
  "comments_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819/comments",
  "events_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819/events",
  "html_url": "https://github.com/ethereum/consensus-specs/issues/1819",
  "id": 620296891,
  "node_id": "MDU6SXNzdWU2MjAyOTY4OTE=",
  "number": 1819,
  "title": "Snappy frames delimiter wanted",
  "user": {
    "login": "Nashatyrev",
    "id": 8173857,
    "node_id": "MDQ6VXNlcjgxNzM4NTc=",
    "avatar_url": "https://avatars.githubusercontent.com/u/8173857?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/Nashatyrev",
    "html_url": "https://github.com/Nashatyrev",
    "followers_url": "https://api.github.com/users/Nashatyrev/followers",
    "following_url": "https://api.github.com/users/Nashatyrev/following{/other_user}",
    "gists_url": "https://api.github.com/users/Nashatyrev/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/Nashatyrev/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Nashatyrev/subscriptions",
    "organizations_url": "https://api.github.com/users/Nashatyrev/orgs",
    "repos_url": "https://api.github.com/users/Nashatyrev/repos",
    "events_url": "https://api.github.com/users/Nashatyrev/events{/privacy}",
    "received_events_url": "https://api.github.com/users/Nashatyrev/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1295438707,
      "node_id": "MDU6TGFiZWwxMjk1NDM4NzA3",
      "url": "https://api.github.com/repos/ethereum/consensus-specs/labels/scope:networking",
      "name": "scope:networking",
      "color": "F596AA",
      "default": false,
      "description": ""
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 7,
  "created_at": "2020-05-18T15:21:23Z",
  "updated_at": "2020-06-25T12:07:13Z",
  "closed_at": "2020-06-25T11:19:05Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "body": "Currently it is a bit tricky from the implementation point of view to uncompress snappy frames in the response stream. For example we get a response with several chunks:\r\n```\r\n| result-1 | sszLength-1 | snappyFrame-1-1 | snappyFrame-1-2 | result-2 | sszLength-2 | snappyFrame-2-1 | ...\r\n```\r\nWithout hacking into the Snappy frame format (Snappy frames has length prefix) we can't figure out how many bytes we should take from the stream to feed to the Snappy decompressor. \r\n\r\nMay be it makes sense to length-prefix every Snappy frame explicitly to avoid error-prone implementation tricks?",
  "closed_by": {
    "login": "Nashatyrev",
    "id": 8173857,
    "node_id": "MDQ6VXNlcjgxNzM4NTc=",
    "avatar_url": "https://avatars.githubusercontent.com/u/8173857?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/Nashatyrev",
    "html_url": "https://github.com/Nashatyrev",
    "followers_url": "https://api.github.com/users/Nashatyrev/followers",
    "following_url": "https://api.github.com/users/Nashatyrev/following{/other_user}",
    "gists_url": "https://api.github.com/users/Nashatyrev/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/Nashatyrev/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Nashatyrev/subscriptions",
    "organizations_url": "https://api.github.com/users/Nashatyrev/orgs",
    "repos_url": "https://api.github.com/users/Nashatyrev/repos",
    "events_url": "https://api.github.com/users/Nashatyrev/events{/privacy}",
    "received_events_url": "https://api.github.com/users/Nashatyrev/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
[
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/630973293",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1819#issuecomment-630973293",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
    "id": 630973293,
    "node_id": "MDEyOklzc3VlQ29tbWVudDYzMDk3MzI5Mw==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-05-19T17:37:05Z",
    "updated_at": "2020-05-19T17:37:05Z",
    "author_association": "MEMBER",
    "body": "Your approach to consume the snappy frames can be changed to make it work. Other clients do this, even in javascript, and it can work fine, without API changes to a proper snappy library.\r\n\r\nYou read the SSZ length, then read that many bytes from the Snappy reader (assuming you've snappy framed format exposed as with a streaming API). And snappy should only read more frames when more SSZ bytes are requested than available from the currently buffered frame. More frames could be buffered before returning the decompressed bytes in a stream-read call, if the requested ssz byte length is big enough to warrant that. Then just loop the read call until you get the full N ssz bytes, and at the same time you can decode them into the SSZ datastructures. SSZ isn't technically a streaming format, but since the SSZ length is already known, a decoder can build the datastructure without buffering the full bytes first!\r\n\r\nSimilarly, for encoding you can compute the size efficiently first, then write to the (snappy compressed) stream, without buffering the full ssz bytes. And snappy writes the completed frames to the network output, while maintaining a single frame. And after encoding the ssz object to the stream, you should then flush it to complete the last frame (snappy compressor could still be waiting for more bytes to put in the last frame, hence the need to flush before closing it), and you have a full streaming (as good as possible here) compressed SSZ protocol!\r\n\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/630973293/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631402544",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1819#issuecomment-631402544",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
    "id": 631402544,
    "node_id": "MDEyOklzc3VlQ29tbWVudDYzMTQwMjU0NA==",
    "user": {
      "login": "Nashatyrev",
      "id": 8173857,
      "node_id": "MDQ6VXNlcjgxNzM4NTc=",
      "avatar_url": "https://avatars.githubusercontent.com/u/8173857?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Nashatyrev",
      "html_url": "https://github.com/Nashatyrev",
      "followers_url": "https://api.github.com/users/Nashatyrev/followers",
      "following_url": "https://api.github.com/users/Nashatyrev/following{/other_user}",
      "gists_url": "https://api.github.com/users/Nashatyrev/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Nashatyrev/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Nashatyrev/subscriptions",
      "organizations_url": "https://api.github.com/users/Nashatyrev/orgs",
      "repos_url": "https://api.github.com/users/Nashatyrev/repos",
      "events_url": "https://api.github.com/users/Nashatyrev/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Nashatyrev/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-05-20T10:56:41Z",
    "updated_at": "2020-05-20T10:56:41Z",
    "author_association": "MEMBER",
    "body": "> Your approach to consume the snappy frames can be changed to make it work. Other clients do this, even in javascript, and it can work fine, without API changes to a proper snappy library.\r\n\r\nI didn't mean it's impossible but it requires some extra efforts from the client implementation and  requires some specific capabilities from Snappy library. This seems to me like unnecessary over-complexity and can be implementation error-prone \r\n\r\nLet me dive a bit deeper with the assumption that the whole response can be fragmented in any arbitrary fashion.\r\n\r\nLet's for example suppose the following response (2 single frame chunks)\r\n```\r\n| result-1 | sszLength-1 | snappyFrame-1 | result-2 | sszLength-2 | snappyFrame-2 |\r\n```\r\n\r\nand it is fragmented and delivered with 2 packets (`ByteFuffer`s or whatever) :\r\n```\r\n( result-1 | sszLength-1 | snappyFr)  // packet #1\r\n(ame-1 | result-2 | sszLength-2 | snappyFrame-2 )  // packet #2\r\n```\r\n(note: you can't reconstruct Eth2 response chunks from these fragments without hacking into Snappy frames format)\r\n\r\n\r\n> You read the SSZ length, then read that many bytes from the Snappy reader (assuming you've snappy framed format exposed as with a streaming API)\r\n\r\nAssuming you are feeding the `packet #1` to an abstract Snappy decompressor and call `read(ssLength_1)`\r\nThere are different possible options which decompressor may or may not support: \r\n- call would block until the rest of the frame (`packet #2`) is fed (blocking API) \r\n- returns maximum it can while keeping some rest state internally \r\n- returns 0 bytes while caching incomplete frame data\r\n- throws error\r\n\r\nAlso when feeding the `packet #2` to the decompressor we need to make sure it doesn't pre-fetch any bytes internally and would read the compressed `stream` (`ByteBuffer` or whatever ) exactly until `| result-2 `\r\n\r\nAnother strong argument is that we would like to process Eth2 chunks (either with Snappy compression or not) at a higher level without dealing with packet fragmentation issues.\r\n\r\nJust checked the Lighthouse code https://github.com/sigp/lighthouse/blob/master/beacon_node/eth2-libp2p/src/rpc/codec/ssz_snappy.rs#L260-L262 and I'm not sure if it would work in case of fragmentation described above. \r\nI'm suspecting other libraries may potentially have issues in case of fragmentation and just work fine cause it's not happening with the current Libp2p muxer implementation\r\n\r\nAgain I'm not saying it's impossible to implement the current protocol but why do we need to impose those complexities when we are able to simplify the things? \r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631402544/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631467416",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1819#issuecomment-631467416",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
    "id": 631467416,
    "node_id": "MDEyOklzc3VlQ29tbWVudDYzMTQ2NzQxNg==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-05-20T13:17:12Z",
    "updated_at": "2020-05-20T13:17:12Z",
    "author_association": "MEMBER",
    "body": "> (note: you can't reconstruct Eth2 response chunks from these fragments without hacking into Snappy frames format\r\n\r\nIf it's split like that, and you can't read the second part, then you can't reconstruct a response chunk regardless. Not a problem specific to snappy.\r\n\r\n> Also when feeding the packet #2 to the decompressor we need to make sure it doesn't pre-fetch any bytes internally and would read the compressed stream (ByteBuffer or whatever ) exactly until | result-2 \r\n\r\nI think that is just a design problem of the implementation. It should not be \"feeding\", the decompression should ask for the data from the feeder. Otherwise you have to understand the contents (expected ssz byte length) on the producer side, instead of the consumer side. The \"feeding\" is prone to pre-fetching indeed if you have no length information, and just a bad idea, since reading bytes from a stream that is not supposed to return them is unexpected bad behavior. I would keep it simple and memory efficient: have the snappy library do the work, and ask for the bytes it needs, not the bytes you guess it could need.\r\n\r\nAlso, if we start encoding the full compressed length we can't do streaming-encoding anymore, as we have to compute it ahead of time. That would mean compressing everything first, and if an RPC chunk is very large that can be a problem.\r\n\r\n> Just checked the Lighthouse code\r\n\r\nIt should work fine, was tested with rumor first, then plenty of use in production (schlesi). And soon network tests will help too, but I think this part of lighthouse RPC is good. However, what lighthouse does not yet do is to decode the bytes directly into a data structure. Instead, it buffers the full ssz-length worth of bytes, and only then encodes it. No problem, that can be improved later on their side. The idea really is that if we had bigger RPC chunks (E.g. beacon states, or later phase 1 things), we can support them with the current RPC without problems.\r\n\r\n> Again I'm not saying it's impossible to implement the current protocol but why do we need to impose those complexities when we are able to simplify the things?\r\n\r\nYes, I understand it's not easy, but I do think it has its benefits. We reduce memory (so far a bigger problem in clients than CPU or network io), support large chunks, enable better compression within a chunk (can have multiple frame types), and streaming in both encoding and decoding (except for length computation, but that can be very cheap).\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631467416/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631506632",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1819#issuecomment-631506632",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
    "id": 631506632,
    "node_id": "MDEyOklzc3VlQ29tbWVudDYzMTUwNjYzMg==",
    "user": {
      "login": "Nashatyrev",
      "id": 8173857,
      "node_id": "MDQ6VXNlcjgxNzM4NTc=",
      "avatar_url": "https://avatars.githubusercontent.com/u/8173857?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Nashatyrev",
      "html_url": "https://github.com/Nashatyrev",
      "followers_url": "https://api.github.com/users/Nashatyrev/followers",
      "following_url": "https://api.github.com/users/Nashatyrev/following{/other_user}",
      "gists_url": "https://api.github.com/users/Nashatyrev/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Nashatyrev/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Nashatyrev/subscriptions",
      "organizations_url": "https://api.github.com/users/Nashatyrev/orgs",
      "repos_url": "https://api.github.com/users/Nashatyrev/repos",
      "events_url": "https://api.github.com/users/Nashatyrev/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Nashatyrev/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-05-20T14:24:29Z",
    "updated_at": "2020-05-20T14:24:29Z",
    "author_association": "MEMBER",
    "body": "> I think that is just a design problem of the implementation. It should not be \"feeding\", the decompression should ask for the data from the feeder. Otherwise you have to understand the contents (expected ssz byte length) on the producer side, instead of the consumer side. The \"feeding\" is prone to pre-fetching indeed if you have no length information, and just a bad idea, since reading bytes from a stream that is not supposed to return them is unexpected bad behavior. I would keep it simple and memory efficient: have the snappy library do the work, and ask for the bytes it needs, not the bytes you guess it could need.\r\n\r\nOk there was probably terms misunderstanding...\r\n\r\nLet's see to the lighthouse code in more details to demonstrate my concerns: \r\n\r\nSuppose we have two packets (as described above) where Snappy frame is fragmented\r\n\r\nWhen the `packet #1` arrives I suppose it is immediately passed to this method: \r\n[`fn decode(&mut self, src: &mut BytesMut)`](https://github.com/sigp/lighthouse/blob/master/beacon_node/eth2-libp2p/src/rpc/codec/ssz_snappy.rs#L243)\r\nI'm not sure this code would process it correctly cause the Snappy frame is incomplete: \r\n[`reader.read_exact(&mut decoded_buffer)`](https://github.com/sigp/lighthouse/blob/master/beacon_node/eth2-libp2p/src/rpc/codec/ssz_snappy.rs#L262)\r\n\r\nI think it might be good idea to invite someone from Lighthouse team for comments. \r\n\r\n> Also, if we start encoding the full compressed length we can't do streaming-encoding anymore, as we have to compute it ahead of time. That would mean compressing everything first, and if an RPC chunk is very large that can be a problem.\r\n\r\nThat's why I would suggest to length-prefix every Snappy frame with the length of compressed frame. This should not affect Snappy streaming ability but should allow handling wire packets more explicitly",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631506632/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631645988",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1819#issuecomment-631645988",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
    "id": 631645988,
    "node_id": "MDEyOklzc3VlQ29tbWVudDYzMTY0NTk4OA==",
    "user": {
      "login": "pawanjay176",
      "id": 9890508,
      "node_id": "MDQ6VXNlcjk4OTA1MDg=",
      "avatar_url": "https://avatars.githubusercontent.com/u/9890508?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/pawanjay176",
      "html_url": "https://github.com/pawanjay176",
      "followers_url": "https://api.github.com/users/pawanjay176/followers",
      "following_url": "https://api.github.com/users/pawanjay176/following{/other_user}",
      "gists_url": "https://api.github.com/users/pawanjay176/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/pawanjay176/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/pawanjay176/subscriptions",
      "organizations_url": "https://api.github.com/users/pawanjay176/orgs",
      "repos_url": "https://api.github.com/users/pawanjay176/repos",
      "events_url": "https://api.github.com/users/pawanjay176/events{/privacy}",
      "received_events_url": "https://api.github.com/users/pawanjay176/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-05-20T18:26:13Z",
    "updated_at": "2020-05-20T18:26:13Z",
    "author_association": "NONE",
    "body": "So the way it works in Lighthouse for your example is that the `read_exact` function will read exactly the number of bytes from the underlying snappy reader to fill the `decoded_buffer` with `sszLength-1` bytes. If there aren't enough bytes like in your example, we return `None` which buffers the received bytes in expectation of more bytes to come.\r\n\r\nAfter we receive `packet #2`, we now have enough bytes to fill the `decoded_buffer`, so `read_exact` returns the decompressed bytes which we ssz decode to return the decoded chunk. The second frame would be straightforward now since we have all the bytes needed to decode it.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/631645988/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/649477534",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1819#issuecomment-649477534",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
    "id": 649477534,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0OTQ3NzUzNA==",
    "user": {
      "login": "Nashatyrev",
      "id": 8173857,
      "node_id": "MDQ6VXNlcjgxNzM4NTc=",
      "avatar_url": "https://avatars.githubusercontent.com/u/8173857?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/Nashatyrev",
      "html_url": "https://github.com/Nashatyrev",
      "followers_url": "https://api.github.com/users/Nashatyrev/followers",
      "following_url": "https://api.github.com/users/Nashatyrev/following{/other_user}",
      "gists_url": "https://api.github.com/users/Nashatyrev/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/Nashatyrev/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/Nashatyrev/subscriptions",
      "organizations_url": "https://api.github.com/users/Nashatyrev/orgs",
      "repos_url": "https://api.github.com/users/Nashatyrev/repos",
      "events_url": "https://api.github.com/users/Nashatyrev/events{/privacy}",
      "received_events_url": "https://api.github.com/users/Nashatyrev/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-25T11:19:04Z",
    "updated_at": "2020-06-25T11:19:04Z",
    "author_association": "MEMBER",
    "body": "Well I completed non-blocking implementation in Teku, and I can say that was somewhat challenging. I needed to carefully handle  possible packets fragmentations and sticking through the whole application level protocol decoders pipeline. That was kind of building an application protocol right on top of IP protocol. \r\nProbably blocking streams and coroutines would make the life much easier but we are lacking this in Java world at the moment. \r\n\r\nJust thought that protocol compression ideally should be somewhat transparent to the upper levels. I.e. why not compress the whole stream of RPC chunks, so the lower network lower level handler may decompress it without knowledge of higher level message structure.\r\n\r\nAnyways everyone I believe has already implemented this with more or less pain and revising this protocol has no much sense now. But for future changes and future protocols I would suggest to consider this implementation side issue ",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/649477534/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/649500976",
    "html_url": "https://github.com/ethereum/consensus-specs/issues/1819#issuecomment-649500976",
    "issue_url": "https://api.github.com/repos/ethereum/consensus-specs/issues/1819",
    "id": 649500976,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY0OTUwMDk3Ng==",
    "user": {
      "login": "protolambda",
      "id": 19571989,
      "node_id": "MDQ6VXNlcjE5NTcxOTg5",
      "avatar_url": "https://avatars.githubusercontent.com/u/19571989?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/protolambda",
      "html_url": "https://github.com/protolambda",
      "followers_url": "https://api.github.com/users/protolambda/followers",
      "following_url": "https://api.github.com/users/protolambda/following{/other_user}",
      "gists_url": "https://api.github.com/users/protolambda/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/protolambda/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/protolambda/subscriptions",
      "organizations_url": "https://api.github.com/users/protolambda/orgs",
      "repos_url": "https://api.github.com/users/protolambda/repos",
      "events_url": "https://api.github.com/users/protolambda/events{/privacy}",
      "received_events_url": "https://api.github.com/users/protolambda/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-06-25T12:07:12Z",
    "updated_at": "2020-06-25T12:07:12Z",
    "author_association": "MEMBER",
    "body": "@Nashatyrev Sorry to hear it was a pain, that was not the intention. With the right abstractions to read/write to streams it was doable from a Golang perspective at least, and I tried to get feedback from Lighthouse, Nimbus and Prysm. Other clients seemed too busy with other things at the time. The change to support snappy was discussed and generally accepted on the network call and in PR #1606 (which was open for at least a month), and so it went through. If you like to suggest things for easier implementation in Java you're welcome to, but I think we'll keep this as-is for now, and then check future changes more closely against the Java way of doing things.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/consensus-specs/issues/comments/649500976/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
