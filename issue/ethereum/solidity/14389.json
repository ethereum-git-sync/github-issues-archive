{
  "url": "https://api.github.com/repos/ethereum/solidity/issues/14389",
  "repository_url": "https://api.github.com/repos/ethereum/solidity",
  "labels_url": "https://api.github.com/repos/ethereum/solidity/issues/14389/labels{/name}",
  "comments_url": "https://api.github.com/repos/ethereum/solidity/issues/14389/comments",
  "events_url": "https://api.github.com/repos/ethereum/solidity/issues/14389/events",
  "html_url": "https://github.com/ethereum/solidity/issues/14389",
  "id": 1794986280,
  "node_id": "I_kwDOAm_5kc5q_VEo",
  "number": 14389,
  "title": "IPFS hash feature use non-specified algorithm which is not widely compatible in the ecosystem",
  "user": {
    "login": "Jorropo",
    "id": 24391983,
    "node_id": "MDQ6VXNlcjI0MzkxOTgz",
    "avatar_url": "https://avatars.githubusercontent.com/u/24391983?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/Jorropo",
    "html_url": "https://github.com/Jorropo",
    "followers_url": "https://api.github.com/users/Jorropo/followers",
    "following_url": "https://api.github.com/users/Jorropo/following{/other_user}",
    "gists_url": "https://api.github.com/users/Jorropo/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/Jorropo/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Jorropo/subscriptions",
    "organizations_url": "https://api.github.com/users/Jorropo/orgs",
    "repos_url": "https://api.github.com/users/Jorropo/repos",
    "events_url": "https://api.github.com/users/Jorropo/events{/privacy}",
    "received_events_url": "https://api.github.com/users/Jorropo/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 249074435,
      "node_id": "MDU6TGFiZWwyNDkwNzQ0MzU=",
      "url": "https://api.github.com/repos/ethereum/solidity/labels/bug%20:bug:",
      "name": "bug :bug:",
      "color": "fc1313",
      "default": false,
      "description": ""
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 0,
  "created_at": "2023-07-08T15:26:01Z",
  "updated_at": "2023-07-08T19:20:09Z",
  "closed_at": null,
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "body": "It looks like you are implementing what looks like the [Kubo](https://github.com/ipfs/kubo/) defaults, they are nearing 10 years and lack the newest features we support, I thus want to change thoses so I am poking around where people rely on thoses defaults in the ecosystem.\r\n\r\nhttps://github.com/ethereum/solidity/blob/develop/libsolutil/IpfsHash.cpp\r\n\r\nUnixfs is an open format which allows for multiple writer implementations to implement their own linking logic such as append logs, content aware chunking (cutting around logical boundries in the content, such as iframes in video files, content in archive formats, ...), more packed representation, ... while all of thoses are automatically compatible with all reader implementations.\r\nThis as designed lead to a inconsistent hashes in the ecosystem, examples with implementations that produce different CIDs:\r\n- [`github.com/Jorropo/linux2ipfs`](https://github.com/Jorropo/linux2ipfs) use 2MiB raw leaves with 2MiB roots (instead of 174 links).\r\n- [`github.com/ipld/go-car/cmd/car`](https://github.com/ipld/go-car/tree/master/cmd/car) use a different TSize logic.\r\n- [`github.com/ipfs/boxo/mfs`](https://github.com/ipfs/boxo/tree/main/mfs) (which is available in `Kubo` with `ipfs files ...`) has different defaults and can produce identical files with different CIDs if you use a different list of copy, write, append, ... operations.\r\n- [`github.com/filecoin-project/lotus`](https://github.com/filecoin-project/lotus) (*I belive*) uses raw leaves with 1MiB chunks and 1024 links with some variant of blake2\r\n- [`web3.storage`](https://web3.storage/) & [`nft.storage`](https://nft.storage/) use raw leaves with 1MiB chunks\r\n- [`github.com/bmwiedemann/ipfs-iso-jigsaw`](https://github.com/bmwiedemann/ipfs-iso-jigsaw) chunk each file in an ISO separately and then concatenate the resulting files with the ISO metadata allowing different versions of similar isos to share the blocks for the unchanged files (incremental file updates).\r\n- ... more I don't know on the top of my head\r\n\r\nHopefully this serves as a demonstration that unixfs is good at tailoring for usecases, not repeatable hashing of data.\r\n\r\nI see 3 potential fixes:\r\n1. Add an option to the compiler to output a [`.car` file](https://ipld.io/specs/transport/car/), basically instead of relying on `ipfs add` magically perfectly outputing the same CID, you do not run 2 chunkers, the solc chunker would output the blocks in an archive and then the user could `ipfs dag import` (which read blocks for blocks instead of chunking). This is how chunkers are meant to work (this or using some other transport than car).\r\n2. Write a proposal and make a new spec for repeatable unixfs chunkers inside [`ipfs/specs`](https://github.com/ipfs/specs) and implement it, you could then use a single link inline CID with metadata to embed that into the CID. So the CIDs would encode `unixfs-balanced-chunksize-256KiB-dag-pb-leaves-...` and could be fed into an other implementation to have it the same.\r\n3. Replace all the multiblock and dagpb logic with a `raw-blake3` CID. The reason we use the unixfs merkle dag format is unlike plain sha256 it supports for easy incremental verification, seeking (downloading random parts of the file without having to download the full file) and has very high exponential fanout (allows to do parallel multipeer downloads).\r\n   All of thoses features are available builtin in well specified hash functions blake3 being one of them, this removes support for the most esoteric one like custom chunking, but instead adding the same files multiple times.\r\n   Blake3 is also used by default by the new [`github.com/n0-computer/beetle`](https://github.com/n0-computer/beetle) implementation.\r\n\r\n# TL;DR:\r\n\r\nYou implement unixfs which is not a specified repeatable hash function (the same input can hash to different CIDs depending on how the internal datastructure is built which is usecase dependent).\r\nGiven your usecase is simple usually small text files I belive you should switch to use plain blake3 instead which is a well fixed merkletree (instead of the loose merkledag unixfs is).\r\n\r\n## Note 0\r\n\r\nout of all the IPFS implementations I know only beetle knows how to handle blake3 incremental verification *yet*, other Kubo & friends supports blake3 but as dumb hashes, so it still uses unixfs + blake3 to handle files above the block limit 1~4MiB, we are intrested in adding this capability in the future.\r\n\r\n## Note 1\r\n\r\nEven tho there is a one to many `file bytes → CID` unixfs relationship, assuming cryptographically secure hash functions there always is a unique `CID → bytes` relationship.\r\n\r\n## Note 2\r\n\r\nBlake3 might not be the best sollution, what I am sure is that relying on random unspecified behaviours of some old piece of software is definitely wrong. :)",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/ethereum/solidity/issues/14389/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ethereum/solidity/issues/14389/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[

]
