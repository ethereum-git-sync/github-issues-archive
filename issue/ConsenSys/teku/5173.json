{
  "url": "https://api.github.com/repos/ConsenSys/teku/issues/5173",
  "repository_url": "https://api.github.com/repos/ConsenSys/teku",
  "labels_url": "https://api.github.com/repos/ConsenSys/teku/issues/5173/labels{/name}",
  "comments_url": "https://api.github.com/repos/ConsenSys/teku/issues/5173/comments",
  "events_url": "https://api.github.com/repos/ConsenSys/teku/issues/5173/events",
  "html_url": "https://github.com/ConsenSys/teku/issues/5173",
  "id": 1168085741,
  "node_id": "I_kwDOCM9I9M5Fn5Lt",
  "number": 5173,
  "title": "Mass peer drop anomaly with Lighthouse",
  "user": {
    "login": "zilm13",
    "id": 6196452,
    "node_id": "MDQ6VXNlcjYxOTY0NTI=",
    "avatar_url": "https://avatars.githubusercontent.com/u/6196452?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/zilm13",
    "html_url": "https://github.com/zilm13",
    "followers_url": "https://api.github.com/users/zilm13/followers",
    "following_url": "https://api.github.com/users/zilm13/following{/other_user}",
    "gists_url": "https://api.github.com/users/zilm13/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/zilm13/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/zilm13/subscriptions",
    "organizations_url": "https://api.github.com/users/zilm13/orgs",
    "repos_url": "https://api.github.com/users/zilm13/repos",
    "events_url": "https://api.github.com/users/zilm13/events{/privacy}",
    "received_events_url": "https://api.github.com/users/zilm13/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 3,
  "created_at": "2022-03-14T09:10:47Z",
  "updated_at": "2022-12-02T01:03:23Z",
  "closed_at": "2022-12-02T01:03:23Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "body": "<!-- \r\nBy filing an Issue, you are expected to comply with the Code of Conduct, \r\nincluding treating everyone with respect:\r\nhttps://github.com/ConsenSys/teku/blob/master/CODE-OF-CONDUCT.md\r\n\r\nNot all sections will apply to all issue types.\r\n-->\r\n\r\n### Description\r\nDuring full sync starting from genesis I regularly get mass ban disconnection of Lighthouse peers (initiated from other side) from my Teku peer in some periods of sync. I cut a part of log to show how it looks\r\n[5_peers_lost.txt](https://github.com/ConsenSys/teku/files/8243248/5_peers_lost.txt)\r\nMy research into Lighthouse code and additional logging didn't follow a fix of it, because I'm still unsure in reason of disconnect. From our side the only evident anomaly around drop is delay of finalization compared to the `head_slot`\r\n```\r\n2022-03-14 01:23:32.049+03:00 | p2p-async-3 | DEBUG | DefaultEth2Peer | Sending status message StatusMessage{forkDigest=0xee71e973, finalizedRoot=0x635d2405b861196a0cf2a8476a0d3befbef9bc09c6701d9671dede9b9fd027a3, finalizedEpoch=4828, headRoot=0xe516878a16dc4db03d50e94743c0546ad2951972e7b5f73c374bae756a82bd39, headSlot=154750} to 16Uiu2HAmECgXtHBfmdLzoHpH6Ucyh26FZZL4pg8DB2QZauJzfcwt\r\n2022-03-14 01:23:32.387+03:00 | nioEventLoopGroup-3-6 | DEBUG | ReputationManager | reportDisconnection peer: 16Uiu2HAmECgXtHBfmdLzoHpH6Ucyh26FZZL4pg8DB2QZauJzfcwt, reputation: Reputation{suitableAfter=Optional.empty, score=14}, reason: Optional[IRRELEVANT_NETWORK], local: false\r\n\r\n2022-03-14 01:23:41.315+03:00 | p2p-async-6 | DEBUG | DefaultEth2Peer | Sending status message StatusMessage{forkDigest=0xee71e973, finalizedRoot=0x635d2405b861196a0cf2a8476a0d3befbef9bc09c6701d9671dede9b9fd027a3, finalizedEpoch=4828, headRoot=0xf449fa412d8668e497f5e7340f6a556da1f236f6def7dcb8e4458003c88554ca, headSlot=155047} to 16Uiu2HAmBe61oL9JkQHhyz18z7jMdaY3HNm6MPE7LqrVCPHYDszh\r\n2022-03-14 01:23:41.677+03:00 | nioEventLoopGroup-3-8 | DEBUG | ReputationManager | reportDisconnection peer: 16Uiu2HAmBe61oL9JkQHhyz18z7jMdaY3HNm6MPE7LqrVCPHYDszh, reputation: Reputation{suitableAfter=Optional.empty, score=14}, reason: Optional[IRRELEVANT_NETWORK], local: false\r\n```\r\nUsually it's in 2 epochs, finalized epoch=1000, head epoch=1002 (in the example above all epochs between finalized and head are finalized in the real network) . But Lighthouse code for checking irrelevance contains only these related lines:\r\n```rust\r\n        } else if remote.finalized_epoch <= local.finalized_epoch\r\n            && remote.finalized_root != Hash256::zero()\r\n            && local.finalized_root != Hash256::zero()\r\n            && self\r\n                .chain\r\n                .block_root_at_slot(start_slot(remote.finalized_epoch), WhenSlotSkipped::Prev)\r\n                .map(|root_opt| root_opt != Some(remote.finalized_root))?\r\n        {\r\n            // The remote's finalized epoch is less than or equal to ours, but the block root is\r\n            // different to the one in our chain. Therefore, the node is on a different chain and we\r\n            // should not communicate with them.\r\n            Some(\"Different finalized chain\".to_string())\r\n```\r\nIt should pass over for the provided `StatusMessage` as head data is not involved. So I don't know what to fix and how. Looks like we shouldn't be disconnected, but we were. Maybe I missed the real reason.\r\n\r\n### Steps to Reproduce (Bug)\r\nFull sync in Kintsugi network\r\nRemote side Lighthouse versions:\r\nLighthouse/v2.1.0-rc.1-ef7351d/x86_64-linux (most)\r\nLighthouse/v2.1.2-6156957/x86_64-linux (rarely)\r\nDoesn't matter match as [irrelevance code in Lighthouse](https://github.com/sigp/lighthouse/blob/02e2fd2fb8cd27070e4acc39872bd4e7a38497de/beacon_node/network/src/beacon_processor/worker/rpc_methods.rs#L58-L102) haven't changed for a long time\r\n",
  "closed_by": {
    "login": "ajsutton",
    "id": 72675,
    "node_id": "MDQ6VXNlcjcyNjc1",
    "avatar_url": "https://avatars.githubusercontent.com/u/72675?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ajsutton",
    "html_url": "https://github.com/ajsutton",
    "followers_url": "https://api.github.com/users/ajsutton/followers",
    "following_url": "https://api.github.com/users/ajsutton/following{/other_user}",
    "gists_url": "https://api.github.com/users/ajsutton/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ajsutton/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ajsutton/subscriptions",
    "organizations_url": "https://api.github.com/users/ajsutton/orgs",
    "repos_url": "https://api.github.com/users/ajsutton/repos",
    "events_url": "https://api.github.com/users/ajsutton/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ajsutton/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/5173/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ConsenSys/teku/issues/5173/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
[
  {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1082459297",
    "html_url": "https://github.com/ConsenSys/teku/issues/5173#issuecomment-1082459297",
    "issue_url": "https://api.github.com/repos/ConsenSys/teku/issues/5173",
    "id": 1082459297,
    "node_id": "IC_kwDOCM9I9M5AhQSh",
    "user": {
      "login": "jimmygchen",
      "id": 742762,
      "node_id": "MDQ6VXNlcjc0Mjc2Mg==",
      "avatar_url": "https://avatars.githubusercontent.com/u/742762?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jimmygchen",
      "html_url": "https://github.com/jimmygchen",
      "followers_url": "https://api.github.com/users/jimmygchen/followers",
      "following_url": "https://api.github.com/users/jimmygchen/following{/other_user}",
      "gists_url": "https://api.github.com/users/jimmygchen/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jimmygchen/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jimmygchen/subscriptions",
      "organizations_url": "https://api.github.com/users/jimmygchen/orgs",
      "repos_url": "https://api.github.com/users/jimmygchen/repos",
      "events_url": "https://api.github.com/users/jimmygchen/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jimmygchen/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-03-29T23:16:51Z",
    "updated_at": "2022-03-29T23:16:51Z",
    "author_association": "CONTRIBUTOR",
    "body": "@zilm13 \r\nInteresting - I had some issue syncing the other way around when trying to sync a lighthouse node from scratch with a network of teku nodes (custom network).\r\n\r\nWhat I found was Lighthouse was requesting blocks quicker than what Teku allowed by default. When syncing it hits the rate limit after a while and then gets disconnected from Teku - was quite difficult to find out as this only shows up in debug log. I had to workaround this by increasing the limit on Teku nodes: `--Xpeer-rate-limit` or `--Xpeer-request-limit`.\r\n\r\nMay not be relevant here but thought I'd mention it anyway. I've checked the rate limiter code in Lighthouse and it seems to return an error response rather than an empty block.\r\n",
    "reactions": {
      "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1082459297/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1082460928",
    "html_url": "https://github.com/ConsenSys/teku/issues/5173#issuecomment-1082460928",
    "issue_url": "https://api.github.com/repos/ConsenSys/teku/issues/5173",
    "id": 1082460928,
    "node_id": "IC_kwDOCM9I9M5AhQsA",
    "user": {
      "login": "ajsutton",
      "id": 72675,
      "node_id": "MDQ6VXNlcjcyNjc1",
      "avatar_url": "https://avatars.githubusercontent.com/u/72675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ajsutton",
      "html_url": "https://github.com/ajsutton",
      "followers_url": "https://api.github.com/users/ajsutton/followers",
      "following_url": "https://api.github.com/users/ajsutton/following{/other_user}",
      "gists_url": "https://api.github.com/users/ajsutton/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ajsutton/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ajsutton/subscriptions",
      "organizations_url": "https://api.github.com/users/ajsutton/orgs",
      "repos_url": "https://api.github.com/users/ajsutton/repos",
      "events_url": "https://api.github.com/users/ajsutton/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ajsutton/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-03-29T23:19:39Z",
    "updated_at": "2022-03-29T23:19:39Z",
    "author_association": "CONTRIBUTOR",
    "body": "On a custom network that's very much expected because there are so few peers to request blocks from it's easy for a client to send requests too fast and hit limits. It should be much less likely on real networks where requests are spread across a larger number of peers and so each peer is only receiving a small number of requests.",
    "reactions": {
      "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1082460928/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1334636763",
    "html_url": "https://github.com/ConsenSys/teku/issues/5173#issuecomment-1334636763",
    "issue_url": "https://api.github.com/repos/ConsenSys/teku/issues/5173",
    "id": 1334636763,
    "node_id": "IC_kwDOCM9I9M5PjPDb",
    "user": {
      "login": "ajsutton",
      "id": 72675,
      "node_id": "MDQ6VXNlcjcyNjc1",
      "avatar_url": "https://avatars.githubusercontent.com/u/72675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ajsutton",
      "html_url": "https://github.com/ajsutton",
      "followers_url": "https://api.github.com/users/ajsutton/followers",
      "following_url": "https://api.github.com/users/ajsutton/following{/other_user}",
      "gists_url": "https://api.github.com/users/ajsutton/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ajsutton/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ajsutton/subscriptions",
      "organizations_url": "https://api.github.com/users/ajsutton/orgs",
      "repos_url": "https://api.github.com/users/ajsutton/repos",
      "events_url": "https://api.github.com/users/ajsutton/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ajsutton/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-12-02T01:03:23Z",
    "updated_at": "2022-12-02T01:03:23Z",
    "author_association": "CONTRIBUTOR",
    "body": "Closing this - I don't think we have enough info to go on and the kintsugi network is long since dead.  I haven't seen any issues syncing generally so it doesn't seem to be causing issues in the real world (or has somehow since been fixed).",
    "reactions": {
      "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1334636763/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
