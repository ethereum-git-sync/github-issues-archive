{
  "url": "https://api.github.com/repos/ConsenSys/teku/issues/4658",
  "repository_url": "https://api.github.com/repos/ConsenSys/teku",
  "labels_url": "https://api.github.com/repos/ConsenSys/teku/issues/4658/labels{/name}",
  "comments_url": "https://api.github.com/repos/ConsenSys/teku/issues/4658/comments",
  "events_url": "https://api.github.com/repos/ConsenSys/teku/issues/4658/events",
  "html_url": "https://github.com/ConsenSys/teku/issues/4658",
  "id": 1054697898,
  "node_id": "I_kwDOCM9I9M4-3Wmq",
  "number": 4658,
  "title": "Investigate slow down in tree archive sync",
  "user": {
    "login": "benjaminion",
    "id": 20796281,
    "node_id": "MDQ6VXNlcjIwNzk2Mjgx",
    "avatar_url": "https://avatars.githubusercontent.com/u/20796281?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/benjaminion",
    "html_url": "https://github.com/benjaminion",
    "followers_url": "https://api.github.com/users/benjaminion/followers",
    "following_url": "https://api.github.com/users/benjaminion/following{/other_user}",
    "gists_url": "https://api.github.com/users/benjaminion/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/benjaminion/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/benjaminion/subscriptions",
    "organizations_url": "https://api.github.com/users/benjaminion/orgs",
    "repos_url": "https://api.github.com/users/benjaminion/repos",
    "events_url": "https://api.github.com/users/benjaminion/events{/privacy}",
    "received_events_url": "https://api.github.com/users/benjaminion/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2021-11-16T10:19:02Z",
  "updated_at": "2021-12-16T10:23:05Z",
  "closed_at": "2021-12-16T10:23:05Z",
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "body": "### Summary\r\n\r\nI recently synced an archive node using the new `leveldb-tree` format. The sync became very slow towards the end, and seemed to have increasingly frequent and lengthy pauses where nothing much seemed to be happening.\r\n\r\nAfter finishing, when running a short catch-up sync the next day, progress was twice as fast and there were no incidents of the pausing behaviour.\r\n\r\n### Configuration\r\n\r\nVersion: `teku/v21.11.1/linux-x86_64/-privatebuild-openjdk64bitservervm-java-17`\r\n\r\nEnvironment:\r\n  - Ubuntu Server 20.04 Virtualbox VM  (4 CPUs, 8GB memory) on a Windows host (12 cores/24 threads, 64GB memory).\r\n  - SSD storage\r\n  - `JAVA_OPTS=-Xmx5g`\r\n\r\nRelevant config:\r\n```\r\nnetwork: \"mainnet\"\r\ndata-path: /home/ben/teku/data\r\ndata-storage-mode: \"ARCHIVE\"\r\np2p-peer-upper-bound: 15\r\nXdata-storage-create-db-version: \"leveldb-tree\"\r\n```\r\n\r\n### Observations\r\n\r\nAfter syncing over 1m slots, occasional brief pauses in the log output can be seen. The timestamps ought to be at regular 12s intervals. Here are two examples close together where there were pauses in output for 24s, with two log lines coming simultaneously:\r\n```\r\n2021-11-13 01:45:24.747+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2495325, Head slot: 1177441, Remaining slots: 1317884, Connected peers: 16\r\n2021-11-13 01:45:48.862+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2495326, Head slot: 1177667, Remaining slots: 1317659, Connected peers: 16\r\n2021-11-13 01:45:48.881+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2495327, Head slot: 1177668, Remaining slots: 1317659, Connected peers: 16\r\n2021-11-13 01:45:48.948+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Epoch Event *** Epoch: 77979, Justified checkpoint: 36801, Finalized checkpoint: 36800, Finalized root: 597785..ff86\r\n2021-11-13 01:45:59.349+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2495328, Head slot: 1177857, Remaining slots: 1317471, Connected peers: 16\r\n2021-11-13 01:46:23.460+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2495329, Head slot: 1178112, Remaining slots: 1317217, Connected peers: 16\r\n2021-11-13 01:46:23.607+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2495330, Head slot: 1178113, Remaining slots: 1317217, Connected peers: 16\r\n```\r\n\r\nFrequency and length of these logging pauses increased. Here's around slot 2m, with a 38s pause.\r\n```\r\n2021-11-14 02:13:03.599+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2502663, Head slot: 2004928, Remaining slots: 497735, Connected peers: 16\r\n2021-11-14 02:13:11.438+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2502664, Head slot: 2005024, Remaining slots: 497640, Connected peers: 16\r\n2021-11-14 02:13:49.926+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2502665, Head slot: 2005120, Remaining slots: 497545, Connected peers: 16\r\n2021-11-14 02:13:50.163+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2502666, Head slot: 2005121, Remaining slots: 497545, Connected peers: 16\r\n2021-11-14 02:13:50.216+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2502667, Head slot: 2005122, Remaining slots: 497545, Connected peers: 16\r\n```\r\n\r\nTowards the end of the sync, frequent pauses of over a minute were occurring:\r\n```\r\n2021-11-14 23:53:23.434+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509165, Head slot: 2406821, Remaining slots: 102344, Connected peers: 15\r\n2021-11-14 23:55:08.575+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509166, Head slot: 2406881, Remaining slots: 102285, Connected peers: 15\r\n2021-11-14 23:55:08.656+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509167, Head slot: 2406882, Remaining slots: 102285, Connected peers: 15\r\n2021-11-14 23:55:08.656+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509168, Head slot: 2406882, Remaining slots: 102286, Connected peers: 15\r\n2021-11-14 23:55:08.656+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509169, Head slot: 2406882, Remaining slots: 102287, Connected peers: 15\r\n2021-11-14 23:55:08.732+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509170, Head slot: 2406883, Remaining slots: 102287, Connected peers: 15\r\n2021-11-14 23:55:08.807+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509171, Head slot: 2406884, Remaining slots: 102287, Connected peers: 15\r\n2021-11-14 23:55:08.807+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509172, Head slot: 2406884, Remaining slots: 102288, Connected peers: 15\r\n2021-11-14 23:55:08.808+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509173, Head slot: 2406884, Remaining slots: 102289, Connected peers: 15\r\n2021-11-14 23:55:11.313+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509174, Head slot: 2406911, Remaining slots: 102263, Connected peers: 15\r\n2021-11-14 23:55:24.221+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509175, Head slot: 2406976, Remaining slots: 102199, Connected peers: 15\r\n2021-11-14 23:55:35.476+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509176, Head slot: 2407041, Remaining slots: 102135, Connected peers: 15\r\n2021-11-14 23:56:44.125+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509177, Head slot: 2407073, Remaining slots: 102104, Connected peers: 15\r\n2021-11-14 23:56:44.204+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509178, Head slot: 2407074, Remaining slots: 102104, Connected peers: 15\r\n2021-11-14 23:56:44.205+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509179, Head slot: 2407074, Remaining slots: 102105, Connected peers: 15\r\n2021-11-14 23:56:44.287+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509180, Head slot: 2407075, Remaining slots: 102105, Connected peers: 15\r\n2021-11-14 23:56:44.366+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509181, Head slot: 2407076, Remaining slots: 102105, Connected peers: 15\r\n2021-11-14 23:56:49.290+00:00 | TimeTickChannel-0 | INFO  | teku-event-log | Syncing     *** Target slot: 2509182, Head slot: 2407104, Remaining slots: 102078, Connected peers: 15\r\n```\r\n\r\nDuring these pause events, it was not clear what was going on: CPU usage dropped from a steady ~250% to ~60% (400% is max in the VM); disk IO was constant and not maxed out; nothing unusual was evident in the network IO.",
  "closed_by": {
    "login": "benjaminion",
    "id": 20796281,
    "node_id": "MDQ6VXNlcjIwNzk2Mjgx",
    "avatar_url": "https://avatars.githubusercontent.com/u/20796281?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/benjaminion",
    "html_url": "https://github.com/benjaminion",
    "followers_url": "https://api.github.com/users/benjaminion/followers",
    "following_url": "https://api.github.com/users/benjaminion/following{/other_user}",
    "gists_url": "https://api.github.com/users/benjaminion/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/benjaminion/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/benjaminion/subscriptions",
    "organizations_url": "https://api.github.com/users/benjaminion/orgs",
    "repos_url": "https://api.github.com/users/benjaminion/repos",
    "events_url": "https://api.github.com/users/benjaminion/events{/privacy}",
    "received_events_url": "https://api.github.com/users/benjaminion/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/4658/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ConsenSys/teku/issues/4658/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
[
  {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/981329726",
    "html_url": "https://github.com/ConsenSys/teku/issues/4658#issuecomment-981329726",
    "issue_url": "https://api.github.com/repos/ConsenSys/teku/issues/4658",
    "id": 981329726,
    "node_id": "IC_kwDOCM9I9M46fec-",
    "user": {
      "login": "ajsutton",
      "id": 72675,
      "node_id": "MDQ6VXNlcjcyNjc1",
      "avatar_url": "https://avatars.githubusercontent.com/u/72675?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ajsutton",
      "html_url": "https://github.com/ajsutton",
      "followers_url": "https://api.github.com/users/ajsutton/followers",
      "following_url": "https://api.github.com/users/ajsutton/following{/other_user}",
      "gists_url": "https://api.github.com/users/ajsutton/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ajsutton/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ajsutton/subscriptions",
      "organizations_url": "https://api.github.com/users/ajsutton/orgs",
      "repos_url": "https://api.github.com/users/ajsutton/repos",
      "events_url": "https://api.github.com/users/ajsutton/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ajsutton/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2021-11-29T06:32:46Z",
    "updated_at": "2021-11-29T06:32:46Z",
    "author_association": "CONTRIBUTOR",
    "body": "I've run this on an ec2 node a few times and don't see the same dramatic slow down, however towards the end of the run the young gen GC time does increase which likely indicates it winds up using a fair bit of heap space. I wonder if a 5g heap on an 8g box meant Java wasn't able to allocate it all or wound up using swap which contributed to the slowness. Should have shown up in either disk IO or CPU usage though.",
    "reactions": {
      "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/981329726/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/995636478",
    "html_url": "https://github.com/ConsenSys/teku/issues/4658#issuecomment-995636478",
    "issue_url": "https://api.github.com/repos/ConsenSys/teku/issues/4658",
    "id": 995636478,
    "node_id": "IC_kwDOCM9I9M47WDT-",
    "user": {
      "login": "benjaminion",
      "id": 20796281,
      "node_id": "MDQ6VXNlcjIwNzk2Mjgx",
      "avatar_url": "https://avatars.githubusercontent.com/u/20796281?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/benjaminion",
      "html_url": "https://github.com/benjaminion",
      "followers_url": "https://api.github.com/users/benjaminion/followers",
      "following_url": "https://api.github.com/users/benjaminion/following{/other_user}",
      "gists_url": "https://api.github.com/users/benjaminion/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/benjaminion/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/benjaminion/subscriptions",
      "organizations_url": "https://api.github.com/users/benjaminion/orgs",
      "repos_url": "https://api.github.com/users/benjaminion/repos",
      "events_url": "https://api.github.com/users/benjaminion/events{/privacy}",
      "received_events_url": "https://api.github.com/users/benjaminion/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2021-12-16T10:23:05Z",
    "updated_at": "2021-12-16T10:23:05Z",
    "author_association": "CONTRIBUTOR",
    "body": "I'm still seeing this behaviour very consistently. I suspect it's disk IO that is the limiting factor, I've ruled out pretty much everything else. Teku is writing to disk at a constant 40-50 MB/s while doing the tree archive sync, which conceivably maxes out the write speed, especially since it's in a growable VM volume.\r\n\r\nCan close this; some day I'll try again with a faster SSD.",
    "reactions": {
      "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/995636478/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
