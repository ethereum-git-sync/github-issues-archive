{
  "url": "https://api.github.com/repos/ConsenSys/teku/issues/6240",
  "repository_url": "https://api.github.com/repos/ConsenSys/teku",
  "labels_url": "https://api.github.com/repos/ConsenSys/teku/issues/6240/labels{/name}",
  "comments_url": "https://api.github.com/repos/ConsenSys/teku/issues/6240/comments",
  "events_url": "https://api.github.com/repos/ConsenSys/teku/issues/6240/events",
  "html_url": "https://github.com/ConsenSys/teku/issues/6240",
  "id": 1381499128,
  "node_id": "I_kwDOCM9I9M5SWAD4",
  "number": 6240,
  "title": "Proposal: Support running a quorum of validator clients with leader election",
  "user": {
    "login": "ThomasDalla",
    "id": 609945,
    "node_id": "MDQ6VXNlcjYwOTk0NQ==",
    "avatar_url": "https://avatars.githubusercontent.com/u/609945?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/ThomasDalla",
    "html_url": "https://github.com/ThomasDalla",
    "followers_url": "https://api.github.com/users/ThomasDalla/followers",
    "following_url": "https://api.github.com/users/ThomasDalla/following{/other_user}",
    "gists_url": "https://api.github.com/users/ThomasDalla/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/ThomasDalla/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/ThomasDalla/subscriptions",
    "organizations_url": "https://api.github.com/users/ThomasDalla/orgs",
    "repos_url": "https://api.github.com/users/ThomasDalla/repos",
    "events_url": "https://api.github.com/users/ThomasDalla/events{/privacy}",
    "received_events_url": "https://api.github.com/users/ThomasDalla/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2022-09-21T21:14:16Z",
  "updated_at": "2023-03-24T20:42:54Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "### Rationale\r\nIn most clients, including Teku, the validator client is a single point of failure. Even if you are using distributed remote signers and multiple EL/CL pairs of nodes, the validator client remains in most infrastructures a single point of failure.\r\nWhile there are solutions coming up to achieve high resiliency (DVT), this is early stage and requires a fundamental infrastructure change.\r\nBeing able to run Teku validator clients in a reliable active/standby fashion would remove the single point of failure, making it a lot more resilient. \r\n\r\n### Implementation\r\nOne low-effort high-benefit way to implement an active-standby failover for Teku validator client is to use the [Apache Curator Leader Election recipe](https://curator.apache.org/curator-recipes/leader-latch.html), that relies on a preferably replicated [Apache Zookeeper quorum](https://zookeeper.apache.org/doc/r3.1.2/zookeeperStarted.html#sc_RunningReplicatedZooKeeper), whose setup is beyond the scope of Teku and assumed already configured and running.\r\nFrom Teku's perspective, we only need to specify the quorum connection string (example: `127.0.0.1:2181`), the path for the group (optional, defaulted to `/validator-client`) and the participant ID (example: `1`, `2`, `3` for each validator client).\r\nThis approach would also make the quorum agnostic to a particular client, meaning we could have a quorum with a mix of different validator clients, increasing further the diversity and therefore resiliency (if we ever implement that feature on other clients).\r\n\r\n### Risks\r\nWith the right, conservative configuration of the `sessionTimeout` (default: 60 sec) and `connectionTimeout` (default: 15 sec), there should be very little to no risk of having two validator clients being up and asserting at the same time, but in order to protect from an unforeseen edge case issue, using a **distributed remote signer** is recommended (as it would ensure 2 validator clients cannot assert/propose the same message).\r\n\r\n### Draft\r\nTo illustrate how simple the changes would be to support quite a powerful feature, I have committed a draft: https://github.com/ThomasDalla/teku/commit/42705758cdc24eee9ec535c7192cfc6f2aceda5a\r\n\r\nI have omitted the configuration part, focusing on the actual implementation that would require 2 small changes:\r\n\r\n#### Teku node startup\r\nWrap the Node within a QuorumNode that runs only when we are the leader.\r\n```java\r\n    if (config.quorumConfig() != null\r\n        && !Strings.isNullOrEmpty(config.quorumConfig().connectionString())) {\r\n      QuorumNode quorumNode = new QuorumNode(node, config.quorumConfig());\r\n      quorumNode.start();\r\n      return quorumNode;\r\n    } else {\r\n      node.start();\r\n      return node;\r\n    }\r\n```\r\n\r\n#### QuorumNode implementation\r\n`QuorumNode` implements `Node` (`start()`/`stop()`) and `LeaderLatchListener` (`isLeader()`, `notLeader()`).\r\n```java\r\npublic class QuorumNode implements Node, LeaderLatchListener {\r\n  private final Node node; // reference to the actual node we run when we are the leader\r\n  private final CuratorFramework client; // the Zookeeper client\r\n  private final LeaderLatch latch; // the latch that notifies us when we're leader or not\r\n\r\n  public QuorumNode(Node node, QuorumConfig config) {\r\n    this.node = node;\r\n    this.client =\r\n        CuratorFrameworkFactory.newClient(\r\n            config.connectionString(),\r\n            config.sessionTimeoutMs(),\r\n            config.connectionTimeoutMs(),\r\n            new ExponentialBackoffRetry(config.baseSleepTimeMs(), config.maxRetries()));\r\n    this.latch = new LeaderLatch(client, config.getPath(), config.getParticipantId());\r\n  }\r\n\r\n  /** LeaderLatchListener.isLeader() | We acquired leadership, so let's start the node */\r\n  @Override\r\n  public void isLeader() {\r\n    this.node.start();\r\n  }\r\n\r\n  /** LeaderLatchListener.notLeader() | We lost leadership, stop the node! */\r\n  @Override\r\n  public void notLeader() {\r\n    this.node.stop();\r\n  }\r\n\r\n  /** Node.start() | Kick-off the process */\r\n  @Override\r\n  public void start() {\r\n    this.client.start();\r\n    try {\r\n      this.latch.start();\r\n    } catch (Exception e) {\r\n      throw new UnsupportedOperationException(e);\r\n    }\r\n    Runtime.getRuntime().addShutdownHook(new Thread(this::stop));\r\n  }\r\n\r\n  /** Node.stop() | Exit the node (release the latch and stop the quorum client) */\r\n  @Override\r\n  public void stop() {\r\n    this.node.stop();\r\n    CloseableUtils.closeQuietly(this.latch);\r\n    CloseableUtils.closeQuietly(this.client);\r\n  }\r\n\r\n  @Override\r\n  public ServiceControllerFacade getServiceController() {\r\n    return this.node.getServiceController();\r\n  }\r\n}\r\n```\r\n\r\n### Next steps\r\n* Finish implementation (wire the configuration into command-line / env var / yaml file configs)\r\n* Add unit tests\r\n* Add metrics of the quorum's health\r\n* Document the feature on Teku's doc\r\n\r\nFeedback welcome!\r\nPlanning to test on Goerli as soon as I complete the implementation.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/6240/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ConsenSys/teku/issues/6240/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1477507467",
    "html_url": "https://github.com/ConsenSys/teku/issues/6240#issuecomment-1477507467",
    "issue_url": "https://api.github.com/repos/ConsenSys/teku/issues/6240",
    "id": 1477507467,
    "node_id": "IC_kwDOCM9I9M5YEPmL",
    "user": {
      "login": "benjaminion",
      "id": 20796281,
      "node_id": "MDQ6VXNlcjIwNzk2Mjgx",
      "avatar_url": "https://avatars.githubusercontent.com/u/20796281?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/benjaminion",
      "html_url": "https://github.com/benjaminion",
      "followers_url": "https://api.github.com/users/benjaminion/followers",
      "following_url": "https://api.github.com/users/benjaminion/following{/other_user}",
      "gists_url": "https://api.github.com/users/benjaminion/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/benjaminion/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/benjaminion/subscriptions",
      "organizations_url": "https://api.github.com/users/benjaminion/orgs",
      "repos_url": "https://api.github.com/users/benjaminion/repos",
      "events_url": "https://api.github.com/users/benjaminion/events{/privacy}",
      "received_events_url": "https://api.github.com/users/benjaminion/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-21T09:23:21Z",
    "updated_at": "2023-03-21T09:23:21Z",
    "author_association": "CONTRIBUTOR",
    "body": "Hi @ThomasDalla - how are things going with this? It is an interesting approach. We're mainly focusing our attention on supporting the more standardised DVT platforms, but are open to features such as this.",
    "reactions": {
      "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1477507467/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1483383997",
    "html_url": "https://github.com/ConsenSys/teku/issues/6240#issuecomment-1483383997",
    "issue_url": "https://api.github.com/repos/ConsenSys/teku/issues/6240",
    "id": 1483383997,
    "node_id": "IC_kwDOCM9I9M5YaqS9",
    "user": {
      "login": "ThomasDalla",
      "id": 609945,
      "node_id": "MDQ6VXNlcjYwOTk0NQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/609945?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ThomasDalla",
      "html_url": "https://github.com/ThomasDalla",
      "followers_url": "https://api.github.com/users/ThomasDalla/followers",
      "following_url": "https://api.github.com/users/ThomasDalla/following{/other_user}",
      "gists_url": "https://api.github.com/users/ThomasDalla/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ThomasDalla/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ThomasDalla/subscriptions",
      "organizations_url": "https://api.github.com/users/ThomasDalla/orgs",
      "repos_url": "https://api.github.com/users/ThomasDalla/repos",
      "events_url": "https://api.github.com/users/ThomasDalla/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ThomasDalla/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-03-24T20:42:53Z",
    "updated_at": "2023-03-24T20:42:53Z",
    "author_association": "NONE",
    "body": "Hi @benjaminion - thanks for your comment.\r\nI am also following closely the standardised DVT platform (participating in their testnets), which is an exciting area.\r\nThis proposal is using a more common technology in the industry (Zookeeper) that many non-crypto developers are already familiar with, which might attract some companies and solo stakers.\r\n\r\nI had a working prototype on Goerli that was running well, but never took the time to polish the code and add unit tests so nowhere close to a pull request level.\r\nIt highlighted 2 main points:\r\n1. It only makes sense and should only be allowed if running a standalone validator client\r\n2. When a new leader is elected, it starts performing duties immediately, so it should either force doppelganger detection (to ensure 2 epochs wait) or force the use of an external signer (like `web3signer`, which is the option I went for as doppelganger was not implemented yet in Teku when I experimented with that)\r\n\r\nI have since taken the idea a step further and built a generic orchestration layer for docker containers that allows me to run a quorum of services, ensuring only the elected leader actually runs (other ones on standby).\r\n\r\nThe reason for going that way was to achieve client diversity without having to change the code of each client.\r\nThis is closer to what the standardised DVT platforms achieve.\r\nIf I want a Zookeeper quorum with a mix of different validator clients, I need to implement the feature as a first class citizen on each client (Teku, Prysm, Nimbus, Lodestar, etc...). As I barely have time and expertise (Java) to do so for one of them (Teku, ma favourite!), I would never be able to implement that natively in all of them (alone).\r\n\r\nHowever, with an independent process that orchestrates the docker container of the validator client, it is agnostic to the code of the client itself.\r\nThe code is rather lightweight (a handful of classes) and consists in:\r\n- a config that specifies the container (for the Docker API to orchestrate it) and the zookeeper endpoint\r\n- joining the quorum and, if we are the leader, start the container, otherwise stop it\r\n- in the background, periodically check that if we are the leader, the container is running and if we're not, the container is stopped (but exists)\r\n- a Pushover config to alert me when an incident occurs (like auto-switching to a new host)\r\n\r\nThe periodic check is for additional protection as the zookeeper connection is \"live\" and we \"receive\" `obtainedLeadership()` / `lostLeadership()` events.\r\nIt's also to detect if the container is started by mistake (manually for example) while we are not the leader.\r\nIt also supports a \"transition\" mode where I can force switching to another leader when I want to do maintenance on the box that has leadership.\r\n\r\nI have been running like that on Mainnet for months now and it has prevented me from going offline during power or network outages (all my boxes are solo boxes in various locations at family homes).\r\n\r\nThis diagram illustrates the setup (and my poor LibreOffice Draw skills):\r\n![image](https://user-images.githubusercontent.com/609945/227633907-099de9e0-44a2-4a6e-ae84-64a89fa0cc1f.png)\r\n\r\nAnd this is what the logs look like (periodic check) with hosts 1 and 2 (3 was down for maintenance):\r\n![image](https://user-images.githubusercontent.com/609945/227633430-ca02edfb-db49-49f3-ada9-cde2e992492b.png)\r\n\r\nValidating on Mainnet for months with only one downtime of about a minute for an upgrade of  `web3signer` that required a database update (and therefore to stop all of them).\r\n\r\nOn a closing note, I still think that supporting the quorum natively in Teku (and other clients) could have benefits (and why not, start a trend in other validator clients!) by removing the custom \"docker-leader\" layer.\r\n\r\nIf I find the time/motivation, I'll polish the code and submit something.",
    "reactions": {
      "url": "https://api.github.com/repos/ConsenSys/teku/issues/comments/1483383997/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
