{
  "url": "https://api.github.com/repos/status-im/status-desktop/issues/8857",
  "repository_url": "https://api.github.com/repos/status-im/status-desktop",
  "labels_url": "https://api.github.com/repos/status-im/status-desktop/issues/8857/labels{/name}",
  "comments_url": "https://api.github.com/repos/status-im/status-desktop/issues/8857/comments",
  "events_url": "https://api.github.com/repos/status-im/status-desktop/issues/8857/events",
  "html_url": "https://github.com/status-im/status-desktop/issues/8857",
  "id": 1504508201,
  "node_id": "I_kwDOD5KrTM5ZrPkp",
  "number": 8857,
  "title": "Import tool: use streams instead of keeping all data in memory",
  "user": {
    "login": "0x-r4bbit",
    "id": 445106,
    "node_id": "MDQ6VXNlcjQ0NTEwNg==",
    "avatar_url": "https://avatars.githubusercontent.com/u/445106?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/0x-r4bbit",
    "html_url": "https://github.com/0x-r4bbit",
    "followers_url": "https://api.github.com/users/0x-r4bbit/followers",
    "following_url": "https://api.github.com/users/0x-r4bbit/following{/other_user}",
    "gists_url": "https://api.github.com/users/0x-r4bbit/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/0x-r4bbit/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/0x-r4bbit/subscriptions",
    "organizations_url": "https://api.github.com/users/0x-r4bbit/orgs",
    "repos_url": "https://api.github.com/users/0x-r4bbit/repos",
    "events_url": "https://api.github.com/users/0x-r4bbit/events{/privacy}",
    "received_events_url": "https://api.github.com/users/0x-r4bbit/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 4371312179,
      "node_id": "LA_kwDOD5KrTM8AAAABBIzuMw",
      "url": "https://api.github.com/repos/status-im/status-desktop/labels/E:%20Discord-Import",
      "name": "E: Discord-Import",
      "color": "c2e0c6",
      "default": false,
      "description": "Issues related to importing discord communities into Status"
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 3,
  "created_at": "2022-12-20T12:48:34Z",
  "updated_at": "2023-01-23T11:48:15Z",
  "closed_at": "2023-01-23T11:48:15Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "body": "Right now the discord import tool keeps a bunch of things in memory to load and import messages:\r\n\r\n1. Message data (discord message payloads and Status chat message equivalents)\r\n2. Discord message attachments (images, videos etc)\r\n3. Archives (created from the data above)\r\n\r\nThis makes the data grow in memory a lot and in fact, there's not stop to this at this point.\r\nWe should instead figure out a while how we can leverage stream buffers, such that the tool won't use more memory than what the buffer allows for",
  "closed_by": {
    "login": "0x-r4bbit",
    "id": 445106,
    "node_id": "MDQ6VXNlcjQ0NTEwNg==",
    "avatar_url": "https://avatars.githubusercontent.com/u/445106?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/0x-r4bbit",
    "html_url": "https://github.com/0x-r4bbit",
    "followers_url": "https://api.github.com/users/0x-r4bbit/followers",
    "following_url": "https://api.github.com/users/0x-r4bbit/following{/other_user}",
    "gists_url": "https://api.github.com/users/0x-r4bbit/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/0x-r4bbit/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/0x-r4bbit/subscriptions",
    "organizations_url": "https://api.github.com/users/0x-r4bbit/orgs",
    "repos_url": "https://api.github.com/users/0x-r4bbit/repos",
    "events_url": "https://api.github.com/users/0x-r4bbit/events{/privacy}",
    "received_events_url": "https://api.github.com/users/0x-r4bbit/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/status-im/status-desktop/issues/8857/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/status-im/status-desktop/issues/8857/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
[
  {
    "url": "https://api.github.com/repos/status-im/status-desktop/issues/comments/1362788402",
    "html_url": "https://github.com/status-im/status-desktop/issues/8857#issuecomment-1362788402",
    "issue_url": "https://api.github.com/repos/status-im/status-desktop/issues/8857",
    "id": 1362788402,
    "node_id": "IC_kwDOD5KrTM5ROoAy",
    "user": {
      "login": "0x-r4bbit",
      "id": 445106,
      "node_id": "MDQ6VXNlcjQ0NTEwNg==",
      "avatar_url": "https://avatars.githubusercontent.com/u/445106?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/0x-r4bbit",
      "html_url": "https://github.com/0x-r4bbit",
      "followers_url": "https://api.github.com/users/0x-r4bbit/followers",
      "following_url": "https://api.github.com/users/0x-r4bbit/following{/other_user}",
      "gists_url": "https://api.github.com/users/0x-r4bbit/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/0x-r4bbit/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/0x-r4bbit/subscriptions",
      "organizations_url": "https://api.github.com/users/0x-r4bbit/orgs",
      "repos_url": "https://api.github.com/users/0x-r4bbit/repos",
      "events_url": "https://api.github.com/users/0x-r4bbit/events{/privacy}",
      "received_events_url": "https://api.github.com/users/0x-r4bbit/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-12-22T12:36:37Z",
    "updated_at": "2022-12-22T12:36:37Z",
    "author_association": "MEMBER",
    "body": "I gave this a bit more thought. Here's a brain dump: \r\n\r\n### Places where we could leverage streaming\r\nThere's a few places within the import tool where streaming data **could** make sense:\r\n\r\n1. Extracting messages from discord export files\r\n2. Downloading assets and profiles pictures\r\n3. Saving message and asset data\r\n4. Creating message archives\r\n\r\nHowever, in order to stream data, we need to let the data flow from a source to some target and that's where we bump into some constraints.\r\n\r\n### Things to consider and figure out\r\n\r\n#### Sequential import tasks\r\n\r\nRight now, the import is broken down into a few tasks that are executed sequentially:\r\n\r\n1. Extract message data from export files\r\n2. Create community and categories\r\n3. Create channels and convert messages\r\n5. Save messages\r\n6. Download and save assets\r\n7. Create WakuMessages from messages\r\n8. Create message archives\r\n\r\nGiven that data streaming requires us to process buffers into some target process, we could assume that 1), 3) and 5) are one stream pipeline. This means, we'd read in some message data, create channels if necessary, convert the message data and then save the message data. At this point, we'd process the next chunk of data and repeat the work.\r\n\r\n^ As you can see, instead of have one pass through all the tasks above, we'd have multiple passes with different data chunks that need to be processed. \r\n\r\n**If this understanding is correct** then, one thing that's unclear to me is, how would we ensure sequential import progress updates? At which point can we say that, say, task 1) is done and we continued to task 3) when in reality we have to do this multiple times?\r\n\r\nOne thing I could imagine we could do, is work with metadata. For example, we first determine the total amount of messages to be processed (without actually reading all of them in) and calculate the progress update based on that. \r\nWe could probably do the same thing for 6) and 8), although, for 6) we first need to have finished 3).\r\n\r\n#### Streaming into the database\r\n\r\nOne thing I'm not sure about, is that I don't think we can actually stream straight into the database. We'd still end up with making insertions as soon as any dataset is complete. So we'd stream in a bunch of data, but would still have to wait until the message data is complete and then it can be inserted. I think that's fine for raw message data it's typically not too heavy (right now we get away with saving 1000 messages per chunk).\r\n\r\nIt is however problematic with assets. If the whole point is to not load all attachments into memory (and then saving the data), we still won't get around at least still waiting until any chunk of asset data is completed before we can insert it into the database.\r\n\r\nWe would still have the problem that writing files bigger than 1.5 MB into the database takes quite a bit of time. \r\n\r\n#### Creating message archives from database\r\n\r\nTypically, archives are created by querying a bunch of waku messages from the database in certain time range, then constructing the archive data and writing it to disk.\r\nBecause writing the waku messages to database first takes a looong time (because they need to be signed etc, which increases their raw payload size), archives are created straight from in memory wakumessages at this point in time.\r\n\r\nIf we go down the route of streaming message data, we will not have all of the messages in memory anymore to convert them to waku messages. Instead, those messages need to be converted first, then being written to the database, **then** read out again to create archives.\r\n\r\n^ This is probably the same issue as discussed in the first point. How do we stream data **out** of the database (can we even do that?). At least we somehow need to ensure that we don't too much data back into memory.\r\n\r\nIn addition, reading the data will be slow as well.\r\n\r\n### Closing thoughts\r\n\r\nThe above is just a brainstorm and there might be things that are wrong or that I'm misunderstanding.\r\nI'd hope for @cammellos and/or @Samyoul to chime in and correct me on things and/or provide advice feedback on how these things can be addressed.\r\n\r\nOne thing I have discussed with @Samyoul partially at least, is the idea of storing message attachments on disk, instead of truing to squeeze them into the database.\r\n\r\nThis would make a) streaming the data easier, as we could potentially stream straight from http requests into the file system and b) make it way faster as well, since we can leverage file system concurrency provided by the OS.\r\n\r\nIt will not address any of the points discussed above related to read/writing messages, waku messages and creating message archives.",
    "reactions": {
      "url": "https://api.github.com/repos/status-im/status-desktop/issues/comments/1362788402/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/status-im/status-desktop/issues/comments/1370081399",
    "html_url": "https://github.com/status-im/status-desktop/issues/8857#issuecomment-1370081399",
    "issue_url": "https://api.github.com/repos/status-im/status-desktop/issues/8857",
    "id": 1370081399,
    "node_id": "IC_kwDOD5KrTM5Rqch3",
    "user": {
      "login": "caybro",
      "id": 5377645,
      "node_id": "MDQ6VXNlcjUzNzc2NDU=",
      "avatar_url": "https://avatars.githubusercontent.com/u/5377645?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/caybro",
      "html_url": "https://github.com/caybro",
      "followers_url": "https://api.github.com/users/caybro/followers",
      "following_url": "https://api.github.com/users/caybro/following{/other_user}",
      "gists_url": "https://api.github.com/users/caybro/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/caybro/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/caybro/subscriptions",
      "organizations_url": "https://api.github.com/users/caybro/orgs",
      "repos_url": "https://api.github.com/users/caybro/repos",
      "events_url": "https://api.github.com/users/caybro/events{/privacy}",
      "received_events_url": "https://api.github.com/users/caybro/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-01-03T18:14:25Z",
    "updated_at": "2023-01-03T18:14:25Z",
    "author_association": "MEMBER",
    "body": "Might be related: https://github.com/status-im/status-desktop/issues/8931",
    "reactions": {
      "url": "https://api.github.com/repos/status-im/status-desktop/issues/comments/1370081399/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/status-im/status-desktop/issues/comments/1385286749",
    "html_url": "https://github.com/status-im/status-desktop/issues/8857#issuecomment-1385286749",
    "issue_url": "https://api.github.com/repos/status-im/status-desktop/issues/8857",
    "id": 1385286749,
    "node_id": "IC_kwDOD5KrTM5Skcxd",
    "user": {
      "login": "0x-r4bbit",
      "id": 445106,
      "node_id": "MDQ6VXNlcjQ0NTEwNg==",
      "avatar_url": "https://avatars.githubusercontent.com/u/445106?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/0x-r4bbit",
      "html_url": "https://github.com/0x-r4bbit",
      "followers_url": "https://api.github.com/users/0x-r4bbit/followers",
      "following_url": "https://api.github.com/users/0x-r4bbit/following{/other_user}",
      "gists_url": "https://api.github.com/users/0x-r4bbit/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/0x-r4bbit/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/0x-r4bbit/subscriptions",
      "organizations_url": "https://api.github.com/users/0x-r4bbit/orgs",
      "repos_url": "https://api.github.com/users/0x-r4bbit/repos",
      "events_url": "https://api.github.com/users/0x-r4bbit/events{/privacy}",
      "received_events_url": "https://api.github.com/users/0x-r4bbit/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-01-17T11:30:11Z",
    "updated_at": "2023-01-17T11:30:11Z",
    "author_association": "MEMBER",
    "body": "A fix for this is happening here: https://github.com/status-im/status-go/pull/3086\r\nStill needs testing under a lot of data tho",
    "reactions": {
      "url": "https://api.github.com/repos/status-im/status-desktop/issues/comments/1385286749/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 1,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
