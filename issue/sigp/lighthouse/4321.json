{
  "url": "https://api.github.com/repos/sigp/lighthouse/issues/4321",
  "repository_url": "https://api.github.com/repos/sigp/lighthouse",
  "labels_url": "https://api.github.com/repos/sigp/lighthouse/issues/4321/labels{/name}",
  "comments_url": "https://api.github.com/repos/sigp/lighthouse/issues/4321/comments",
  "events_url": "https://api.github.com/repos/sigp/lighthouse/issues/4321/events",
  "html_url": "https://github.com/sigp/lighthouse/issues/4321",
  "id": 1720725054,
  "node_id": "I_kwDOCFeAzc5mkC4-",
  "number": 4321,
  "title": "Enforce the `BATCH_BUFFER_SIZE` when handling failed batches in range sync",
  "user": {
    "login": "divagant-martian",
    "id": 26765164,
    "node_id": "MDQ6VXNlcjI2NzY1MTY0",
    "avatar_url": "https://avatars.githubusercontent.com/u/26765164?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/divagant-martian",
    "html_url": "https://github.com/divagant-martian",
    "followers_url": "https://api.github.com/users/divagant-martian/followers",
    "following_url": "https://api.github.com/users/divagant-martian/following{/other_user}",
    "gists_url": "https://api.github.com/users/divagant-martian/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/divagant-martian/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/divagant-martian/subscriptions",
    "organizations_url": "https://api.github.com/users/divagant-martian/orgs",
    "repos_url": "https://api.github.com/users/divagant-martian/repos",
    "events_url": "https://api.github.com/users/divagant-martian/events{/privacy}",
    "received_events_url": "https://api.github.com/users/divagant-martian/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 2336800125,
      "node_id": "MDU6TGFiZWwyMzM2ODAwMTI1",
      "url": "https://api.github.com/repos/sigp/lighthouse/labels/t%20Networking",
      "name": "t Networking",
      "color": "40E0D0",
      "default": false,
      "description": ""
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": {
    "login": "jmcph4",
    "id": 717268,
    "node_id": "MDQ6VXNlcjcxNzI2OA==",
    "avatar_url": "https://avatars.githubusercontent.com/u/717268?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/jmcph4",
    "html_url": "https://github.com/jmcph4",
    "followers_url": "https://api.github.com/users/jmcph4/followers",
    "following_url": "https://api.github.com/users/jmcph4/following{/other_user}",
    "gists_url": "https://api.github.com/users/jmcph4/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/jmcph4/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/jmcph4/subscriptions",
    "organizations_url": "https://api.github.com/users/jmcph4/orgs",
    "repos_url": "https://api.github.com/users/jmcph4/repos",
    "events_url": "https://api.github.com/users/jmcph4/events{/privacy}",
    "received_events_url": "https://api.github.com/users/jmcph4/received_events",
    "type": "User",
    "site_admin": false
  },
  "assignees": [
    {
      "login": "jmcph4",
      "id": 717268,
      "node_id": "MDQ6VXNlcjcxNzI2OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/717268?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jmcph4",
      "html_url": "https://github.com/jmcph4",
      "followers_url": "https://api.github.com/users/jmcph4/followers",
      "following_url": "https://api.github.com/users/jmcph4/following{/other_user}",
      "gists_url": "https://api.github.com/users/jmcph4/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jmcph4/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jmcph4/subscriptions",
      "organizations_url": "https://api.github.com/users/jmcph4/orgs",
      "repos_url": "https://api.github.com/users/jmcph4/repos",
      "events_url": "https://api.github.com/users/jmcph4/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jmcph4/received_events",
      "type": "User",
      "site_admin": false
    }
  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2023-05-22T22:36:59Z",
  "updated_at": "2023-09-10T23:47:48Z",
  "closed_at": null,
  "author_association": "COLLABORATOR",
  "active_lock_reason": null,
  "body": "## Description\r\n\r\nWhen a batch fails, we need to re-request and re-reprocess all previous not-yet-validated batches. This can easily be a very large number of batches if -for example- they are all empty. We should enforce the `BATCH_BUFFER_SIZE` when this happens, making sure no more than this constant of batches is being downloaded at the same time.\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/4321/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/sigp/lighthouse/issues/4321/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1712974686",
    "html_url": "https://github.com/sigp/lighthouse/issues/4321#issuecomment-1712974686",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/4321",
    "id": 1712974686,
    "node_id": "IC_kwDOCFeAzc5mGete",
    "user": {
      "login": "jmcph4",
      "id": 717268,
      "node_id": "MDQ6VXNlcjcxNzI2OA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/717268?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/jmcph4",
      "html_url": "https://github.com/jmcph4",
      "followers_url": "https://api.github.com/users/jmcph4/followers",
      "following_url": "https://api.github.com/users/jmcph4/following{/other_user}",
      "gists_url": "https://api.github.com/users/jmcph4/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/jmcph4/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/jmcph4/subscriptions",
      "organizations_url": "https://api.github.com/users/jmcph4/orgs",
      "repos_url": "https://api.github.com/users/jmcph4/repos",
      "events_url": "https://api.github.com/users/jmcph4/events{/privacy}",
      "received_events_url": "https://api.github.com/users/jmcph4/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-09-10T23:47:48Z",
    "updated_at": "2023-09-10T23:47:48Z",
    "author_association": "COLLABORATOR",
    "body": "If we add some instrumentation to the range sync implementation, like this:\r\n\r\n```diff\r\ndiff --git a/beacon_node/network/src/sync/manager.rs b/beacon_node/network/src/sync/manager.rs\r\nindex b910f7b33..731809043 100644\r\n--- a/beacon_node/network/src/sync/manager.rs\r\n+++ b/beacon_node/network/src/sync/manager.rs\r\n@@ -359,6 +359,7 @@ impl<T: BeaconChainTypes> SyncManager<T> {\r\n     /// - If there is no range sync and no required backfill and we have synced up to the currently\r\n     /// known peers, we consider ourselves synced.\r\n     fn update_sync_state(&mut self) {\r\n+        self.range_sync.any_over();\r\n         let new_state: SyncState = match self.range_sync.state() {\r\n             Err(e) => {\r\n                 crit!(self.log, \"Error getting range sync state\"; \"error\" => %e);\r\ndiff --git a/beacon_node/network/src/sync/range_sync/chain.rs b/beacon_node/network/src/sync/range_sync/chain.rs\r\nindex af547885d..5b4131646 100644\r\n--- a/beacon_node/network/src/sync/range_sync/chain.rs\r\n+++ b/beacon_node/network/src/sync/range_sync/chain.rs\r\n@@ -7,7 +7,7 @@ use beacon_chain::BeaconChainTypes;\r\n use fnv::FnvHashMap;\r\n use lighthouse_network::{PeerAction, PeerId};\r\n use rand::seq::SliceRandom;\r\n-use slog::{crit, debug, o, warn};\r\n+use slog::{crit, debug, o, trace, warn};\r\n use std::collections::{btree_map::Entry, BTreeMap, HashSet};\r\n use std::hash::{Hash, Hasher};\r\n use std::sync::Arc;\r\n@@ -114,6 +114,31 @@ pub enum ChainSyncingState {\r\n }\r\n \r\n impl<T: BeaconChainTypes> SyncingChain<T> {\r\n+    pub fn buffered_batches(&self) -> usize {\r\n+        let in_buffer = |batch: &BatchInfo<T::EthSpec>| {\r\n+            matches!(\r\n+                batch.state(),\r\n+                BatchState::Downloading(..) | BatchState::AwaitingProcessing(..)\r\n+            )\r\n+        };\r\n+        self.batches\r\n+            .iter()\r\n+            .filter(|&(_epoch, batch)| in_buffer(batch))\r\n+            .count()\r\n+    }\r\n+\r\n+    pub fn over(&self) -> bool {\r\n+        self.buffered_batches() > BATCH_BUFFER_SIZE as usize\r\n+    }\r\n+\r\n+    fn check_buffer_size(&self) {\r\n+        if self.over() {\r\n+            debug!(self.log, \"Batch buffer overfull\"; \"buflen\" => self.buffered_batches(), \"limit\" => BATCH_BUFFER_SIZE);\r\n+        } else {\r\n+            trace!(self.log, \"Report batch buffer state\"; \"buflen\" => self.buffered_batches(), \"limit\" => BATCH_BUFFER_SIZE);\r\n+        }\r\n+    }\r\n+\r\n     pub fn id(target_root: &Hash256, target_slot: &Slot) -> u64 {\r\n         let mut hasher = std::collections::hash_map::DefaultHasher::new();\r\n         (target_root, target_slot).hash(&mut hasher);\r\n@@ -178,6 +203,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         peer_id: &PeerId,\r\n         network: &mut SyncNetworkContext<T>,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         if let Some(batch_ids) = self.peers.remove(peer_id) {\r\n             // fail the batches\r\n             for id in batch_ids {\r\n@@ -223,6 +250,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         request_id: Id,\r\n         beacon_block: Option<Arc<SignedBeaconBlock<T::EthSpec>>>,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         // check if we have this batch\r\n         let batch = match self.batches.get_mut(&batch_id) {\r\n             None => {\r\n@@ -289,6 +318,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         network: &mut SyncNetworkContext<T>,\r\n         batch_id: BatchId,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         // Only process batches if this chain is Syncing, and only one at a time\r\n         if self.state != ChainSyncingState::Syncing || self.current_processing_batch.is_some() {\r\n             return Ok(KeepChain);\r\n@@ -335,6 +366,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         &mut self,\r\n         network: &mut SyncNetworkContext<T>,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         // Only process batches if this chain is Syncing and only process one batch at a time\r\n         if self.state != ChainSyncingState::Syncing || self.current_processing_batch.is_some() {\r\n             return Ok(KeepChain);\r\n@@ -438,6 +471,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         batch_id: BatchId,\r\n         result: &BatchProcessResult,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         // the first two cases are possible if the chain advances while waiting for a processing\r\n         // result\r\n         let batch = match &self.current_processing_batch {\r\n@@ -571,6 +606,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         redownload: bool,\r\n         reason: &str,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         if let Some(epoch) = self.optimistic_start.take() {\r\n             self.attempted_optimistic_starts.insert(epoch);\r\n             // if this batch is inside the current processing range, keep it, otherwise drop\r\n@@ -599,6 +636,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n     /// If a previous batch has been validated and it had been re-processed, penalize the original\r\n     /// peer.\r\n     fn advance_chain(&mut self, network: &mut SyncNetworkContext<T>, validating_epoch: Epoch) {\r\n+        self.check_buffer_size();\r\n+\r\n         // make sure this epoch produces an advancement\r\n         if validating_epoch <= self.start_epoch {\r\n             return;\r\n@@ -705,6 +744,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         network: &mut SyncNetworkContext<T>,\r\n         batch_id: BatchId,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         // The current batch could not be processed, indicating either the current or previous\r\n         // batches are invalid.\r\n \r\n@@ -752,6 +793,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n     }\r\n \r\n     pub fn stop_syncing(&mut self) {\r\n+        self.check_buffer_size();\r\n+\r\n         self.state = ChainSyncingState::Stopped;\r\n     }\r\n \r\n@@ -765,6 +808,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         local_finalized_epoch: Epoch,\r\n         optimistic_start_epoch: Epoch,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         // to avoid dropping local progress, we advance the chain wrt its batch boundaries. This\r\n         let align = |epoch| {\r\n             // start_epoch + (number of batches in between)*length_of_batch\r\n@@ -802,6 +847,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         network: &mut SyncNetworkContext<T>,\r\n         peer_id: PeerId,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         // add the peer without overwriting its active requests\r\n         if self.peers.entry(peer_id).or_default().is_empty() {\r\n             // Either new or not, this peer is idle, try to request more batches\r\n@@ -821,6 +868,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         peer_id: &PeerId,\r\n         request_id: Id,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         if let Some(batch) = self.batches.get_mut(&batch_id) {\r\n             // A batch could be retried without the peer failing the request (disconnecting/\r\n             // sending an error /timeout) if the peer is removed from the chain for other\r\n@@ -851,6 +900,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         network: &mut SyncNetworkContext<T>,\r\n         batch_id: BatchId,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         let batch = match self.batches.get_mut(&batch_id) {\r\n             Some(batch) => batch,\r\n             None => return Ok(KeepChain),\r\n@@ -885,6 +936,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         batch_id: BatchId,\r\n         peer: PeerId,\r\n     ) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         if let Some(batch) = self.batches.get_mut(&batch_id) {\r\n             let request = batch.to_blocks_by_range_request();\r\n             match network.blocks_by_range_request(peer, request, self.id, batch_id) {\r\n@@ -956,6 +1009,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         &mut self,\r\n         network: &mut SyncNetworkContext<T>,\r\n     ) -> Result<KeepChain, RemoveChain> {\r\n+        self.check_buffer_size();\r\n+\r\n         // Request more batches if needed.\r\n         self.request_batches(network)?;\r\n         // If there is any batch ready for processing, send it.\r\n@@ -965,6 +1020,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n     /// Attempts to request the next required batches from the peer pool if the chain is syncing. It will exhaust the peer\r\n     /// pool and left over batches until the batch buffer is reached or all peers are exhausted.\r\n     fn request_batches(&mut self, network: &mut SyncNetworkContext<T>) -> ProcessingResult {\r\n+        self.check_buffer_size();\r\n+\r\n         if !matches!(self.state, ChainSyncingState::Syncing) {\r\n             return Ok(KeepChain);\r\n         }\r\n@@ -1015,6 +1072,8 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n     /// Creates the next required batch from the chain. If there are no more batches required,\r\n     /// `false` is returned.\r\n     fn include_next_batch(&mut self) -> Option<BatchId> {\r\n+        self.check_buffer_size();\r\n+\r\n         // don't request batches beyond the target head slot\r\n         if self\r\n             .to_be_downloaded\r\ndiff --git a/beacon_node/network/src/sync/range_sync/chain_collection.rs b/beacon_node/network/src/sync/range_sync/chain_collection.rs\r\nindex 65ddcefe8..ac3d8e0ae 100644\r\n--- a/beacon_node/network/src/sync/range_sync/chain_collection.rs\r\n+++ b/beacon_node/network/src/sync/range_sync/chain_collection.rs\r\n@@ -62,6 +62,11 @@ impl<T: BeaconChainTypes, C: BlockStorage> ChainCollection<T, C> {\r\n         }\r\n     }\r\n \r\n+    pub fn any_over(&self) -> bool {\r\n+        self.finalized_chains.values().any(|chain| chain.over())\r\n+            || self.head_chains.values().any(|chain| chain.over())\r\n+    }\r\n+\r\n     /// Updates the Syncing state of the collection after a chain is removed.\r\n     fn on_chain_removed(&mut self, id: &ChainId, was_syncing: bool, sync_type: RangeSyncType) {\r\n         let _ = metrics::get_int_gauge(&metrics::SYNCING_CHAINS_COUNT, &[sync_type.as_str()])\r\ndiff --git a/beacon_node/network/src/sync/range_sync/range.rs b/beacon_node/network/src/sync/range_sync/range.rs\r\nindex 05ad5204b..08f1d276c 100644\r\n--- a/beacon_node/network/src/sync/range_sync/range.rs\r\n+++ b/beacon_node/network/src/sync/range_sync/range.rs\r\n@@ -101,6 +101,10 @@ where\r\n         self.chains.state()\r\n     }\r\n \r\n+    pub fn any_over(&self) -> bool {\r\n+        self.chains.any_over()\r\n+    }\r\n+\r\n     /// A useful peer has been added. The SyncManager has identified this peer as needing either\r\n     /// a finalized or head chain sync. This processes the peer and starts/resumes any chain that\r\n     /// may need to be synced as a result. A new peer, may increase the peer pool of a finalized\r\n```\r\n\r\n...and then add a guard to `send_batch`, like this:\r\n\r\n```diff\r\ndiff --git a/beacon_node/network/src/sync/range_sync/chain.rs b/beacon_node/network/src/sync/range_sync/chain.rs\r\nindex af547885d..497b8f42f 100644\r\n--- a/beacon_node/network/src/sync/range_sync/chain.rs\r\n+++ b/beacon_node/network/src/sync/range_sync/chain.rs\r\n@@ -885,6 +885,10 @@ impl<T: BeaconChainTypes> SyncingChain<T> {\r\n         batch_id: BatchId,\r\n         peer: PeerId,\r\n     ) -> ProcessingResult {\r\n+        if self.buffered_batches() == BATCH_BUFFER_SIZE as usize || self.over() {\r\n+            return Ok(KeepChain);\r\n+        }\r\n+\r\n         if let Some(batch) = self.batches.get_mut(&batch_id) {\r\n             let request = batch.to_blocks_by_range_request();\r\n             match network.blocks_by_range_request(peer, request, self.id, batch_id) {\r\n```\r\n\r\n...the `\"Batch buffer overfull\"` instances seem to disappear (on an empty Lighthouse data directory, of course).\r\n\r\nThis makes sense to me as there's already a guard on `include_next_batch`:\r\n\r\nhttps://github.com/sigp/lighthouse/blob/2841f60686d642fcc0785c884d43e34e47a800dc/beacon_node/network/src/sync/range_sync/chain.rs#L1035-L1043\r\n\r\nand `send_batch` is the only other call site where we might possible mutate the batch buffer in such a way as to overfill it.",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1712974686/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
