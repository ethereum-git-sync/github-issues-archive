{
  "url": "https://api.github.com/repos/sigp/lighthouse/issues/2701",
  "repository_url": "https://api.github.com/repos/sigp/lighthouse",
  "labels_url": "https://api.github.com/repos/sigp/lighthouse/issues/2701/labels{/name}",
  "comments_url": "https://api.github.com/repos/sigp/lighthouse/issues/2701/comments",
  "events_url": "https://api.github.com/repos/sigp/lighthouse/issues/2701/events",
  "html_url": "https://github.com/sigp/lighthouse/issues/2701",
  "id": 1022327384,
  "node_id": "I_kwDOCFeAzc4873pY",
  "number": 2701,
  "title": "Inbound Stream Termination Edge Case",
  "user": {
    "login": "AgeManning",
    "id": 7454587,
    "node_id": "MDQ6VXNlcjc0NTQ1ODc=",
    "avatar_url": "https://avatars.githubusercontent.com/u/7454587?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/AgeManning",
    "html_url": "https://github.com/AgeManning",
    "followers_url": "https://api.github.com/users/AgeManning/followers",
    "following_url": "https://api.github.com/users/AgeManning/following{/other_user}",
    "gists_url": "https://api.github.com/users/AgeManning/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/AgeManning/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/AgeManning/subscriptions",
    "organizations_url": "https://api.github.com/users/AgeManning/orgs",
    "repos_url": "https://api.github.com/users/AgeManning/repos",
    "events_url": "https://api.github.com/users/AgeManning/events{/privacy}",
    "received_events_url": "https://api.github.com/users/AgeManning/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 985647281,
      "node_id": "MDU6TGFiZWw5ODU2NDcyODE=",
      "url": "https://api.github.com/repos/sigp/lighthouse/labels/bug",
      "name": "bug",
      "color": "d73a4a",
      "default": true,
      "description": "Something isn't working"
    },
    {
      "id": 2336800125,
      "node_id": "MDU6TGFiZWwyMzM2ODAwMTI1",
      "url": "https://api.github.com/repos/sigp/lighthouse/labels/t%20Networking",
      "name": "t Networking",
      "color": "40E0D0",
      "default": false,
      "description": ""
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 2,
  "created_at": "2021-10-11T06:49:58Z",
  "updated_at": "2022-10-20T17:09:30Z",
  "closed_at": "2022-01-20T07:31:13Z",
  "author_association": "MEMBER",
  "active_lock_reason": null,
  "body": "## Description\r\n\r\nI saw some inbound stream timeouts. I'm suss on them because they seem to be resolved, but not registered in the handler. I saw these logs:\r\n```\r\nOct 11 14:25:18.359 DEBG Received BlocksByRange Request          step: 1, start_slot: 2156384, count: 64, peer_id: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:18.359 DEBG Received BlocksByRange Request          step: 1, start_slot: 2156704, count: 64, peer_id: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:18.360 DEBG Received BlocksByRange Request          step: 1, start_slot: 2156768, count: 64, peer_id: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:18.599 DEBG BlocksByRange Response sent             returned: 64, requested: 64, current_slot: 2258224, start_slot: 2156384, peer: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:18.609 DEBG BlocksByRange Response sent             returned: 63, requested: 64, current_slot: 2258224, start_slot: 2156704, msg: Failed to return all requested blocks, peer: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:18.610 DEBG BlocksByRange Response sent             returned: 62, requested: 64, current_slot: 2258224, start_slot: 2156768, msg: Failed to return all requested blocks, peer: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:18.670 DEBG Received BlocksByRange Request          step: 1, start_slot: 2156832, count: 64, peer_id: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:18.937 DEBG BlocksByRange Response sent             returned: 64, requested: 64, current_slot: 2258224, start_slot: 2156832, peer: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ\r\nOct 11 14:25:29.054 DEBG RPC Error                               direction: Incoming, score: 0, peer_id: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ, client: Prysm: version: 29d48dfe7eb38959752048aec700f31c3abf7484, os_version: unknown, err: Stream Timeout, protocol: beacon_blocks_by_range, service: libp2p\r\nOct 11 14:25:29.054 WARN Timed out to a peer's request. Likely too many resources, reduce peer count, service: libp2p\r\nOct 11 14:25:30.690 DEBG RPC Error                               direction: Incoming, score: 0, peer_id: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ, client: Prysm: version: 29d48dfe7eb38959752048aec700f31c3abf7484, os_version: unknown, err: Stream Timeout, protocol: beacon_blocks_by_range, service: libp2p\r\nOct 11 14:25:30.691 WARN Timed out to a peer's request. Likely too many resources, reduce peer count, service: libp2p\r\nOct 11 14:25:30.691 DEBG RPC Error                               direction: Incoming, score: 0, peer_id: 16Uiu2HAmHJt8Hsqd8oJZkiz9i3Thiy8V7D7g9LDoqGvEmhCPnBTZ, client: Prysm: version: 29d48dfe7eb38959752048aec700f31c3abf7484, os_version: unknown, err: Stream Timeout, protocol: beacon_blocks_by_range, service: libp2p\r\nOct 11 14:25:30.691 WARN Timed out to a peer's request. Likely too many resources, reduce peer count, service: libp2p\r\n```\r\n\r\nThese were the only BBRange requests and they get answered very shortly after. The worker seems to be sending the stream termination to the network task. I doubt the network task is halted for 10 seconds, so I would assume the stream termination makes it into the RPC Handler. \r\n\r\nNow the question is why is the RPC Handler not removing the inbound stream from the delay queue?\r\n\r\nI had a quick look through the handler and didn't see anything off. It looks like for stream terminations we start attempting to close the substream and once the substream is closed we remove it from the delay queue. Potentially there is some hold up in closing the stream, but I think also maybe we are missing some edge case where potentially the stream gets closed before we attemtp to close it or some race condition that leads us to not remove the substream from the delay queue. \r\n\r\nI think it requires some investigation. ",
  "closed_by": {
    "login": "paulhauner",
    "id": 6660660,
    "node_id": "MDQ6VXNlcjY2NjA2NjA=",
    "avatar_url": "https://avatars.githubusercontent.com/u/6660660?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/paulhauner",
    "html_url": "https://github.com/paulhauner",
    "followers_url": "https://api.github.com/users/paulhauner/followers",
    "following_url": "https://api.github.com/users/paulhauner/following{/other_user}",
    "gists_url": "https://api.github.com/users/paulhauner/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/paulhauner/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/paulhauner/subscriptions",
    "organizations_url": "https://api.github.com/users/paulhauner/orgs",
    "repos_url": "https://api.github.com/users/paulhauner/repos",
    "events_url": "https://api.github.com/users/paulhauner/events{/privacy}",
    "received_events_url": "https://api.github.com/users/paulhauner/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/2701/reactions",
    "total_count": 3,
    "+1": 1,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 2
  },
  "timeline_url": "https://api.github.com/repos/sigp/lighthouse/issues/2701/timeline",
  "performed_via_github_app": null,
  "state_reason": "completed"
}
[
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/939739668",
    "html_url": "https://github.com/sigp/lighthouse/issues/2701#issuecomment-939739668",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/2701",
    "id": 939739668,
    "node_id": "IC_kwDOCFeAzc44A0oU",
    "user": {
      "login": "AgeManning",
      "id": 7454587,
      "node_id": "MDQ6VXNlcjc0NTQ1ODc=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7454587?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AgeManning",
      "html_url": "https://github.com/AgeManning",
      "followers_url": "https://api.github.com/users/AgeManning/followers",
      "following_url": "https://api.github.com/users/AgeManning/following{/other_user}",
      "gists_url": "https://api.github.com/users/AgeManning/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AgeManning/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AgeManning/subscriptions",
      "organizations_url": "https://api.github.com/users/AgeManning/orgs",
      "repos_url": "https://api.github.com/users/AgeManning/repos",
      "events_url": "https://api.github.com/users/AgeManning/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AgeManning/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2021-10-11T06:58:58Z",
    "updated_at": "2021-10-11T06:58:58Z",
    "author_association": "MEMBER",
    "body": "I've seen this with a different node also, and it also seems to be from a BlocksByRange request, which is a bit suss. \r\n\r\nThe handler doesn't really distinguish between different RPCMethods, so this could just be a coincidence. I doesn't seem like we are locking on the db, because the logs indicate the worker completes the request. \r\n\r\nThis might have something to do with expected chunks remaining in the handler?",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/939739668/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/940460265",
    "html_url": "https://github.com/sigp/lighthouse/issues/2701#issuecomment-940460265",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/2701",
    "id": 940460265,
    "node_id": "IC_kwDOCFeAzc44Dkjp",
    "user": {
      "login": "divagant-martian",
      "id": 26765164,
      "node_id": "MDQ6VXNlcjI2NzY1MTY0",
      "avatar_url": "https://avatars.githubusercontent.com/u/26765164?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/divagant-martian",
      "html_url": "https://github.com/divagant-martian",
      "followers_url": "https://api.github.com/users/divagant-martian/followers",
      "following_url": "https://api.github.com/users/divagant-martian/following{/other_user}",
      "gists_url": "https://api.github.com/users/divagant-martian/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/divagant-martian/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/divagant-martian/subscriptions",
      "organizations_url": "https://api.github.com/users/divagant-martian/orgs",
      "repos_url": "https://api.github.com/users/divagant-martian/repos",
      "events_url": "https://api.github.com/users/divagant-martian/events{/privacy}",
      "received_events_url": "https://api.github.com/users/divagant-martian/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2021-10-11T21:36:21Z",
    "updated_at": "2021-10-15T20:39:31Z",
    "author_association": "COLLABORATOR",
    "body": "I think it might have to do with how the handler is polled. My hypothesis would be that when a response is received for an outbound request, the swarm polls the handler and this is processed immediately. When the handler gets a (lh) event injected it gets polled but if we need to queue items, those would be sent until the next poll. I'll check how to (dis)prove this. If this is it then the solution is probably to store a waker.\r\nThis would explain why it's only happening with inbound requests for which `remaining_chunks > 1`\r\n\r\n---------------\r\n\r\nDid some tests with enough load to force the handler to queue chunks (Sometimes as many as a complete batch - 64) and can't make it timeout, so maybe it's something else. ",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/940460265/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
