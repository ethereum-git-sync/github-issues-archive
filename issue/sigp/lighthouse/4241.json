{
  "url": "https://api.github.com/repos/sigp/lighthouse/issues/4241",
  "repository_url": "https://api.github.com/repos/sigp/lighthouse",
  "labels_url": "https://api.github.com/repos/sigp/lighthouse/issues/4241/labels{/name}",
  "comments_url": "https://api.github.com/repos/sigp/lighthouse/issues/4241/comments",
  "events_url": "https://api.github.com/repos/sigp/lighthouse/issues/4241/events",
  "html_url": "https://github.com/sigp/lighthouse/issues/4241",
  "id": 1685135859,
  "node_id": "I_kwDOCFeAzc5kcSHz",
  "number": 4241,
  "title": "Metrics scraping shouldn't access the disk",
  "user": {
    "login": "karalabe",
    "id": 129561,
    "node_id": "MDQ6VXNlcjEyOTU2MQ==",
    "avatar_url": "https://avatars.githubusercontent.com/u/129561?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/karalabe",
    "html_url": "https://github.com/karalabe",
    "followers_url": "https://api.github.com/users/karalabe/followers",
    "following_url": "https://api.github.com/users/karalabe/following{/other_user}",
    "gists_url": "https://api.github.com/users/karalabe/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/karalabe/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/karalabe/subscriptions",
    "organizations_url": "https://api.github.com/users/karalabe/orgs",
    "repos_url": "https://api.github.com/users/karalabe/repos",
    "events_url": "https://api.github.com/users/karalabe/events{/privacy}",
    "received_events_url": "https://api.github.com/users/karalabe/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1999784343,
      "node_id": "MDU6TGFiZWwxOTk5Nzg0MzQz",
      "url": "https://api.github.com/repos/sigp/lighthouse/labels/optimization",
      "name": "optimization",
      "color": "f9de40",
      "default": false,
      "description": "Something to make Lighthouse run more efficiently."
    },
    {
      "id": 2336798682,
      "node_id": "MDU6TGFiZWwyMzM2Nzk4Njgy",
      "url": "https://api.github.com/repos/sigp/lighthouse/labels/database",
      "name": "database",
      "color": "C01C9D",
      "default": false,
      "description": ""
    },
    {
      "id": 2336800343,
      "node_id": "MDU6TGFiZWwyMzM2ODAwMzQz",
      "url": "https://api.github.com/repos/sigp/lighthouse/labels/HTTP-API",
      "name": "HTTP-API",
      "color": "5A63A2",
      "default": false,
      "description": ""
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2023-04-26T14:22:07Z",
  "updated_at": "2023-05-01T08:27:55Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "This is more of a philosophical issue. I've spent a lot of time debugging a hang on the metrics endpoint (https://github.com/sigp/lighthouse/issues/4230), and whilst it turned out that the hang was caused by some mounting problem in Unraid, it still feels off that it happened. It means that scraping the metrics does some disk access / read. IMO that should not happen. Geth at least keeps - almost - all it's metrics in RAM so listing them is just a dump. The only metrics we seem to directly pull is reading a few `/proc` files, but those didn't have an issue with the \"faulty\" Unraid mount, so lighthouse must be doing some heavier disk access.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/4241/reactions",
    "total_count": 1,
    "+1": 1,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/sigp/lighthouse/issues/4241/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1524071621",
    "html_url": "https://github.com/sigp/lighthouse/issues/4241#issuecomment-1524071621",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/4241",
    "id": 1524071621,
    "node_id": "IC_kwDOCFeAzc5a13zF",
    "user": {
      "login": "michaelsproul",
      "id": 4452260,
      "node_id": "MDQ6VXNlcjQ0NTIyNjA=",
      "avatar_url": "https://avatars.githubusercontent.com/u/4452260?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/michaelsproul",
      "html_url": "https://github.com/michaelsproul",
      "followers_url": "https://api.github.com/users/michaelsproul/followers",
      "following_url": "https://api.github.com/users/michaelsproul/following{/other_user}",
      "gists_url": "https://api.github.com/users/michaelsproul/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/michaelsproul/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/michaelsproul/subscriptions",
      "organizations_url": "https://api.github.com/users/michaelsproul/orgs",
      "repos_url": "https://api.github.com/users/michaelsproul/repos",
      "events_url": "https://api.github.com/users/michaelsproul/events{/privacy}",
      "received_events_url": "https://api.github.com/users/michaelsproul/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2023-04-26T21:32:57Z",
    "updated_at": "2023-04-26T21:32:57Z",
    "author_association": "MEMBER",
    "body": "I think the big disk access we do is checking the size of the data directory, which is pretty much a `du` call. I can see the case for doing that much less regularly and just serving the cached value on the metrics endpoint ",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1524071621/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
