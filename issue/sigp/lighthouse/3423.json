{
  "url": "https://api.github.com/repos/sigp/lighthouse/issues/3423",
  "repository_url": "https://api.github.com/repos/sigp/lighthouse",
  "labels_url": "https://api.github.com/repos/sigp/lighthouse/issues/3423/labels{/name}",
  "comments_url": "https://api.github.com/repos/sigp/lighthouse/issues/3423/comments",
  "events_url": "https://api.github.com/repos/sigp/lighthouse/issues/3423/events",
  "html_url": "https://github.com/sigp/lighthouse/issues/3423",
  "id": 1327808171,
  "node_id": "I_kwDOCFeAzc5PJL6r",
  "number": 3423,
  "title": " \"Failed to send scheduled attestation\" errors—how to remedy or at least reduce?",
  "user": {
    "login": "JamesCropcho",
    "id": 3657,
    "node_id": "MDQ6VXNlcjM2NTc=",
    "avatar_url": "https://avatars.githubusercontent.com/u/3657?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/JamesCropcho",
    "html_url": "https://github.com/JamesCropcho",
    "followers_url": "https://api.github.com/users/JamesCropcho/followers",
    "following_url": "https://api.github.com/users/JamesCropcho/following{/other_user}",
    "gists_url": "https://api.github.com/users/JamesCropcho/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/JamesCropcho/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/JamesCropcho/subscriptions",
    "organizations_url": "https://api.github.com/users/JamesCropcho/orgs",
    "repos_url": "https://api.github.com/users/JamesCropcho/repos",
    "events_url": "https://api.github.com/users/JamesCropcho/events{/privacy}",
    "received_events_url": "https://api.github.com/users/JamesCropcho/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [

  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 1,
  "created_at": "2022-08-03T21:25:51Z",
  "updated_at": "2022-09-07T14:08:55Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "## Description\r\n\r\nThe log of `lighthouse beacon_node` has _large_ clumps of entries of:\r\n\r\n```\r\n21:06:33.945 ERRO Failed to send scheduled attestation\r\n21:06:33.945 ERRO Failed to send scheduled attestation\r\n21:06:33.946 ERRO Failed to send scheduled attestation\r\n```\r\n_[…and so on and so on]_\r\n\r\nEverything is working perfectly (e.g. no recent restarts) save for the occasional `Previous epoch attestation(s) failed to match head` and `Previous epoch attestation(s) had sub-optimal inclusion delay` warnings, and then all at once I get more than 100 of those `ERRO` log entries.\r\n\r\nThese clumps appear perhaps once every 8 hours on a beacon node whose validator client has ~100 validators. Notable configuration includes:\r\n\r\n```\r\n--validator-monitor-auto\r\n--http-disable-legacy-spec\r\n--block-cache-size 15\r\n```\r\n\r\nAt the time of the most recent occurrence (note `peers`):\r\n\r\n```\r\n21:06:29.001 INFO Synced  slot: 4394730, block:    …  empty, epoch: 137335, finalized_epoch: 137333, finalized_root: 0x585c…60d2, exec_hash: n/a, peers: 85, service: slot_notifier\r\n```\r\n\r\n**UPDATE:**\r\n\r\nI am now also getting all-at-once gobs of these sorts of errors:\r\n```\r\nERRO Unable to send message to the beacon processor, type: gossip_attestation, error: no available capacity, service: router\r\n```\r\nThey were preceded by one of these entries:\r\n```\r\nERRO Attestation queue full                  queue_len: 16384, msg: the system has insufficient resources for load\r\n```\r\n\r\nAlso have gobs of these two:\r\n```\r\nERRO Unable to send message to the beacon processor, type: gossip_aggregate, error: no available capacity, service: router\r\nERRO slog-async: logger dropped messages due to channel overflow, count: 11, service: router\r\n```\r\n…and then:\r\n```\r\nERRO Attestation delay queue is full         msg: check system clock, queue_size: 16384\r\n```\r\n…followed by another slew of:\r\n```\r\nERRO Failed to send scheduled attestation\r\n```\r\n\r\nAccording to `htop` I am using just a fraction of available RAM, and instantaneous average CPU use (8-core) spikes to 100% for about 3 seconds about every 15 seconds, then goes back to about 10%.\r\n\r\n```\r\n● ntp.service - Network Time Service\r\n     Loaded: loaded (/lib/systemd/system/ntp.service; enabled; vendor preset: enabled)\r\n     Active: active (running) since Fri 2022-07-22 16:41:38 EDT; 1 weeks 5 days ago\r\n       Docs: man:ntpd(8)\r\n   Main PID: 750 (ntpd)\r\n      Tasks: 2 (limit: 18486)\r\n     Memory: 1.8M\r\n     CGroup: /system.slice/ntp.service\r\n             └─710 /usr/sbin/ntpd -p /var/run/ntpd.pid -g -u 113:121\r\n\r\nWarning: journal has been rotated since unit was started, output may be incomplete.\r\n~$\r\n```\r\n\r\n**END OF UPDATE**; original ticket continues below.\r\n\r\n## Version\r\n\r\nhttps://github.com/sigp/lighthouse/releases/download/v2.5.1/lighthouse-v2.5.1-aarch64-unknown-linux-gnu.tar.gz\r\n\r\n## Steps to resolve\r\n\r\nIf anyone is able to tell me whether any of the below tactics (or others) may possibly reduce the frequency of said `ERRO` events, and hence to resolve the issue, or—just as valuable—if any/all are likely to have _no_ helpful effect, I would appreciate it:\r\n\r\n1) Enabling `--subscribe-all-subnets`\r\n1) Increase/decrease `target-peers`\r\n1) Increase/decrease `block-cache-size`\r\n1) Increase the IOPS (I/O ops per second) available to the SDD storage of the `data-dir`\r\n1) Increase the throughput (MB per second) available to the SDD storage of the `data-dir`\r\n1) Increase the number of CPU/vCPU cores of the cloud instance running `lighthouse beacon_node`\r\n1) Increase the allotted network performance capacity of the cloud instance running `lighthouse beacon_node`\r\n\r\nThanks for reading.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/3423/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/sigp/lighthouse/issues/3423/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1239442742",
    "html_url": "https://github.com/sigp/lighthouse/issues/3423#issuecomment-1239442742",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/3423",
    "id": 1239442742,
    "node_id": "IC_kwDOCFeAzc5J4GU2",
    "user": {
      "login": "realbigsean",
      "id": 5160426,
      "node_id": "MDQ6VXNlcjUxNjA0MjY=",
      "avatar_url": "https://avatars.githubusercontent.com/u/5160426?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/realbigsean",
      "html_url": "https://github.com/realbigsean",
      "followers_url": "https://api.github.com/users/realbigsean/followers",
      "following_url": "https://api.github.com/users/realbigsean/following{/other_user}",
      "gists_url": "https://api.github.com/users/realbigsean/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/realbigsean/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/realbigsean/subscriptions",
      "organizations_url": "https://api.github.com/users/realbigsean/orgs",
      "repos_url": "https://api.github.com/users/realbigsean/repos",
      "events_url": "https://api.github.com/users/realbigsean/events{/privacy}",
      "received_events_url": "https://api.github.com/users/realbigsean/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-09-07T14:08:55Z",
    "updated_at": "2022-09-07T14:08:55Z",
    "author_association": "COLLABORATOR",
    "body": "These logs generally indicate it's an issue of system resources, if it doesn't look RAM is near it's limit, it might be a limitation of the CPU or disk. If the VPS uses shared CPUs that might be causing issues, it could be throttling you during the bursts you are seeing. So increasing the number of CPUs might solve it (although 8 should be plenty if you are just running lighthouse on this machine). If you have grafana metrics set up, you could also get an idea of whether it might be a CPU vs disk bottleneck by checking out some dashboards",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1239442742/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
