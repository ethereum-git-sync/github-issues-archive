{
  "url": "https://api.github.com/repos/sigp/lighthouse/issues/3344",
  "repository_url": "https://api.github.com/repos/sigp/lighthouse",
  "labels_url": "https://api.github.com/repos/sigp/lighthouse/issues/3344/labels{/name}",
  "comments_url": "https://api.github.com/repos/sigp/lighthouse/issues/3344/comments",
  "events_url": "https://api.github.com/repos/sigp/lighthouse/issues/3344/events",
  "html_url": "https://github.com/sigp/lighthouse/issues/3344",
  "id": 1308476042,
  "node_id": "I_kwDOCFeAzc5N_cKK",
  "number": 3344,
  "title": "Unexpected behavior on hive optimistic sync tests",
  "user": {
    "login": "marioevz",
    "id": 11726710,
    "node_id": "MDQ6VXNlcjExNzI2NzEw",
    "avatar_url": "https://avatars.githubusercontent.com/u/11726710?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/marioevz",
    "html_url": "https://github.com/marioevz",
    "followers_url": "https://api.github.com/users/marioevz/followers",
    "following_url": "https://api.github.com/users/marioevz/following{/other_user}",
    "gists_url": "https://api.github.com/users/marioevz/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/marioevz/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/marioevz/subscriptions",
    "organizations_url": "https://api.github.com/users/marioevz/orgs",
    "repos_url": "https://api.github.com/users/marioevz/repos",
    "events_url": "https://api.github.com/users/marioevz/events{/privacy}",
    "received_events_url": "https://api.github.com/users/marioevz/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 3743113288,
      "node_id": "LA_kwDOCFeAzc7fG2BI",
      "url": "https://api.github.com/repos/sigp/lighthouse/labels/bellatrix",
      "name": "bellatrix",
      "color": "A906A4",
      "default": false,
      "description": "Required to support the Bellatrix Upgrade"
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 3,
  "created_at": "2022-07-18T19:49:19Z",
  "updated_at": "2022-07-20T06:29:18Z",
  "closed_at": null,
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "## Description\r\n\r\nCurrently all optimistic sync tests running lighthouse have an unexpected behavior where the node does not seem to be attempting to sync to the other nodes even after `SAFE_SLOTS_TO_IMPORT_OPTIMISTICALLY` slots have passed.\r\n\r\nThe test cases use 2 or 3 nodes (depending on the test case) and to put them in optimistic sync mode the execution clients are started with two different precomputed PoW chains that have the exact same total difficulty value on different heads.\r\n\r\nLighthouse `SAFE_SLOTS_TO_IMPORT_OPTIMISTICALLY` is configured by using the cli parameter `--safe-slots-to-import-optimistically`, and also the same value is written to the `config.yaml` file (see https://github.com/ethereum/hive/pull/604).\r\n\r\nThere are several types of test cases, where the alternate chain is either a valid or invalid chain.\r\n\r\nWhen the alternate chain is supposed to be invalid, the chain is mocked by using an execution client proxy which returns a constructed `INVALID` response, but this resolution is not expected to happen before `SAFE_SLOTS_TO_IMPORT_OPTIMISTICALLY` slots.\r\n\r\nHive currently waits an extra 4 slots on top of `SAFE_SLOTS_TO_IMPORT_OPTIMISTICALLY` for the client to attempt to check the alternative chain, after which the test case finally fails.\r\n\r\nTo reproduce, run any of the following test cases using https://github.com/marioevz/hive/tree/all-tests-plus-cl-clients branch:\r\n\r\n```\r\nhive --client go-ethereum,lighthouse-bn,lighthouse-vc --sim eth2/engine --sim.limit \"/syncing-with-chain-having-valid-transition-block\"\r\n```\r\n```\r\nhive --client go-ethereum,lighthouse-bn,lighthouse-vc --sim eth2/engine --sim.limit \"/syncing-with-chain-having-invalid-transition-block\"\r\n```\r\n```\r\nhive --client go-ethereum,lighthouse-bn,lighthouse-vc --sim eth2/engine --sim.limit \"/syncing-with-chain-having-invalid-post-transition-block\"\r\n```\r\n```\r\nhive --client go-ethereum,lighthouse-bn,lighthouse-vc --sim eth2/engine --sim.limit \"/re-org-and-sync-with-chain-having-invalid-terminal-block\"\r\n```\r\n\r\n## Version\r\n\r\nLighthouse v2.3.2-rc.0-da7b7a0\r\n\r\n## Present Behaviour\r\n\r\nLighthouse is not trying to sync even after `SAFE_SLOTS_TO_IMPORT_OPTIMISTICALLY` slots have passed.\r\n\r\n## Expected Behaviour\r\n\r\nLighthouse should try to sync only after `SAFE_SLOTS_TO_IMPORT_OPTIMISTICALLY` slots have passed.\r\n",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/3344/reactions",
    "total_count": 2,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 2
  },
  "timeline_url": "https://api.github.com/repos/sigp/lighthouse/issues/3344/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1189799326",
    "html_url": "https://github.com/sigp/lighthouse/issues/3344#issuecomment-1189799326",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/3344",
    "id": 1189799326,
    "node_id": "IC_kwDOCFeAzc5G6uWe",
    "user": {
      "login": "ethDreamer",
      "id": 37123614,
      "node_id": "MDQ6VXNlcjM3MTIzNjE0",
      "avatar_url": "https://avatars.githubusercontent.com/u/37123614?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/ethDreamer",
      "html_url": "https://github.com/ethDreamer",
      "followers_url": "https://api.github.com/users/ethDreamer/followers",
      "following_url": "https://api.github.com/users/ethDreamer/following{/other_user}",
      "gists_url": "https://api.github.com/users/ethDreamer/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/ethDreamer/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/ethDreamer/subscriptions",
      "organizations_url": "https://api.github.com/users/ethDreamer/orgs",
      "repos_url": "https://api.github.com/users/ethDreamer/repos",
      "events_url": "https://api.github.com/users/ethDreamer/events{/privacy}",
      "received_events_url": "https://api.github.com/users/ethDreamer/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-07-20T04:00:25Z",
    "updated_at": "2022-07-20T04:00:25Z",
    "author_association": "COLLABORATOR",
    "body": "So I found one bug with this. We were inappropriately downscoring the peer and disconnecting it when it sent us a block we couldn't optimistically import. I've created #3350 to address that. Unfortunately this hasn't solved the issue. What happens now is the following:\r\n1. Block comes in from gossip\r\n2. Lighthouse finds it does not have the parent for this block\r\n3. Request block from RPC\r\n4. Go to step 2 until you hit genesis\r\n\r\nThis happens for every single block in the chain (twice for some reason), every single time a new gossip block is received. This makes the total number of RPC requests:\r\n$$2 \\sum_{n=1}^{128} n = 16512$$\r\n\r\nWell before this number is hit, the node on the receiving end of these requests rate-limits the requesting node. This causes the requesting node to downscore it and eventually kick the peer. I don't think this is _necessarily_ the wrong behavior.. it's dumb, but programming the right behavior for this case is also tough.\r\n\r\nOne way that could potentially get around this is utilizing the `--trusted-peers` flag on at least one of the nodes\r\n```\r\n--trusted-peers <TRUSTED_PEERS>\r\n    One or more comma-delimited trusted peer ids which always have the highest score according to the peer\r\n    scoring system.\r\n```",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1189799326/reactions",
      "total_count": 1,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 1,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1189823849",
    "html_url": "https://github.com/sigp/lighthouse/issues/3344#issuecomment-1189823849",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/3344",
    "id": 1189823849,
    "node_id": "IC_kwDOCFeAzc5G60Vp",
    "user": {
      "login": "paulhauner",
      "id": 6660660,
      "node_id": "MDQ6VXNlcjY2NjA2NjA=",
      "avatar_url": "https://avatars.githubusercontent.com/u/6660660?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/paulhauner",
      "html_url": "https://github.com/paulhauner",
      "followers_url": "https://api.github.com/users/paulhauner/followers",
      "following_url": "https://api.github.com/users/paulhauner/following{/other_user}",
      "gists_url": "https://api.github.com/users/paulhauner/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/paulhauner/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/paulhauner/subscriptions",
      "organizations_url": "https://api.github.com/users/paulhauner/orgs",
      "repos_url": "https://api.github.com/users/paulhauner/repos",
      "events_url": "https://api.github.com/users/paulhauner/events{/privacy}",
      "received_events_url": "https://api.github.com/users/paulhauner/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-07-20T04:45:04Z",
    "updated_at": "2022-07-20T04:45:04Z",
    "author_association": "MEMBER",
    "body": "> This happens for every single block in the chain (twice for some reason), every single time a new gossip block is received.\r\n\r\nIs the reason we do a parent-by-parent lookup here because there's no finality on the network?",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1189823849/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1189881455",
    "html_url": "https://github.com/sigp/lighthouse/issues/3344#issuecomment-1189881455",
    "issue_url": "https://api.github.com/repos/sigp/lighthouse/issues/3344",
    "id": 1189881455,
    "node_id": "IC_kwDOCFeAzc5G7CZv",
    "user": {
      "login": "AgeManning",
      "id": 7454587,
      "node_id": "MDQ6VXNlcjc0NTQ1ODc=",
      "avatar_url": "https://avatars.githubusercontent.com/u/7454587?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AgeManning",
      "html_url": "https://github.com/AgeManning",
      "followers_url": "https://api.github.com/users/AgeManning/followers",
      "following_url": "https://api.github.com/users/AgeManning/following{/other_user}",
      "gists_url": "https://api.github.com/users/AgeManning/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AgeManning/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AgeManning/subscriptions",
      "organizations_url": "https://api.github.com/users/AgeManning/orgs",
      "repos_url": "https://api.github.com/users/AgeManning/repos",
      "events_url": "https://api.github.com/users/AgeManning/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AgeManning/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2022-07-20T06:29:17Z",
    "updated_at": "2022-07-20T06:29:17Z",
    "author_association": "MEMBER",
    "body": "Let me explain the behaviour in detail, as there may be some misunderstanding here. \r\n\r\nLighthouse under normal operation should sync until it reaches the head of the chain. There are some circumstances where a node can fall out of sync, or temporarily miss blocks. We have a mechanism to catch back up to head without having to run a full blown sync. In these cases when we only need to grab a few blocks, we use the RPC. This is the case happening here.\r\n\r\nWhen we see a block on gossip that we don't know about we try and grab it via the RPC from the peer that says it exists. If that block's parent is not known, we sequentially ask for the parent of the block. This process continues, not until genesis, but only for 2 epochs. If we can't find the source within two epochs, we give up and we should start a full-blown sync. \r\n\r\nWhen we are requesting blocks from a peer, we try and load-balance and request from many peers to avoid overly requesting from a single peer. If a peer says they have a block and we ask them for it and they return an error and refuse to give us the block we rightly downscore them. I don't consider this behaviour dumb, rather if we didn't do this, peers could freely advertise arbitrary blocks and never return them when asked, instead constantly give us a rate-limit error.\r\n\r\nThe final thing that adds to this issue is that RPC rate-limiting amongst clients has never been spec'd because no one could agree on the right way to do it. So all clients impose their own non-spec'd RPC rate-limit error, which we cannot program or account for.\r\n\r\nIn summary, the error witnessed can only occur when all of the following circumstances are met, simultaneously:\r\n- The lighthouse node's main sync algorithm has failed (or the node has gone offline for some reason) causing it not to follow the current head\r\n- A peer on the network gossips about an unknown block (we don't know about this because the peer is malicious or we are not following head correctly for some reason)\r\n- There is a chain of blocks we are not following, so we sequentially try and download this unknown chain of blocks\r\n- The chain is sufficiently long, that makes us do many RPC requests to get the chain (one per block).\r\n- We have only a single peer on the network (more peers mean we would load-balance and therefore be unlikely to hit RPC rate limits)\r\n- The process of checking a block and network round-trip-time of requesting then receiving a block and then re-requesting is faster than the rate limit of a specific node (likely a local network)\r\n\r\nI'm not sure we want to program for this edge case, because a 2-peer network is unrealistic and I think its unlikely to see this set of conditions occur in the real world. \r\n\r\nIf this is just for a test, then I think we should use the `--trusted-peers` command as @ethDreamer suggests, which disables the peer-scoring. \r\nAlternatively, adding more peers to the network will also solve the problem. We could also relax some rate-limiting parameters, but I think they are somewhat generous at the moment.",
    "reactions": {
      "url": "https://api.github.com/repos/sigp/lighthouse/issues/comments/1189881455/reactions",
      "total_count": 1,
      "+1": 1,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
